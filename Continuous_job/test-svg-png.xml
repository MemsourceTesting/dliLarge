<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>Managing file systems</title><subtitle>Creating, modifying, and administering file systems in Red Hat Enterprise Linux 8</subtitle>

<date>2021-03-11</date>
<productname>Red Hat Enterprise Linux</productname>
<productnumber>8</productnumber>
<release>{ProductRelease}</release>
<abstract><para>This documentation collection provides instructions on how to effectively manage file systems in Red Hat Enterprise Linux 8.</para></abstract>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Author_Group.xml"/>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Common_Content/Legal_Notice.xml"/>
<orgname>Red Hat</orgname>
</info>
<preface xml:id="making-open-source-more-inclusive">
<title>Making open source more inclusive</title>
<simpara>Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see <link xlink:href="https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language">our CTO Chris Wright’s message</link>.</simpara>
</preface>
<preface xml:id="proc_providing-feedback-on-red-hat-documentation_managing-file-systems">
<title>Providing feedback on Red Hat documentation</title>
<simpara role="_abstract">We appreciate your input on our documentation. Please let us know how we could make it better. To do so:</simpara>
<itemizedlist>
<listitem>
<simpara>For simple comments on specific passages:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Make sure you are viewing the documentation in the <emphasis>Multi-page HTML</emphasis> format. In addition, ensure you see the <emphasis role="strong">Feedback</emphasis> button in the upper right corner of the document.</simpara>
</listitem>
<listitem>
<simpara>Use your mouse cursor to highlight the part of text that you want to comment on.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Add Feedback</emphasis> pop-up that appears below the highlighted text.</simpara>
</listitem>
<listitem>
<simpara>Follow the displayed instructions.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For submitting more complex feedback, create a Bugzilla ticket:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to the <link xlink:href="https://bugzilla.redhat.com/enter_bug.cgi?product=Red%20Hat%20Enterprise%20Linux%208">Bugzilla</link> website.</simpara>
</listitem>
<listitem>
<simpara>As the Component, use <emphasis role="strong">Documentation</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Fill in the <emphasis role="strong">Description</emphasis> field with your suggestion for improvement. Include a link to the relevant part(s) of documentation.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Submit Bug</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</preface>
<chapter xml:id="overview-of-available-file-systems_managing-file-systems">
<title>Overview of available file systems</title>
<simpara>Choosing the file system that is appropriate for your application is an important decision due to the large number of options available and the trade-offs involved. This chapter describes some of the file systems that ship with Red Hat Enterprise Linux 8 and provides historical background and recommendations on the right file system to suit your application.</simpara>
<section xml:id="types-of-file-systems_overview-of-available-file-systems">
<title>Types of file systems</title>
<simpara>Red Hat Enterprise Linux 8 supports a variety of file systems (FS). Different types of file systems solve different kinds of problems, and their usage is application specific. At the most general level, available file systems can be grouped into the following major types:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Types of file systems and their use cases</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">File system</entry>
<entry align="left" valign="top">Attributes and use cases</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Disk or local FS</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
<entry align="left" valign="top"><simpara>XFS is the default file system in RHEL. Because it lays out files as extents, it is less vulnerable to fragmentation than ext4. Red Hat recommends deploying XFS as your local file system unless there are specific reasons to do otherwise: for example, compatibility or corner cases around performance.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
<entry align="left" valign="top"><simpara>ext4 has the benefit of longevity in Linux. Therefore, it is supported by almost all Linux applications. In most cases, it rivals XFS on performance. ext4 is commonly used for home directories.</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Network or client-and-server FS</simpara></entry>
<entry align="left" valign="top"><simpara>NFS</simpara></entry>
<entry align="left" valign="top"><simpara>Use NFS to share files between multiple systems on the same network.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>SMB</simpara></entry>
<entry align="left" valign="top"><simpara>Use SMB for file sharing with Microsoft Windows systems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Shared storage or shared disk FS</simpara></entry>
<entry align="left" valign="top"><simpara>GFS2</simpara></entry>
<entry align="left" valign="top"><simpara>GFS2 provides shared write access to members of a compute cluster. The emphasis is on stability and reliability, with the functional experience of a local file system as possible. SAS Grid, Tibco MQ, IBM Websphere MQ, and Red Hat Active MQ have been deployed successfully on GFS2.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Volume-managing FS</simpara></entry>
<entry align="left" valign="top"><simpara>Stratis (Technology Preview)</simpara></entry>
<entry align="left" valign="top"><simpara>Stratis is a volume manager built on a combination of XFS and LVM. The purpose of Stratis is to emulate capabilities offered by volume-managing file systems like Btrfs and ZFS. It is possible to build this stack manually, but Stratis reduces configuration complexity, implements best practices, and consolidates error information.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="local-file-systems_overview-of-available-file-systems">
<title>Local file systems</title>
<simpara>Local file systems are file systems that run on a single, local server and are directly attached to storage.</simpara>
<simpara>For example, a local file system is the only choice for internal SATA or SAS disks, and is used when your server has internal hardware RAID controllers with local drives. Local file systems are also the most common file systems used on SAN attached storage when the device exported on the SAN is not shared.</simpara>
<simpara>All local file systems are POSIX-compliant and are fully compatible with all supported Red Hat Enterprise Linux releases. POSIX-compliant file systems provide support for a well-defined set of system calls, such as <literal>read()</literal>, <literal>write()</literal>, and <literal>seek()</literal>.</simpara>
<simpara>From the application programmer’s point of view, there are relatively few differences between local file systems. The most notable differences from a user’s perspective are related to scalability and performance. When considering a file system choice, consider how large the file system needs to be, what unique features it should have, and how it performs under your workload.</simpara>
<bridgehead xml:id="available_local_file_systems" renderas="sect3" remap="_available_local_file_systems">Available local file systems</bridgehead>
<itemizedlist>
<listitem>
<simpara>XFS</simpara>
</listitem>
<listitem>
<simpara>ext4</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="the-xfs-file-system_overview-of-available-file-systems">
<title>The XFS file system</title>
<simpara>XFS is a highly scalable, high-performance, robust, and mature 64-bit journaling file system that supports very large files and file systems on a single host. It is the default file system in Red Hat Enterprise Linux 8. XFS was originally developed in the early 1990s by SGI and has a long history of running on extremely large servers and storage arrays.</simpara>
<simpara>The features of XFS include:</simpara>
<variablelist>
<varlistentry>
<term>Reliability</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Metadata journaling, which ensures file system integrity after a system crash by keeping a record of file system operations that can be replayed when the system is restarted and the file system remounted</simpara>
</listitem>
<listitem>
<simpara>Extensive run-time metadata consistency checking</simpara>
</listitem>
<listitem>
<simpara>Scalable and fast repair utilities</simpara>
</listitem>
<listitem>
<simpara>Quota journaling. This avoids the need for lengthy quota consistency checks after a crash.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Scalability and performance</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Supported file system size up to 1024 TiB</simpara>
</listitem>
<listitem>
<simpara>Ability to support a large number of concurrent operations</simpara>
</listitem>
<listitem>
<simpara>B-tree indexing for scalability of free space management</simpara>
</listitem>
<listitem>
<simpara>Sophisticated metadata read-ahead algorithms</simpara>
</listitem>
<listitem>
<simpara>Optimizations for streaming video workloads</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Allocation schemes</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Extent-based allocation</simpara>
</listitem>
<listitem>
<simpara>Stripe-aware allocation policies</simpara>
</listitem>
<listitem>
<simpara>Delayed allocation</simpara>
</listitem>
<listitem>
<simpara>Space pre-allocation</simpara>
</listitem>
<listitem>
<simpara>Dynamically allocated inodes</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Other features</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Reflink-based file copies (new in Red Hat Enterprise Linux 8)</simpara>
</listitem>
<listitem>
<simpara>Tightly integrated backup and restore utilities</simpara>
</listitem>
<listitem>
<simpara>Online defragmentation</simpara>
</listitem>
<listitem>
<simpara>Online file system growing</simpara>
</listitem>
<listitem>
<simpara>Comprehensive diagnostics capabilities</simpara>
</listitem>
<listitem>
<simpara>Extended attributes (<literal>xattr</literal>). This allows the system to associate several additional name/value pairs per file.</simpara>
</listitem>
<listitem>
<simpara>Project or directory quotas. This allows quota restrictions over a directory tree.</simpara>
</listitem>
<listitem>
<simpara>Subsecond timestamps</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<bridgehead xml:id="performance_characteristics" renderas="sect3" remap="_performance_characteristics">Performance characteristics</bridgehead>
<simpara>XFS has a high performance on large systems with enterprise workloads. A large system is one with a relatively high number of CPUs, multiple HBAs, and connections to external disk arrays. XFS also performs well on smaller systems that have a multi-threaded, parallel I/O workload.</simpara>
<simpara>XFS has a relatively low performance for single threaded, metadata-intensive workloads: for example, a workload that creates or deletes large numbers of small files in a single thread.</simpara>
</section>
<section xml:id="the-ext4-file-system_overview-of-available-file-systems">
<title>The ext4 file system</title>
<simpara>The ext4 file system is the fourth generation of the ext file system family. It was the default file system in Red Hat Enterprise Linux 6.</simpara>
<simpara>The ext4 driver can read and write to ext2 and ext3 file systems, but the ext4 file system format is not compatible with ext2 and ext3 drivers.</simpara>
<simpara>ext4 adds several new and improved features, such as:</simpara>
<itemizedlist>
<listitem>
<simpara>Supported file system size up to 50 TiB</simpara>
</listitem>
<listitem>
<simpara>Extent-based metadata</simpara>
</listitem>
<listitem>
<simpara>Delayed allocation</simpara>
</listitem>
<listitem>
<simpara>Journal checksumming</simpara>
</listitem>
<listitem>
<simpara>Large storage support</simpara>
</listitem>
</itemizedlist>
<simpara>The extent-based metadata and the delayed allocation features provide a more compact and efficient way to track utilized space in a file system. These features improve file system performance and reduce the space consumed by metadata. Delayed allocation allows the file system to postpone selection of the permanent location for newly written user data until the data is flushed to disk. This enables higher performance since it can allow for larger, more contiguous allocations, allowing the file system to make decisions with much better information.</simpara>
<simpara>File system repair time using the <literal>fsck</literal> utility in ext4 is much faster than in ext2 and ext3. Some file system repairs have demonstrated up to a six-fold increase in performance.</simpara>
</section>
<section xml:id="comparison-of-xfs-and-ext4_overview-of-available-file-systems">
<title>Comparison of XFS and ext4</title>
<simpara>XFS is the default file system in RHEL. This section compares the usage and features of XFS and ext4.</simpara>
<variablelist>
<varlistentry>
<term>Metadata error behavior</term>
<listitem>
<simpara>In ext4, you can configure the behavior when the file system encounters metadata errors. The default behavior is to simply continue the operation. When XFS encounters an unrecoverable metadata error, it shuts down the file system and returns the <literal>EFSCORRUPTED</literal> error.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Quotas</term>
<listitem>
<simpara>In ext4, you can enable quotas when creating the file system or later on an existing file system. You can then configure the quota enforcement using a mount option.</simpara>
<simpara>XFS quotas are not a remountable option. You must activate quotas on the initial mount.</simpara>
<simpara>Running the <literal>quotacheck</literal> command on an XFS file system has no effect. The first time you turn on quota accounting, XFS checks quotas automatically.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>File system resize</term>
<listitem>
<simpara>XFS has no utility to reduce the size of a file system. You can only increase the size of an XFS file system. In comparison, ext4 supports both extending and reducing the size of a file system.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Inode numbers</term>
<listitem>
<simpara>The ext4 file system does not support more than 2<superscript>32</superscript> inodes.</simpara>
<simpara>XFS dynamically allocates inodes. An XFS file system cannot run out of inodes as long as there is free space on the file system.</simpara>
<simpara>Certain applications cannot properly handle inode numbers larger than 2<superscript>32</superscript> on an XFS file system. These applications might cause the failure of 32-bit stat calls with the <literal>EOVERFLOW</literal> return value. Inode number exceed 2<superscript>32</superscript> under the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>The file system is larger than 1 TiB with 256-byte inodes.</simpara>
</listitem>
<listitem>
<simpara>The file system is larger than 2 TiB with 512-byte inodes.</simpara>
</listitem>
</itemizedlist>
<simpara>If your application fails with large inode numbers, mount the XFS file system with the <literal>-o inode32</literal> option to enforce inode numbers below 2<superscript>32</superscript>. Note that using <literal>inode32</literal> does not affect inodes that are already allocated with 64-bit numbers.</simpara>
<important>
<simpara>Do <emphasis>not</emphasis> use the <literal>inode32</literal> option unless a specific environment requires it. The <literal>inode32</literal> option changes allocation behavior. As a consequence, the <literal>ENOSPC</literal> error might occur if no space is available to allocate inodes in the lower disk blocks.</simpara>
</important>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="choosing-a-local-file-system_overview-of-available-file-systems">
<title>Choosing a local file system</title>
<simpara>To choose a file system that meets your application requirements, you need to understand the target system on which you are going to deploy the file system. You can use the following questions to inform your decision:</simpara>
<itemizedlist>
<listitem>
<simpara>Do you have a large server?</simpara>
</listitem>
<listitem>
<simpara>Do you have large storage requirements or have a local, slow SATA drive?</simpara>
</listitem>
<listitem>
<simpara>What kind of I/O workload do you expect your application to present?</simpara>
</listitem>
<listitem>
<simpara>What are your throughput and latency requirements?</simpara>
</listitem>
<listitem>
<simpara>How stable is your server and storage hardware?</simpara>
</listitem>
<listitem>
<simpara>What is the typical size of your files and data set?</simpara>
</listitem>
<listitem>
<simpara>If the system fails, how much downtime can you suffer?</simpara>
</listitem>
</itemizedlist>
<simpara>If both your server and your storage device are large, XFS is the best choice. Even with smaller storage arrays, XFS performs very well when the average file sizes are large (for example, hundreds of megabytes in size).</simpara>
<simpara>If your existing workload has performed well with ext4, staying with ext4 should provide you and your applications with a very familiar environment.</simpara>
<simpara>The ext4 file system tends to perform better on systems that have limited I/O capability. It performs better on limited bandwidth (less than 200MB/s) and up to around 1000 IOPS capability. For anything with higher capability, XFS tends to be faster.</simpara>
<simpara>XFS consumes about twice the CPU-per-metadata operation compared to ext4, so if you have a CPU-bound workload with little concurrency, then ext4 will be faster. In general, ext4 is better if an application uses a single read/write thread and small files, while XFS shines when an application uses multiple read/write threads and bigger files.</simpara>
<simpara>You cannot shrink an XFS file system. If you need to be able to shrink the file system, consider using ext4, which supports offline shrinking.</simpara>
<simpara>In general, Red Hat recommends that you use XFS unless you have a specific use case for ext4. You should also measure the performance of your specific application on your target server and storage system to make sure that you choose the appropriate type of file system.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Summary of local file system recommendations</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="60*"/>
<colspec colname="col_2" colwidth="40*"/>
<thead>
<row>
<entry align="left" valign="top">Scenario</entry>
<entry align="left" valign="top">Recommended file system</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>No special use case</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Large server</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Large storage devices</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Large files</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Multi-threaded I/O</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Single-threaded I/O</simpara></entry>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Limited I/O capability (under 1000 IOPS)</simpara></entry>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Limited bandwidth (under 200MB/s)</simpara></entry>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>CPU-bound workload</simpara></entry>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Support for offline shrinking</simpara></entry>
<entry align="left" valign="top"><simpara>ext4</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="network-file-systems_overview-of-available-file-systems">
<title>Network file systems</title>
<simpara>Network file systems, also referred to as client/server file systems, enable client systems to access files that are stored on a shared server. This makes it possible for multiple users on multiple systems to share files and storage resources.</simpara>
<simpara>Such file systems are built from one or more servers that export a set of file systems to one or more clients. The client nodes do not have access to the underlying block storage, but rather interact with the storage using a protocol that allows for better access control.</simpara>
<bridgehead xml:id="available_network_file_systems" renderas="sect3" remap="_available_network_file_systems">Available network file systems</bridgehead>
<itemizedlist>
<listitem>
<simpara>The most common client/server file system for RHEL customers is the NFS file system. RHEL provides both an NFS server component to export a local file system over the network and an NFS client to import these file systems.</simpara>
</listitem>
<listitem>
<simpara>RHEL also includes a CIFS client that supports the popular Microsoft SMB file servers for Windows interoperability. The userspace Samba server provides Windows clients with a Microsoft SMB service from a RHEL server.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="shared-storage-file-systems_overview-of-available-file-systems">
<title>Shared storage file systems</title>
<simpara>Shared storage file systems, sometimes referred to as cluster file systems, give each server in the cluster direct access to a shared block device over a local storage area network (SAN).</simpara>
<bridgehead xml:id="comparison_with_network_file_systems" renderas="sect3" remap="_comparison_with_network_file_systems">Comparison with network file systems</bridgehead>
<simpara>Like client/server file systems, shared storage file systems work on a set of servers that are all members of a cluster. Unlike NFS, however, no single server provides access to data or metadata to other members: each member of the cluster has direct access to the same storage device (the <emphasis>shared storage</emphasis>), and all cluster member nodes access the same set of files.</simpara>
<bridgehead xml:id="concurrency" renderas="sect3" remap="_concurrency">Concurrency</bridgehead>
<simpara>Cache coherency is key in a clustered file system to ensure data consistency and integrity. There must be a single version of all files in a cluster visible to all nodes within a cluster. The file system must prevent members of the cluster from updating the same storage block at the same time and causing data corruption. In order to do that, shared storage file systems use a cluster wide-locking mechanism to arbitrate access to the storage as a concurrency control mechanism. For example, before creating a new file or writing to a file that is opened on multiple servers, the file system component on the server must obtain the correct lock.</simpara>
<simpara>The requirement of cluster file systems is to provide a highly available service like an Apache web server. Any member of the cluster will see a fully coherent view of the data stored in their shared disk file system, and all updates will be arbitrated correctly by the locking mechanisms.</simpara>
<bridgehead xml:id="performance_characteristics_2" renderas="sect3" remap="_performance_characteristics_2">Performance characteristics</bridgehead>
<simpara>Shared disk file systems do not always perform as well as local file systems running on the same system due to the computational cost of the locking overhead. Shared disk file systems perform well with workloads where each node writes almost exclusively to a particular set of files that are not shared with other nodes or where a set of files is shared in an almost exclusively read-only manner across a set of nodes. This results in a minimum of cross-node cache invalidation and can maximize performance.</simpara>
<simpara>Setting up a shared disk file system is complex, and tuning an application to perform well on a shared disk file system can be challenging.</simpara>
<bridgehead xml:id="available_shared_storage_file_systems" renderas="sect3" remap="_available_shared_storage_file_systems">Available shared storage file systems</bridgehead>
<itemizedlist>
<listitem>
<simpara>Red Hat Enterprise Linux provides the GFS2 file system. GFS2 comes tightly integrated with the Red Hat Enterprise Linux High Availability Add-On and the Resilient Storage Add-On.</simpara>
<simpara>Red Hat Enterprise Linux supports GFS2 on clusters that range in size from 2 to 16 nodes.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="choosing-between-network-and-shared-storage-file-systems_overview-of-available-file-systems">
<title>Choosing between network and shared storage file systems</title>
<simpara>When choosing between network and shared storage file systems, consider the following points:</simpara>
<itemizedlist>
<listitem>
<simpara>NFS-based network file systems are an extremely common and popular choice for environments that provide NFS servers.</simpara>
</listitem>
<listitem>
<simpara>Network file systems can be deployed using very high-performance networking technologies like Infiniband or 10 Gigabit Ethernet. This means that you should not turn to shared storage file systems just to get raw bandwidth to your storage. If the speed of access is of prime importance, then use NFS to export a local file system like XFS.</simpara>
</listitem>
<listitem>
<simpara>Shared storage file systems are not easy to set up or to maintain, so you should deploy them only when you cannot provide your required availability with either local or network file systems.</simpara>
</listitem>
<listitem>
<simpara>A shared storage file system in a clustered environment helps reduce downtime by eliminating the steps needed for unmounting and mounting that need to be done during a typical fail-over scenario involving the relocation of a high-availability service.</simpara>
</listitem>
</itemizedlist>
<simpara>Red Hat recommends that you use network file systems unless you have a specific use case for shared storage file systems. Use shared storage file systems primarily for deployments that need to provide high-availability services with minimum downtime and have stringent service-level requirements.</simpara>
</section>
<section xml:id="volume-managing-file-systems_overview-of-available-file-systems">
<title>Volume-managing file systems</title>
<simpara>Volume-managing file systems integrate the entire storage stack for the purposes of simplicity and in-stack optimization.</simpara>
<bridgehead xml:id="available_volume_managing_file_systems" renderas="sect3" remap="_available_volume_managing_file_systems">Available volume-managing file systems</bridgehead>
<itemizedlist>
<listitem>
<simpara>Red Hat Enterprise Linux 8 provides the Stratis volume manager as a Technology Preview. Stratis uses XFS for the file system layer and integrates it with LVM, Device Mapper, and other components.</simpara>
<simpara>Stratis was first released in Red Hat Enterprise Linux 8.0. It is conceived to fill the gap created when Red Hat deprecated Btrfs. Stratis 1.0 is an intuitive, command line-based volume manager that can perform significant storage management operations while hiding the complexity from the user:</simpara>
<itemizedlist>
<listitem>
<simpara>Volume management</simpara>
</listitem>
<listitem>
<simpara>Pool creation</simpara>
</listitem>
<listitem>
<simpara>Thin storage pools</simpara>
</listitem>
<listitem>
<simpara>Snapshots</simpara>
</listitem>
<listitem>
<simpara>Automated read cache</simpara>
</listitem>
</itemizedlist>
<simpara>Stratis offers powerful features, but currently lacks certain capabilities of other offerings that it might be compared to, such as Btrfs or ZFS. Most notably, it does not support CRCs with self healing.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="managing-local-storage-using-rhel-system-roles_managing-file-systems">
<title>Managing local storage using RHEL System Roles</title>
<simpara>To manage LVM and local file systems (FS) using Ansible, you can use the <literal>storage</literal> role, which is one of the RHEL System Roles available in RHEL 8.</simpara>
<simpara>Using the <literal>storage</literal> role enables you to automate administration of file systems on disks and logical volumes on multiple machines and across all versions of RHEL starting with RHEL 7.7.</simpara>
<simpara>For more information on RHEL System Roles and how to apply them, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#intro-to-rhel-system-roles_getting-started-with-rhel-system-roles">Introduction to RHEL System Roles</link>.</simpara>
<section xml:id="storage-role-intro_managing-local-storage-using-rhel-system-roles">
<title>Introduction to the storage role</title>
<simpara>The <literal>storage</literal> role can manage:</simpara>
<itemizedlist>
<listitem>
<simpara>File systems on disks which have not been partitioned</simpara>
</listitem>
<listitem>
<simpara>Complete LVM volume groups including their logical volumes and file systems</simpara>
</listitem>
</itemizedlist>
<simpara>With the <literal>storage</literal> role  you can perform the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Create a file system</simpara>
</listitem>
<listitem>
<simpara>Remove a file system</simpara>
</listitem>
<listitem>
<simpara>Mount a file system</simpara>
</listitem>
<listitem>
<simpara>Unmount a file system</simpara>
</listitem>
<listitem>
<simpara>Create LVM volume groups</simpara>
</listitem>
<listitem>
<simpara>Remove LVM volume groups</simpara>
</listitem>
<listitem>
<simpara>Create logical volumes</simpara>
</listitem>
<listitem>
<simpara>Remove logical volumes</simpara>
</listitem>
<listitem>
<simpara>Create RAID volumes</simpara>
</listitem>
<listitem>
<simpara>Remove RAID volumes</simpara>
</listitem>
<listitem>
<simpara>Create LVM pools with RAID</simpara>
</listitem>
<listitem>
<simpara>Remove LVM pools with RAID</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="parameters-that-identify-a-storage-device-in-the-storage-system-role_managing-local-storage-using-rhel-system-roles">
<title>Parameters that identify a storage device in the storage system role</title>
<simpara>Your <literal>storage</literal> role configuration affects only the file systems, volumes, and pools that you list in the following variables.</simpara>
<variablelist>
<varlistentry>
<term><literal>storage_volumes</literal></term>
<listitem>
<simpara>List of file systems on all unpartitioned disks to be managed.</simpara>
<simpara>Partitions are currently unsupported.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>storage_pools</literal></term>
<listitem>
<simpara>List of pools to be managed.</simpara>
<simpara>Currently the only supported pool type is LVM. With LVM, pools represent volume groups (VGs). Under each pool there is a list of volumes to be managed by the role. With LVM, each volume corresponds to a logical volume (LV) with a file system.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="an-example-ansible-playbook-to-create-an-xfs-file-system_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to create an XFS file system on a block device</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create an XFS file system on a block device using the default parameters.</simpara>
<warning>
<simpara>The <literal>storage</literal> role can create a file system only on an unpartitioned, whole disk or a logical volume (LV). It cannot create the file system on a partition.</simpara>
</warning>
<example>
<title>A playbook that creates XFS on /dev/sdb</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: xfs
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The volume name (<literal><emphasis>barefs</emphasis></literal> in the example) is currently arbitrary. The <literal>storage</literal> role identifies the volume by the disk device listed under the <literal>disks:</literal> attribute.</simpara>
</listitem>
<listitem>
<simpara>You can omit the <literal>fs_type: xfs</literal> line because XFS is the default file system in RHEL 8.</simpara>
</listitem>
<listitem>
<simpara>To create the file system on an LV, provide the LVM setup under the <literal>disks:</literal> attribute, including the enclosing volume group. For details, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configuring-lvm-volumes-configuring-and-managing-logical-volumes#an-example-playbook-to-manage-logical-volumes_managing-lvm-logical-volumes-using-rhel-system-roles">Example Ansible playbook to manage logical volumes</link>.</simpara>
<simpara>Do not provide the path to the LV device.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="an-example-ansible-playbook-to-persistently-mount-a-file-system_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to persistently mount a file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to immediately and persistently mount an XFS file system.</simpara>
<example>
<title>A playbook that mounts a file system on /dev/sdb to /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: <emphasis><phrase role="replaceable">xfs</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>This playbook adds the file system to the <literal role="filename">/etc/fstab</literal> file, and mounts the file system immediately.</simpara>
</listitem>
<listitem>
<simpara>If the file system on the <literal>/dev/sdb</literal> device or the mount point directory do not exist, the playbook creates them.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="an-example-playbook-to-manage-logical-volumes_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to manage logical volumes</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create an LVM logical volume in a volume group.</simpara>
<example>
<title>A playbook that creates a mylv logical volume in the myvg volume group</title>
<screen>- hosts: all
  vars:
    storage_pools:
      - name: <emphasis><phrase role="replaceable">myvg</phrase></emphasis>
        disks:
          - <emphasis><phrase role="replaceable">sda</phrase></emphasis>
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
          - <emphasis><phrase role="replaceable">sdc</phrase></emphasis>
        volumes:
          - name: <emphasis><phrase role="replaceable">mylv</phrase></emphasis>
            size: <emphasis><phrase role="replaceable">2G</phrase></emphasis>
            fs_type: <emphasis><phrase role="replaceable">ext4</phrase></emphasis>
            mount_point: <emphasis><phrase role="replaceable">/mnt</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The <literal>myvg</literal> volume group consists of the following disks:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/dev/sda</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/dev/sdb</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/dev/sdc</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If the <literal>myvg</literal> volume group already exists, the playbook adds the logical volume to the volume group.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>myvg</literal> volume group does not exist, the playbook creates it.</simpara>
</listitem>
<listitem>
<simpara>The playbook creates an Ext4 file system on the <literal>mylv</literal> logical volume, and persistently mounts the file system at <literal role="filename">/mnt</literal>.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="an-example-ansible-playbook-to-enable-online-block-discard_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to enable online block discard</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to mount an XFS file system with online block discard enabled.</simpara>
<example>
<title>A playbook that enables online block discard on /mnt/data/</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: <emphasis><phrase role="replaceable">xfs</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
        mount_options: discard
  roles:
    - rhel-system-roles.storage</screen>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>This playbook also performs all the operations of the persistent mount example described in <xref linkend="an-example-ansible-playbook-to-persistently-mount-a-file-system_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="an-example-playbook-to-create-mount-an-ext4-file-system_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to create and mount an Ext4 file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create and mount an Ext4 file system.</simpara>
<example>
<title>A playbook that creates Ext4 on /dev/sdb and mounts it at /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: ext4
        fs_label: <emphasis><phrase role="replaceable">label-name</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The playbook creates the file system on the <literal>/dev/sdb</literal> disk.</simpara>
</listitem>
<listitem>
<simpara>The playbook persistently mounts the file system at the <literal><emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis></literal> directory.</simpara>
</listitem>
<listitem>
<simpara>The label of the file system is <literal><emphasis><phrase role="replaceable">label-name</phrase></emphasis></literal>.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="an-example-ansible-playbook-to-create-mount-ext3-file-system_managing-local-storage-using-rhel-system-roles">
<title>Example Ansible playbook to create and mount an ext3 file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create and mount an Ext3 file system.</simpara>
<example>
<title>A playbook that creates Ext3 on /dev/sdb and mounts it at /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: ext3
        fs_label: <emphasis><phrase role="replaceable">label-name</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The playbook creates the file system on the <literal>/dev/sdb</literal> disk.</simpara>
</listitem>
<listitem>
<simpara>The playbook persistently mounts the file system at the <literal><emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis></literal> directory.</simpara>
</listitem>
<listitem>
<simpara>The label of the file system is <literal><emphasis><phrase role="replaceable">label-name</phrase></emphasis></literal>.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configure-raid-volume-using-storage-system-role_managing-local-storage-using-rhel-system-roles">
<title>Configuring a RAID volume using the storage system role</title>
<simpara>With the <literal>storage</literal>  System Role, you can configure a RAID volume on RHEL using Red Hat Ansible Automation Platform. In this section you will learn how to set up an Ansible playbook with the available parameters to configure a RAID volume to suit your requirements.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have Red Hat Ansible Engine installed on the system from which you want to run the playbook.</simpara>
<note>
<simpara>You do not have to have Red Hat Ansible Automation Platform installed on the systems on which you want to deploy the <literal>storage</literal> solution.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have the <literal>rhel-system-roles</literal> package installed on the system from which you want to run the playbook.</simpara>
</listitem>
<listitem>
<simpara>You have an inventory file detailing the systems on which you want to deploy a RAID volume using the <literal>storage</literal> System Role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new <literal><emphasis>playbook.yml</emphasis></literal> file with the following content:</simpara>
<screen role="white-space-pre">- hosts: all
  vars:
    storage_safe_mode: false
    storage_volumes:
      - name: data
        type: raid
        disks: [sdd, sde, sdf, sdg]
        raid_level: raid0
        raid_chunk_size: 32 KiB
        mount_point: /mnt/data
        state: present
  roles:
    - name: rhel-system-roles.storage</screen>
<warning>
<simpara>Device names can change in certain circumstances; for example, when you add a new disk to a system. Therefore, to prevent data loss, we do not recommend using specific disk names in the playbook.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Optional. Verify playbook syntax.</simpara>
<screen># ansible-playbook --syntax-check <emphasis>playbook.yml</emphasis></screen>
</listitem>
<listitem>
<simpara>Run the playbook on your inventory file:</simpara>
<screen># ansible-playbook -i <emphasis>inventory.file</emphasis> <emphasis>/path/to/file/playbook.yml</emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information about RAID, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/managing-raid_managing-storage-devices">Managing RAID</link>.</simpara>
</listitem>
<listitem>
<simpara>For details about the parameters used in the storage system role, see the <literal>/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-lvm-pool-with-raid-using-storage-system-role_managing-local-storage-using-rhel-system-roles">
<title>Configuring an LVM pool with RAID using the storage system role</title>
<simpara>With the <literal>storage</literal> System Role, you can configure an LVM pool with RAID on RHEL using Red Hat Ansible Automation Platform. In this section you will learn how to set up an Ansible playbook with the available parameters to configure an LVM pool with RAID.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have Red Hat Ansible Engine installed on the system from which you want to run the playbook.</simpara>
<note>
<simpara>You do not have to have Red Hat Ansible Automation Platform installed on the systems on which you want to deploy the <literal>storage</literal> solution.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have the <literal>rhel-system-roles</literal> package installed on the system from which you want to run the playbook.</simpara>
</listitem>
<listitem>
<simpara>You have an inventory file detailing the systems on which you want to configure an LVM pool with RAID using the <literal>storage</literal> System Role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new <literal><emphasis>playbook.yml</emphasis></literal> file with the following content:</simpara>
<screen role="white-space-pre">- hosts: all
  vars:
    storage_safe_mode: false
    storage_pools:
      - name: my_pool
        type: lvm
        disks: [sdh, sdi]
        raid_level: raid1
        volumes:
          - name: my_pool
            size: "1 GiB"
            mount_point: "/mnt/app/shared"
            fs_type: xfs
            state: present
  roles:
    - name: rhel-system-roles.storage</screen>
<note>
<simpara>To create an LVM pool with RAID, you must specify the RAID type using the <literal>raid_level</literal> parameter.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional. Verify playbook syntax.</simpara>
<screen># ansible-playbook --syntax-check <emphasis>playbook.yml</emphasis></screen>
</listitem>
<listitem>
<simpara>Run the playbook on your inventory file:</simpara>
<screen># ansible-playbook -i <emphasis>inventory.file</emphasis> <emphasis>/path/to/file/playbook.yml</emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information about RAID, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/managing-raid_managing-storage-devices">Managing RAID</link>.</simpara>
</listitem>
<listitem>
<simpara>For details about the parameters used in the storage system role, see the <literal>/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_creating-a-luks-encrypted-volume-using-the-storage-role_managing-local-storage-using-rhel-system-roles">
<title>Creating a LUKS encrypted volume using the storage role</title>
<simpara role="_abstract">You can use the <literal>storage</literal> role to create and configure a volume encrypted with LUKS by running an Ansible playbook.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have Red Hat Ansible Engine installed on the system from which you want to run the playbook.</simpara>
<note>
<simpara>You do not have to have Red Hat Ansible Automation Platform installed on the systems on which you want to create the volume.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have the <literal>rhel-system-roles</literal> package installed on the Ansible controller.</simpara>
</listitem>
<listitem>
<simpara>You have an inventory file detailing the systems on which you want to deploy a LUKS encrypted volume using the storage System Role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new <literal><emphasis>playbook.yml</emphasis></literal> file with the following content:</simpara>
<screen>- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis>barefs</emphasis>
        type: disk
        disks:
         <emphasis>- sdb</emphasis>
        fs_type: xfs
        fs_label: <emphasis>label-name</emphasis>
        mount_point: <emphasis>/mnt/data</emphasis>
        encryption: true
        encryption_password: <emphasis>your-password</emphasis>
  roles:
   - rhel-system-roles.storage</screen>
</listitem>
<listitem>
<simpara>Optional. Verify playbook syntax:</simpara>
<screen># ansible-playbook --syntax-check <emphasis>playbook.yml</emphasis></screen>
</listitem>
<listitem>
<simpara>Run the playbook on your inventory file:</simpara>
<screen># ansible-playbook -i <emphasis>inventory.file</emphasis> <emphasis>/path/to/file/playbook.yml</emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information about LUKS, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/encrypting-block-devices-using-luks_managing-storage-devices">17. Encrypting block devices using LUKS.</link>.</simpara>
</listitem>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal>/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information, install the <literal>rhel-system-roles</literal> package and see the following directories:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/usr/share/doc/rhel-system-roles/storage/</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/usr/share/ansible/roles/rhel-system-roles.storage/</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="mounting-nfs-shares_managing-file-systems">
<title>Mounting NFS shares</title>
<simpara>As a system administrator, you can mount remote NFS shares on your system to access shared data.</simpara>
<section xml:id="introduction-to-nfs_mounting-nfs-shares">
<title>Introduction to NFS</title>
<simpara>This section explains the basic concepts of the NFS service.</simpara>
<simpara>A Network File System (NFS) allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally. This enables you to consolidate resources onto centralized servers on the network.</simpara>
<simpara>The NFS server refers to the <literal role="filename">/etc/exports</literal> configuration file to determine whether the client is allowed to access any exported file systems. Once verified, all file and directory operations are available to the user.</simpara>
</section>
<section xml:id="supported-nfs-versions_mounting-nfs-shares">
<title>Supported NFS versions</title>
<simpara>This section lists versions of NFS supported in Red Hat Enterprise Linux and their features.</simpara>
<simpara>Currently, Red Hat Enterprise Linux 8 supports the following major versions of NFS:</simpara>
<itemizedlist>
<listitem>
<simpara>NFS version 3 (NFSv3) supports safe asynchronous writes and is more robust at error handling than the previous NFSv2; it also supports 64-bit file sizes and offsets, allowing clients to access more than 2 GB of file data.</simpara>
</listitem>
<listitem>
<simpara>NFS version 4 (NFSv4) works through firewalls and on the Internet, no longer requires an <literal>rpcbind</literal> service, supports Access Control Lists (ACLs), and utilizes stateful operations.</simpara>
</listitem>
</itemizedlist>
<simpara>NFS version 2 (NFSv2) is no longer supported by Red Hat.</simpara>
<bridgehead xml:id="default_nfs_version" renderas="sect3" remap="_default_nfs_version">Default NFS version</bridgehead>
<simpara>The default NFS version in Red Hat Enterprise Linux 8 is 4.2. NFS clients attempt to mount using NFSv4.2 by default, and fall back to NFSv4.1 when the server does not support NFSv4.2. The mount later falls back to NFSv4.0 and then to NFSv3.</simpara>
<bridgehead xml:id="features_of_minor_nfs_versions" renderas="sect3" remap="_features_of_minor_nfs_versions">Features of minor NFS versions</bridgehead>
<simpara>Following are the features of NFSv4.2 in Red Hat Enterprise Linux 8:</simpara>
<variablelist>
<varlistentry>
<term>Server-side copy</term>
<listitem>
<simpara>Enables the NFS client to efficiently copy data without wasting network resources using the <literal>copy_file_range()</literal> system call.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Sparse files</term>
<listitem>
<simpara>Enables files to have one or more <emphasis>holes</emphasis>, which are unallocated or uninitialized data blocks consisting only of zeroes. The <literal>lseek()</literal> operation in NFSv4.2 supports <literal>seek_hole()</literal> and <literal>seek_data()</literal>, which enables applications to map out the location of holes in the sparse file.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Space reservation</term>
<listitem>
<simpara>Permits storage servers to reserve free space, which prohibits servers to run out of space. NFSv4.2 supports the <literal>allocate()</literal> operation to reserve space, the <literal>deallocate()</literal> operation to unreserve space, and the <literal>fallocate()</literal> operation to preallocate or deallocate space in a file.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Labeled NFS</term>
<listitem>
<simpara>Enforces data access rights and enables SELinux labels between a client and a server for individual files on an NFS file system.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Layout enhancements</term>
<listitem>
<simpara>Provides the <literal>layoutstats()</literal> operation, which enables some Parallel NFS (pNFS) servers to collect better performance statistics.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Following are the features of NFSv4.1:</simpara>
<itemizedlist>
<listitem>
<simpara>Enhances performance and security of network, and also includes client-side support for pNFS.</simpara>
</listitem>
<listitem>
<simpara>No longer requires a separate TCP connection for callbacks, which allows an NFS server to grant delegations even when it cannot contact the client: for example, when NAT or a firewall interferes.</simpara>
</listitem>
<listitem>
<simpara>Provides exactly once semantics (except for reboot operations), preventing a previous issue whereby certain operations sometimes returned an inaccurate result if a reply was lost and the operation was sent twice.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="services-required-by-nfs_mounting-nfs-shares">
<title>Services required by NFS</title>
<simpara>This section lists system services that are required for running an NFS server or mounting NFS shares. Red Hat Enterprise Linux starts these services automatically.</simpara>
<simpara>Red Hat Enterprise Linux uses a combination of kernel-level support and service processes to provide NFS file sharing. All NFS versions rely on Remote Procedure Calls (RPC) between clients and servers. To share or mount NFS file systems, the following services work together depending on which version of NFS is implemented:</simpara>
<variablelist>
<varlistentry>
<term><literal>nfsd</literal></term>
<listitem>
<simpara>The NFS server kernel module that services requests for shared NFS file systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpcbind</literal></term>
<listitem>
<simpara>Accepts port reservations from local RPC services. These ports are then made available (or advertised) so the corresponding remote RPC services can access them. The <literal>rpcbind</literal> service responds to requests for RPC services and sets up connections to the requested RPC service. This is not used with NFSv4.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.mountd</literal></term>
<listitem>
<simpara>This process is used by an NFS server to process <literal>MOUNT</literal> requests from NFSv3 clients. It checks that the requested NFS share is currently exported by the NFS server, and that the client is allowed to access it. If the mount request is allowed, the <literal>nfs-mountd</literal> service replies with a Success status and provides the File-Handle for this NFS share back to the NFS client.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.nfsd</literal></term>
<listitem>
<simpara>This process enables explicit NFS versions and protocols the server advertises to be defined. It works with the Linux kernel to meet the dynamic demands of NFS clients, such as providing server threads each time an NFS client connects. This process corresponds to the <literal>nfs-server</literal> service.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>lockd</literal></term>
<listitem>
<simpara>This is a kernel thread that runs on both clients and servers. It implements the Network Lock Manager (NLM) protocol, which enables NFSv3 clients to lock files on the server. It is started automatically whenever the NFS server is run and whenever an NFS file system is mounted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.statd</literal></term>
<listitem>
<simpara>This process implements the Network Status Monitor (NSM) RPC protocol, which notifies NFS clients when an NFS server is restarted without being gracefully brought down. The <literal>rpc-statd</literal> service is started automatically by the <literal>nfs-server</literal> service, and does not require user configuration. This is not used with NFSv4.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.rquotad</literal></term>
<listitem>
<simpara>This process provides user quota information for remote users. The <literal>rpc-rquotad</literal> service is started automatically by the <literal>nfs-server</literal> service and does not require user configuration.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.idmapd</literal></term>
<listitem>
<simpara>This process provides NFSv4 client and server upcalls, which map between on-the-wire NFSv4 names (strings in the form of <literal><emphasis>user</emphasis>@<emphasis>domain</emphasis></literal>) and local UIDs and GIDs. For <literal>idmapd</literal> to function with NFSv4, the <literal role="filename">/etc/idmapd.conf</literal> file must be configured. At a minimum, the <literal>Domain</literal> parameter should be specified, which defines the NFSv4 mapping domain. If the NFSv4 mapping domain is the same as the DNS domain name, this parameter can be skipped. The client and server must agree on the NFSv4 mapping domain for ID mapping to function properly.</simpara>
<simpara>Only the NFSv4 server uses <literal>rpc.idmapd</literal>, which is started by the <literal>nfs-idmapd</literal> service. The NFSv4 client uses the keyring-based <literal>nfsidmap</literal> utility, which is called by the kernel on-demand to perform ID mapping. If there is a problem with <literal>nfsidmap</literal>, the client falls back to using <literal>rpc.idmapd</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<bridgehead xml:id="the_rpc_services_with_nfsv4" renderas="sect3" remap="_the_rpc_services_with_nfsv4">The RPC services with NFSv4</bridgehead>
<simpara>The mounting and locking protocols have been incorporated into the NFSv4 protocol. The server also listens on the well-known TCP port 2049. As such, NFSv4 does not need to interact with <literal>rpcbind</literal>, <literal>lockd</literal>, and <literal>rpc-statd</literal> services. The <literal>nfs-mountd</literal> service is still required on the NFS server to set up the exports, but is not involved in any over-the-wire operations.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>To configure an NFSv4-only server, which does not require <literal>rpcbind</literal>, see <xref linkend="configuring-an-nfsv4-only-server_exporting-nfs-shares"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nfs-host-name-formats_mounting-nfs-shares">
<title>NFS host name formats</title>
<simpara>This section describes different formats that you can use to specify a host when mounting or exporting an NFS share.</simpara>
<simpara>You can specify the host in the following formats:</simpara>
<variablelist>
<varlistentry>
<term>Single machine</term>
<listitem>
<simpara>Either of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>A fully-qualified domain name (that can be resolved by the server)</simpara>
</listitem>
<listitem>
<simpara>Host name (that can be resolved by the server)</simpara>
</listitem>
<listitem>
<simpara>An IP address.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term>IP networks</term>
<listitem>
<simpara>Either of the following formats is valid:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis><phrase role="replaceable"><literal>a.b.c.d/z</literal></phrase></emphasis>, where <emphasis><phrase role="replaceable"><literal>a.b.c.d</literal></phrase></emphasis> is the network and <emphasis><phrase role="replaceable"><literal>z</literal></phrase></emphasis> is the number of bits in the netmask; for example <literal>192.168.0.0/24</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis><phrase role="replaceable"><literal>a.b.c.d/netmask</literal></phrase></emphasis>, where <emphasis><phrase role="replaceable"><literal>a.b.c.d</literal></phrase></emphasis> is the network and <emphasis><phrase role="replaceable"><literal>netmask</literal></phrase></emphasis> is the netmask; for example, <literal>192.168.100.8/255.255.255.0</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Netgroups</term>
<listitem>
<simpara>The <literal>@<emphasis>group-name</emphasis></literal> format , where <emphasis><phrase role="replaceable"><literal>group-name</literal></phrase></emphasis> is the NIS netgroup name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="installing-nfs_mounting-nfs-shares">
<title>Installing NFS</title>
<simpara>This procedure installs all packages necessary to mount or export NFS shares.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Install the <literal>nfs-utils</literal> package:</simpara>
<screen># yum install nfs-utils</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="discovering-nfs-exports_mounting-nfs-shares">
<title>Discovering NFS exports</title>
<simpara>This procedure discovers which file systems a given NFSv3 or NFSv4 server exports.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>With any server that supports NFSv3, use the <literal>showmount</literal> utility:</simpara>
<screen>$ showmount --exports <emphasis><phrase role="replaceable">my-server</phrase></emphasis>

Export list for <emphasis><phrase role="replaceable">my-server</phrase></emphasis>
<emphasis><phrase role="replaceable">/exports/foo</phrase></emphasis>
<emphasis><phrase role="replaceable">/exports/bar</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>With any server that supports NFSv4, mount the root directory and look around:</simpara>
<screen># mount <emphasis><phrase role="replaceable">my-server</phrase></emphasis>:/ /mnt/
# ls /mnt/

<emphasis><phrase role="replaceable">exports</phrase></emphasis>

# ls /mnt/<emphasis><phrase role="replaceable">exports/</phrase></emphasis>

<emphasis><phrase role="replaceable">foo</phrase></emphasis>
<emphasis><phrase role="replaceable">bar</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<simpara>On servers that support both NFSv4 and NFSv3, both methods work and give the same results.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>showmount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mounting-an-nfs-share-with-mount_mounting-nfs-shares">
<title>Mounting an NFS share with mount</title>
<simpara>This procedure mounts an NFS share exported from a server using the <literal>mount</literal> utility.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To mount an NFS share, use the following command:</simpara>
<screen># mount -t nfs -o <emphasis><phrase role="replaceable">options</phrase></emphasis> <emphasis><phrase role="replaceable">host</phrase></emphasis>:<emphasis><phrase role="replaceable">/remote/export</phrase></emphasis> <emphasis><phrase role="replaceable">/local/directory</phrase></emphasis></screen>
<simpara>This command uses the following variables:</simpara>
<variablelist>
<varlistentry>
<term><emphasis><phrase role="replaceable">options</phrase></emphasis></term>
<listitem>
<simpara>A comma-delimited list of mount options.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">host</phrase></emphasis></term>
<listitem>
<simpara>The host name, IP address, or fully qualified domain name of the server exporting the file system you wish to mount.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">/remote/export</phrase></emphasis></term>
<listitem>
<simpara>The file system or directory being exported from the server, that is, the directory you wish to mount.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">/local/directory</phrase></emphasis></term>
<listitem>
<simpara>The client location where <emphasis><phrase role="replaceable">/remote/export</phrase></emphasis> is mounted.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><xref linkend="common-nfs-mount-options_mounting-nfs-shares"/></simpara>
</listitem>
<listitem>
<simpara><xref linkend="nfs-host-name-formats_mounting-nfs-shares"/></simpara>
</listitem>
<listitem>
<simpara><xref linkend="mounting-a-file-system-with-mount_assembly_mounting-file-systems"/></simpara>
</listitem>
<listitem>
<simpara>The <literal>mount(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="common-nfs-mount-options_mounting-nfs-shares">
<title>Common NFS mount options</title>
<simpara>This section lists options commonly used when mounting NFS shares. These options can be used with manual mount commands, <literal role="filename">/etc/fstab</literal> settings, and <literal>autofs</literal>.</simpara>
<variablelist>
<varlistentry>
<term><literal role="option">lookupcache=<emphasis><phrase role="replaceable">mode</phrase></emphasis></literal></term>
<listitem>
<simpara>Specifies how the kernel should manage its cache of directory entries for a given mount point. Valid arguments for <emphasis><phrase role="replaceable">mode</phrase></emphasis> are <literal>all</literal>, <literal>none</literal>, or <literal>positive</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">nfsvers=<emphasis><phrase role="replaceable">version</phrase></emphasis></literal></term>
<listitem>
<simpara>Specifies which version of the NFS protocol to use, where <emphasis><phrase role="replaceable">version</phrase></emphasis> is <literal>3</literal>, <literal>4</literal>, <literal>4.0</literal>, <literal>4.1</literal>, or <literal>4.2</literal>. This is useful for hosts that run multiple NFS servers, or to disable retrying a mount with lower versions. If no version is specified, NFS uses the highest version supported by the kernel and the <literal>mount</literal> utility.</simpara>
<simpara>The option <literal role="option">vers</literal> is identical to <literal role="option">nfsvers</literal>, and is included in this release for compatibility reasons.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">noacl</literal></term>
<listitem>
<simpara>Turns off all ACL processing. This may be needed when interfacing with older versions of Red Hat Enterprise Linux, Red Hat Linux, or Solaris, because the most recent ACL technology is not compatible with older systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">nolock</literal></term>
<listitem>
<simpara>Disables file locking. This setting is sometimes required when connecting to very old NFS servers.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">noexec</literal></term>
<listitem>
<simpara>Prevents execution of binaries on mounted file systems. This is useful if the system is mounting a non-Linux file system containing incompatible binaries.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">nosuid</literal></term>
<listitem>
<simpara>Disables the <literal>set-user-identifier</literal> and <literal>set-group-identifier</literal> bits. This prevents remote users from gaining higher privileges by running a <literal>setuid</literal> program.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">port=<emphasis><phrase role="replaceable">num</phrase></emphasis></literal></term>
<listitem>
<simpara>Specifies the numeric value of the NFS server port. If <emphasis><phrase role="replaceable">num</phrase></emphasis> is <literal>0</literal> (the default value), then <literal>mount</literal> queries the <literal>rpcbind</literal> service on the remote host for the port number to use. If the NFS service on the remote host is not registered with its <literal>rpcbind</literal> service, the standard NFS port number of TCP 2049 is used instead.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">rsize=<emphasis><phrase role="replaceable">num</phrase></emphasis></literal> and <literal role="option">wsize=<emphasis><phrase role="replaceable">num</phrase></emphasis></literal></term>
<listitem>
<simpara>These options set the maximum number of bytes to be transferred in a single NFS read or write operation.</simpara>
<simpara>There is no fixed default value for <literal>rsize</literal> and <literal>wsize</literal>. By default, NFS uses the largest possible value that both the server and the client support. In Red Hat Enterprise Linux 8, the client and server maximum is 1,048,576 bytes. For more details, see the <link xlink:href="https://access.redhat.com/solutions/753853">What are the default and maximum values for rsize and wsize with NFS mounts?</link> KBase article.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">sec=<emphasis><phrase role="replaceable">flavors</phrase></emphasis></literal></term>
<listitem>
<simpara>Security flavors to use for accessing files on the mounted export. The <emphasis><phrase role="replaceable">flavors</phrase></emphasis> value is a colon-separated list of one or more security flavors.</simpara>
<simpara>By default, the client attempts to find a security flavor that both the client and the server support. If the server does not support any of the selected flavors, the mount operation fails.</simpara>
<simpara>Available flavors:</simpara>
<itemizedlist>
<listitem>
<simpara><literal role="option">sec=sys</literal> uses local UNIX UIDs and GIDs. These use <literal>AUTH_SYS</literal> to authenticate NFS operations.</simpara>
</listitem>
<listitem>
<simpara><literal role="option">sec=krb5</literal> uses Kerberos V5 instead of local UNIX UIDs and GIDs to authenticate users.</simpara>
</listitem>
<listitem>
<simpara><literal role="option">sec=krb5i</literal> uses Kerberos V5 for user authentication and performs integrity checking of NFS operations using secure checksums to prevent data tampering.</simpara>
</listitem>
<listitem>
<simpara><literal role="option">sec=krb5p</literal> uses Kerberos V5 for user authentication, integrity checking, and encrypts NFS traffic to prevent traffic sniffing. This is the most secure setting, but it also involves the most performance overhead.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">tcp</literal></term>
<listitem>
<simpara>Instructs the NFS mount to use the TCP protocol.</simpara>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page</simpara>
</listitem>
<listitem>
<simpara>The <literal>nfs(5)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-mounting-nfs-shares">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The Linux NFS wiki: <link xlink:href="https://linux-nfs.org/wiki/index.php/Main_Page">https://linux-nfs.org/wiki/index.php/Main_Page</link></simpara>
</listitem>
<listitem>
<simpara>To mount NFS shares persistently, see <xref linkend="assembly_persistently-mounting-file-systems_assembly_mounting-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>To mount NFS shares on demand, see <xref linkend="assembly_mounting-file-systems-on-demand_assembly_mounting-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="exporting-nfs-shares_managing-file-systems">
<title>Exporting NFS shares</title>
<simpara>As a system administrator, you can use the NFS server to share a directory on your system over network.</simpara>
<section xml:id="introduction-to-nfs_exporting-nfs-shares">
<title>Introduction to NFS</title>
<simpara>This section explains the basic concepts of the NFS service.</simpara>
<simpara>A Network File System (NFS) allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally. This enables you to consolidate resources onto centralized servers on the network.</simpara>
<simpara>The NFS server refers to the <literal role="filename">/etc/exports</literal> configuration file to determine whether the client is allowed to access any exported file systems. Once verified, all file and directory operations are available to the user.</simpara>
</section>
<section xml:id="supported-nfs-versions_exporting-nfs-shares">
<title>Supported NFS versions</title>
<simpara>This section lists versions of NFS supported in Red Hat Enterprise Linux and their features.</simpara>
<simpara>Currently, Red Hat Enterprise Linux 8 supports the following major versions of NFS:</simpara>
<itemizedlist>
<listitem>
<simpara>NFS version 3 (NFSv3) supports safe asynchronous writes and is more robust at error handling than the previous NFSv2; it also supports 64-bit file sizes and offsets, allowing clients to access more than 2 GB of file data.</simpara>
</listitem>
<listitem>
<simpara>NFS version 4 (NFSv4) works through firewalls and on the Internet, no longer requires an <literal>rpcbind</literal> service, supports Access Control Lists (ACLs), and utilizes stateful operations.</simpara>
</listitem>
</itemizedlist>
<simpara>NFS version 2 (NFSv2) is no longer supported by Red Hat.</simpara>
<bridgehead xml:id="default_nfs_version_2" renderas="sect3" remap="_default_nfs_version_2">Default NFS version</bridgehead>
<simpara>The default NFS version in Red Hat Enterprise Linux 8 is 4.2. NFS clients attempt to mount using NFSv4.2 by default, and fall back to NFSv4.1 when the server does not support NFSv4.2. The mount later falls back to NFSv4.0 and then to NFSv3.</simpara>
<bridgehead xml:id="features_of_minor_nfs_versions_2" renderas="sect3" remap="_features_of_minor_nfs_versions_2">Features of minor NFS versions</bridgehead>
<simpara>Following are the features of NFSv4.2 in Red Hat Enterprise Linux 8:</simpara>
<variablelist>
<varlistentry>
<term>Server-side copy</term>
<listitem>
<simpara>Enables the NFS client to efficiently copy data without wasting network resources using the <literal>copy_file_range()</literal> system call.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Sparse files</term>
<listitem>
<simpara>Enables files to have one or more <emphasis>holes</emphasis>, which are unallocated or uninitialized data blocks consisting only of zeroes. The <literal>lseek()</literal> operation in NFSv4.2 supports <literal>seek_hole()</literal> and <literal>seek_data()</literal>, which enables applications to map out the location of holes in the sparse file.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Space reservation</term>
<listitem>
<simpara>Permits storage servers to reserve free space, which prohibits servers to run out of space. NFSv4.2 supports the <literal>allocate()</literal> operation to reserve space, the <literal>deallocate()</literal> operation to unreserve space, and the <literal>fallocate()</literal> operation to preallocate or deallocate space in a file.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Labeled NFS</term>
<listitem>
<simpara>Enforces data access rights and enables SELinux labels between a client and a server for individual files on an NFS file system.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Layout enhancements</term>
<listitem>
<simpara>Provides the <literal>layoutstats()</literal> operation, which enables some Parallel NFS (pNFS) servers to collect better performance statistics.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Following are the features of NFSv4.1:</simpara>
<itemizedlist>
<listitem>
<simpara>Enhances performance and security of network, and also includes client-side support for pNFS.</simpara>
</listitem>
<listitem>
<simpara>No longer requires a separate TCP connection for callbacks, which allows an NFS server to grant delegations even when it cannot contact the client: for example, when NAT or a firewall interferes.</simpara>
</listitem>
<listitem>
<simpara>Provides exactly once semantics (except for reboot operations), preventing a previous issue whereby certain operations sometimes returned an inaccurate result if a reply was lost and the operation was sent twice.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="the-tcp-and-udp-protocols-in-nfsv3-and-nfsv4_exporting-nfs-shares">
<title>The TCP and UDP protocols in NFSv3 and NFSv4</title>
<simpara>NFSv4 requires the Transmission Control Protocol (TCP) running over an IP network.</simpara>
<simpara>NFSv3 could also use the User Datagram Protocol (UDP) in earlier Red Hat Enterprise Linux versions. In Red Hat Enterprise Linux 8, NFS over UDP is no longer supported. By default, UDP is disabled in the NFS server.</simpara>
</section>
<section xml:id="services-required-by-nfs_exporting-nfs-shares">
<title>Services required by NFS</title>
<simpara>This section lists system services that are required for running an NFS server or mounting NFS shares. Red Hat Enterprise Linux starts these services automatically.</simpara>
<simpara>Red Hat Enterprise Linux uses a combination of kernel-level support and service processes to provide NFS file sharing. All NFS versions rely on Remote Procedure Calls (RPC) between clients and servers. To share or mount NFS file systems, the following services work together depending on which version of NFS is implemented:</simpara>
<variablelist>
<varlistentry>
<term><literal>nfsd</literal></term>
<listitem>
<simpara>The NFS server kernel module that services requests for shared NFS file systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpcbind</literal></term>
<listitem>
<simpara>Accepts port reservations from local RPC services. These ports are then made available (or advertised) so the corresponding remote RPC services can access them. The <literal>rpcbind</literal> service responds to requests for RPC services and sets up connections to the requested RPC service. This is not used with NFSv4.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.mountd</literal></term>
<listitem>
<simpara>This process is used by an NFS server to process <literal>MOUNT</literal> requests from NFSv3 clients. It checks that the requested NFS share is currently exported by the NFS server, and that the client is allowed to access it. If the mount request is allowed, the <literal>nfs-mountd</literal> service replies with a Success status and provides the File-Handle for this NFS share back to the NFS client.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.nfsd</literal></term>
<listitem>
<simpara>This process enables explicit NFS versions and protocols the server advertises to be defined. It works with the Linux kernel to meet the dynamic demands of NFS clients, such as providing server threads each time an NFS client connects. This process corresponds to the <literal>nfs-server</literal> service.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>lockd</literal></term>
<listitem>
<simpara>This is a kernel thread that runs on both clients and servers. It implements the Network Lock Manager (NLM) protocol, which enables NFSv3 clients to lock files on the server. It is started automatically whenever the NFS server is run and whenever an NFS file system is mounted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.statd</literal></term>
<listitem>
<simpara>This process implements the Network Status Monitor (NSM) RPC protocol, which notifies NFS clients when an NFS server is restarted without being gracefully brought down. The <literal>rpc-statd</literal> service is started automatically by the <literal>nfs-server</literal> service, and does not require user configuration. This is not used with NFSv4.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.rquotad</literal></term>
<listitem>
<simpara>This process provides user quota information for remote users. The <literal>rpc-rquotad</literal> service is started automatically by the <literal>nfs-server</literal> service and does not require user configuration.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>rpc.idmapd</literal></term>
<listitem>
<simpara>This process provides NFSv4 client and server upcalls, which map between on-the-wire NFSv4 names (strings in the form of <literal><emphasis>user</emphasis>@<emphasis>domain</emphasis></literal>) and local UIDs and GIDs. For <literal>idmapd</literal> to function with NFSv4, the <literal role="filename">/etc/idmapd.conf</literal> file must be configured. At a minimum, the <literal>Domain</literal> parameter should be specified, which defines the NFSv4 mapping domain. If the NFSv4 mapping domain is the same as the DNS domain name, this parameter can be skipped. The client and server must agree on the NFSv4 mapping domain for ID mapping to function properly.</simpara>
<simpara>Only the NFSv4 server uses <literal>rpc.idmapd</literal>, which is started by the <literal>nfs-idmapd</literal> service. The NFSv4 client uses the keyring-based <literal>nfsidmap</literal> utility, which is called by the kernel on-demand to perform ID mapping. If there is a problem with <literal>nfsidmap</literal>, the client falls back to using <literal>rpc.idmapd</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<bridgehead xml:id="the_rpc_services_with_nfsv4_2" renderas="sect3" remap="_the_rpc_services_with_nfsv4_2">The RPC services with NFSv4</bridgehead>
<simpara>The mounting and locking protocols have been incorporated into the NFSv4 protocol. The server also listens on the well-known TCP port 2049. As such, NFSv4 does not need to interact with <literal>rpcbind</literal>, <literal>lockd</literal>, and <literal>rpc-statd</literal> services. The <literal>nfs-mountd</literal> service is still required on the NFS server to set up the exports, but is not involved in any over-the-wire operations.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>To configure an NFSv4-only server, which does not require <literal>rpcbind</literal>, see <xref linkend="configuring-an-nfsv4-only-server_exporting-nfs-shares"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nfs-host-name-formats_exporting-nfs-shares">
<title>NFS host name formats</title>
<simpara>This section describes different formats that you can use to specify a host when mounting or exporting an NFS share.</simpara>
<simpara>You can specify the host in the following formats:</simpara>
<variablelist>
<varlistentry>
<term>Single machine</term>
<listitem>
<simpara>Either of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>A fully-qualified domain name (that can be resolved by the server)</simpara>
</listitem>
<listitem>
<simpara>Host name (that can be resolved by the server)</simpara>
</listitem>
<listitem>
<simpara>An IP address.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term>IP networks</term>
<listitem>
<simpara>Either of the following formats is valid:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis><phrase role="replaceable"><literal>a.b.c.d/z</literal></phrase></emphasis>, where <emphasis><phrase role="replaceable"><literal>a.b.c.d</literal></phrase></emphasis> is the network and <emphasis><phrase role="replaceable"><literal>z</literal></phrase></emphasis> is the number of bits in the netmask; for example <literal>192.168.0.0/24</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis><phrase role="replaceable"><literal>a.b.c.d/netmask</literal></phrase></emphasis>, where <emphasis><phrase role="replaceable"><literal>a.b.c.d</literal></phrase></emphasis> is the network and <emphasis><phrase role="replaceable"><literal>netmask</literal></phrase></emphasis> is the netmask; for example, <literal>192.168.100.8/255.255.255.0</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Netgroups</term>
<listitem>
<simpara>The <literal>@<emphasis>group-name</emphasis></literal> format , where <emphasis><phrase role="replaceable"><literal>group-name</literal></phrase></emphasis> is the NIS netgroup name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nfs-server-configuration_exporting-nfs-shares">
<title>NFS server configuration</title>
<simpara>This section describes the syntax and options of two ways to configure exports on an NFS server:</simpara>
<itemizedlist>
<listitem>
<simpara>Manually editing the <literal role="filename">/etc/exports</literal> configuration file</simpara>
</listitem>
<listitem>
<simpara>Using the <literal>exportfs</literal> utility on the command line</simpara>
</listitem>
</itemizedlist>
<section xml:id="the-etc-exports-configuration-file_exporting-nfs-shares">
<title>The /etc/exports configuration file</title>
<simpara>The <literal role="filename">/etc/exports</literal> file controls which file systems are exported to remote hosts and specifies options. It follows the following syntax rules:</simpara>
<itemizedlist>
<listitem>
<simpara>Blank lines are ignored.</simpara>
</listitem>
<listitem>
<simpara>To add a comment, start a line with the hash mark (<literal>#</literal>).</simpara>
</listitem>
<listitem>
<simpara>You can wrap long lines with a backslash (<literal>\</literal>).</simpara>
</listitem>
<listitem>
<simpara>Each exported file system should be on its own individual line.</simpara>
</listitem>
<listitem>
<simpara>Any lists of authorized hosts placed after an exported file system must be separated by space characters.</simpara>
</listitem>
<listitem>
<simpara>Options for each of the hosts must be placed in parentheses directly after the host identifier, without any spaces separating the host and the first parenthesis.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="export_entry" renderas="sect4" remap="_export_entry">Export entry</bridgehead>
<simpara>Each entry for an exported file system has the following structure:</simpara>
<screen><emphasis><phrase role="replaceable">export</phrase></emphasis> <emphasis><phrase role="replaceable">host</phrase></emphasis>(<emphasis><phrase role="replaceable">options</phrase></emphasis>)</screen>
<simpara>It is also possible to specify multiple hosts, along with specific options for each host. To do so, list them on the same line as a space-delimited list, with each host name followed by its respective options (in parentheses), as in:</simpara>
<screen><emphasis><phrase role="replaceable">export</phrase></emphasis> <emphasis><phrase role="replaceable">host1</phrase></emphasis>(<emphasis><phrase role="replaceable">options1</phrase></emphasis>) <emphasis><phrase role="replaceable">host2</phrase></emphasis>(<emphasis><phrase role="replaceable">options2</phrase></emphasis>) <emphasis><phrase role="replaceable">host3</phrase></emphasis>(<emphasis><phrase role="replaceable">options3</phrase></emphasis>)</screen>
<simpara>In this structure:</simpara>
<variablelist>
<varlistentry>
<term><emphasis><phrase role="replaceable">export</phrase></emphasis></term>
<listitem>
<simpara>The directory being exported</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">host</phrase></emphasis></term>
<listitem>
<simpara>The host or network to which the export is being shared</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">options</phrase></emphasis></term>
<listitem>
<simpara>The options to be used for host</simpara>
</listitem>
</varlistentry>
</variablelist>
<example>
<title>A simple /etc/exports file</title>
<simpara>In its simplest form, the <literal role="filename">/etc/exports</literal> file only specifies the exported directory and the hosts permitted to access it:</simpara>
<screen>/exported/directory bob.example.com</screen>
<simpara>Here, <literal>bob.example.com</literal> can mount <literal role="filename">/exported/directory/</literal> from the NFS server. Because no options are specified in this example, NFS uses default options.</simpara>
</example>
<important>
<simpara>The format of the <literal role="filename">/etc/exports</literal> file is very precise, particularly in regards to use of the space character. Remember to always separate exported file systems from hosts and hosts from one another with a space character. However, there should be no other space characters in the file except on comment lines.</simpara>
<simpara>For example, the following two lines do not mean the same thing:</simpara>
<screen>/home bob.example.com(rw)
/home bob.example.com (rw)</screen>
<simpara>The first line allows only users from <literal>bob.example.com</literal> read and write access to the <literal role="filename">/home</literal> directory. The second line allows users from <literal>bob.example.com</literal> to mount the directory as read-only (the default), while the rest of the world can mount it read/write.</simpara>
</important>
<bridgehead xml:id="default_options" renderas="sect4" remap="_default_options">Default options</bridgehead>
<simpara>The default options for an export entry are:</simpara>
<variablelist>
<varlistentry>
<term><literal role="option">ro</literal></term>
<listitem>
<simpara>The exported file system is read-only. Remote hosts cannot change the data shared on the file system. To allow hosts to make changes to the file system (that is, read and write), specify the rw option.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="options">sync</literal></term>
<listitem>
<simpara>The NFS server will not reply to requests before changes made by previous requests are written to disk. To enable asynchronous writes instead, specify the option <literal role="option">async</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">wdelay</literal></term>
<listitem>
<simpara>The NFS server will delay writing to the disk if it suspects another write request is imminent. This can improve performance as it reduces the number of times the disk must be accessed by separate write commands, thereby reducing write overhead. To disable this, specify the <literal role="option">no_wdelay</literal> option, which is available only if the default sync option is also specified.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">root_squash</literal></term>
<listitem>
<simpara>This prevents root users connected remotely (as opposed to locally) from having root privileges; instead, the NFS server assigns them the user ID <literal>nobody</literal>. This effectively "squashes" the power of the remote root user to the lowest local user, preventing possible unauthorized writes on the remote server. To disable root squashing, specify the <literal role="option">no_root_squash</literal> option.</simpara>
<simpara>To squash every remote user (including root), use the <literal role="option">all_squash</literal> option. To specify the user and group IDs that the NFS server should assign to remote users from a particular host, use the <literal role="option">anonuid</literal> and <literal role="option">anongid</literal> options, respectively, as in:</simpara>
<screen><emphasis><phrase role="replaceable">export</phrase></emphasis> <emphasis><phrase role="replaceable">host</phrase></emphasis>(anonuid=<emphasis><phrase role="replaceable">uid</phrase></emphasis>,anongid=<emphasis><phrase role="replaceable">gid</phrase></emphasis>)</screen>
<simpara>Here, <emphasis><phrase role="replaceable">uid</phrase></emphasis> and <emphasis><phrase role="replaceable">gid</phrase></emphasis> are user ID number and group ID number, respectively. The <literal role="option">anonuid</literal> and <literal role="option">anongid</literal> options enable you to create a special user and group account for remote NFS users to share.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>By default, access control lists (ACLs) are supported by NFS under Red Hat Enterprise Linux. To disable this feature, specify the <literal role="option">no_acl</literal> option when exporting the file system.</simpara>
<bridgehead xml:id="default_and_overridden_options" renderas="sect4" remap="_default_and_overridden_options">Default and overridden options</bridgehead>
<simpara>Each default for every exported file system must be explicitly overridden. For example, if the <literal>rw</literal> option is not specified, then the exported file system is shared as read-only. The following is a sample line from <literal role="filename">/etc/exports</literal> which overrides two default options:</simpara>
<screen>/another/exported/directory 192.168.0.3(rw,async)</screen>
<simpara>In this example, <literal>192.168.0.3</literal> can mount <literal role="filename">/another/exported/directory/</literal> read and write, and all writes to disk are asynchronous.</simpara>
</section>
<section xml:id="the-exportfs-utility_exporting-nfs-shares">
<title>The exportfs utility</title>
<simpara>The <literal>exportfs</literal> utility enables the root user to selectively export or unexport directories without restarting the NFS service. When given the proper options, the <literal>exportfs</literal> utility writes the exported file systems to <literal role="filename">/var/lib/nfs/xtab</literal>. Because the <literal>nfs-mountd</literal> service refers to the <literal>xtab</literal> file when deciding access privileges to a file system, changes to the list of exported file systems take effect immediately.</simpara>
<bridgehead xml:id="common_exportfs_options" renderas="sect4" remap="_common_exportfs_options">Common exportfs options</bridgehead>
<simpara>The following is a list of commonly-used options available for <literal>exportfs</literal>:</simpara>
<variablelist>
<varlistentry>
<term><literal role="option">-r</literal></term>
<listitem>
<simpara>Causes all directories listed in <literal role="filename">/etc/exports</literal> to be exported by constructing a new export list in <literal role="filename">/var/lib/nfs/etab</literal>. This option effectively refreshes the export list with any changes made to <literal role="filename">/etc/exports</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">-a</literal></term>
<listitem>
<simpara>Causes all directories to be exported or unexported, depending on what other options are passed to <literal>exportfs</literal>. If no other options are specified, <literal>exportfs</literal> exports all file systems specified in <literal role="filename">/etc/exports</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">-o <emphasis><phrase role="replaceable">file-systems</phrase></emphasis></literal></term>
<listitem>
<simpara>Specifies directories to be exported that are not listed in <literal role="filename">/etc/exports</literal>. Replace <emphasis><phrase role="replaceable">file-systems</phrase></emphasis> with additional file systems to be exported. These file systems must be formatted in the same way they are specified in <literal role="filename">/etc/exports</literal>. This option is often used to test an exported file system before adding it permanently to the list of exported file systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">-i</literal></term>
<listitem>
<simpara>Ignores <literal role="filename">/etc/exports</literal>; only options given from the command line are used to define exported file systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">-u</literal></term>
<listitem>
<simpara>Unexports all shared directories. The command <literal role="command">exportfs -ua</literal> suspends NFS file sharing while keeping all NFS services up. To re-enable NFS sharing, use <literal role="command">exportfs -r</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">-v</literal></term>
<listitem>
<simpara>Verbose operation, where the file systems being exported or unexported are displayed in greater detail when the <literal>exportfs</literal> command is executed.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>If no options are passed to the <literal>exportfs</literal> utility, it displays a list of currently exported file systems.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For information on different methods for specifying host names, see <xref linkend="nfs-host-name-formats_exporting-nfs-shares"/>.</simpara>
</listitem>
<listitem>
<simpara>For a complete list of export options, see the <literal>exports(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>For more information about the <literal>exportfs</literal> utility, see the <literal>exportfs(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nfs-and-rpcbind_exporting-nfs-shares">
<title>NFS and rpcbind</title>
<simpara>This section explains the purpose of the <literal>rpcbind</literal> service, which is required by NFSv3.</simpara>
<simpara>The <literal>rpcbind</literal> service maps Remote Procedure Call (RPC) services to the ports on which they listen. RPC processes notify <literal>rpcbind</literal> when they start, registering the ports they are listening on and the RPC program numbers they expect to serve. The client system then contacts <literal>rpcbind</literal> on the server with a particular RPC program number. The <literal>rpcbind</literal> service redirects the client to the proper port number so it can communicate with the requested service.</simpara>
<simpara>Because RPC-based services rely on <literal>rpcbind</literal> to make all connections with incoming client requests, <literal>rpcbind</literal> must be available before any of these services start.</simpara>
<simpara>Access control rules for <literal>rpcbind</literal> affect all RPC-based services. Alternatively, it is possible to specify access control rules for each of the NFS RPC daemons.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For the precise syntax of access control rules, see the <literal>rpc.mountd(8)</literal> and <literal>rpc.statd(8)</literal> man pages.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-nfs_exporting-nfs-shares">
<title>Installing NFS</title>
<simpara>This procedure installs all packages necessary to mount or export NFS shares.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Install the <literal>nfs-utils</literal> package:</simpara>
<screen># yum install nfs-utils</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="starting-the-nfs-server_exporting-nfs-shares">
<title>Starting the NFS server</title>
<simpara>This procedure describes how to start the NFS server, which is required to export NFS shares.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>For servers that support NFSv2 or NFSv3 connections, the <literal>rpcbind</literal> service must be running. To verify that <literal>rpcbind</literal> is active, use the following command:</simpara>
<screen>$ systemctl status rpcbind</screen>
<simpara>If the service is stopped, start and enable it:</simpara>
<screen>$ systemctl enable --now rpcbind</screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To start the NFS server and enable it to start automatically at boot, use the following command:</simpara>
<screen># systemctl enable --now nfs-server</screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>To configure an NFSv4-only server, which does not require <literal>rpcbind</literal>, see <xref linkend="configuring-an-nfsv4-only-server_exporting-nfs-shares"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="troubleshooting-nfs-and-rpcbind_exporting-nfs-shares">
<title>Troubleshooting NFS and rpcbind</title>
<simpara>Because the <literal>rpcbind</literal> service provides coordination between RPC services and the port numbers used to communicate with them, it is useful to view the status of current RPC services using <literal>rpcbind</literal> when troubleshooting. The <literal>rpcinfo</literal> utility shows each RPC-based service with port numbers, an RPC program number, a version number, and an IP protocol type (TCP or UDP).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To make sure the proper NFS RPC-based services are enabled for <literal>rpcbind</literal>, use the following command:</simpara>
<screen># rpcinfo -p</screen>
<example>
<title>rpcinfo -p command output</title>
<simpara>The following is sample output from this command:</simpara>
<screen>   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100024    1   udp  37769  status
    100024    1   tcp  49349  status
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100021    1   udp  56691  nlockmgr
    100021    3   udp  56691  nlockmgr
    100021    4   udp  56691  nlockmgr
    100021    1   tcp  46193  nlockmgr
    100021    3   tcp  46193  nlockmgr
    100021    4   tcp  46193  nlockmgr</screen>
</example>
<simpara>If one of the NFS services does not start up correctly, <literal>rpcbind</literal> will be unable to map RPC requests from clients for that service to the correct port.</simpara>
</listitem>
<listitem>
<simpara>In many cases, if NFS is not present in <literal>rpcinfo</literal> output, restarting NFS causes the service to correctly register with <literal>rpcbind</literal> and begin working:</simpara>
<screen># systemctl restart nfs-server</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information and a list of <literal>rpcinfo</literal> options, see the <literal>rpcinfo(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>To configure an NFSv4-only server, which does not require <literal>rpcbind</literal>, see <xref linkend="configuring-an-nfsv4-only-server_exporting-nfs-shares"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-the-nfs-server-to-run-behind-a-firewall_exporting-nfs-shares">
<title>Configuring the NFS server to run behind a firewall</title>
<simpara>NFS requires the <literal>rpcbind</literal> service, which dynamically assigns ports for RPC services and can cause issues for configuring firewall rules. This procedure describes how to configure the NFS server to work behind a firewall.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To allow clients to access NFS shares behind a firewall, set which ports the RPC services run on in the <literal>[mountd]</literal> section of the <literal role="filename">/etc/nfs.conf</literal> file:</simpara>
<screen>[mountd]

port=<emphasis><phrase role="replaceable">port-number</phrase></emphasis></screen>
<simpara>This adds the <literal role="option">-p <emphasis><phrase role="replaceable">port-number</phrase></emphasis></literal> option to the <literal>rpc.mount</literal> command line: <literal role="command">rpc.mount -p <emphasis><phrase role="replaceable">port-number</phrase></emphasis></literal>.</simpara>
</listitem>
<listitem>
<simpara>To allow clients to access NFS shares behind a firewall, configure the firewall by running the following commands on the NFS server:</simpara>
<screen>firewall-cmd --permanent --add-service mountd
firewall-cmd --permanent --add-service rpc-bind
firewall-cmd --permanent --add-service nfs
firewall-cmd --permanent --add-port=<emphasis>&lt;mountd-port&gt;</emphasis>/tcp
firewall-cmd --permanent --add-port=<emphasis>&lt;mountd-port&gt;</emphasis>/udp
firewall-cmd --reload</screen>
<simpara>In the commands, replace <emphasis>&lt;mountd-port&gt;</emphasis> with the intended port or a port range. When specifying a port range, use the <emphasis>--add-port=&lt;mountd-port&gt;-&lt;mountd-port&gt;/udp</emphasis> syntax.</simpara>
</listitem>
<listitem>
<simpara>To allow NFSv4.0 callbacks to pass through firewalls, set <literal role="filename">/proc/sys/fs/nfs/nfs_callback_tcpport</literal> and allow the server to connect to that port on the client.</simpara>
<simpara>This step is not needed for NFSv4.1 or higher, and the other ports for <literal>mountd</literal>, <literal>statd</literal>, and <literal>lockd</literal> are not required in a pure NFSv4 environment.</simpara>
</listitem>
<listitem>
<simpara>To specify the ports to be used by the RPC service <literal>nlockmgr</literal>, set the port number for the <literal>nlm_tcpport</literal> and <literal>nlm_udpport</literal> options in the <literal role="filename">/etc/modprobe.d/lockd.conf</literal> file.</simpara>
</listitem>
<listitem>
<simpara>Restart the NFS server:</simpara>
<screen>#  systemctl restart nfs-server</screen>
<simpara>If NFS fails to start, check <literal role="filename">/var/log/messages</literal>. Commonly, NFS fails to start if you specify a port number that is already in use.</simpara>
</listitem>
<listitem>
<simpara>Confirm the changes have taken effect:</simpara>
<screen># rpcinfo -p</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>To configure an NFSv4-only server, which does not require <literal>rpcbind</literal>, see <xref linkend="configuring-an-nfsv4-only-server_exporting-nfs-shares"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="exporting-rpc-quota-through-a-firewall_exporting-nfs-shares">
<title>Exporting RPC quota through a firewall</title>
<simpara>If you export a file system that uses disk quotas, you can use the quota Remote Procedure Call (RPC) service to provide disk quota data to NFS clients.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable and start the <literal>rpc-rquotad</literal> service:</simpara>
<screen># systemctl enable --now rpc-rquotad</screen>
<note>
<simpara>The <literal>rpc-rquotad</literal> service is, if enabled, started automatically after starting the nfs-server service.</simpara>
</note>
</listitem>
<listitem>
<simpara>To make the quota RPC service accessible behind a firewall, the TCP (or UDP, if UDP is enabled) port 875 need to be open. The default port number is defined in the <literal role="filename">/etc/services</literal> file.</simpara>
<simpara>You can override the default port number by appending <literal role="option">-p port-number</literal> to the <literal>RPCRQUOTADOPTS</literal> variable in the <literal role="filename">/etc/sysconfig/rpc-rquotad</literal> file.</simpara>
</listitem>
<listitem>
<simpara>By default, remote hosts can only read quotas. If you want to allow clients to set quotas, append the <literal role="option">-S</literal> option to the <literal>RPCRQUOTADOPTS</literal> variable in the <literal role="filename">/etc/sysconfig/rpc-rquotad</literal> file.</simpara>
</listitem>
<listitem>
<simpara>Restart <literal>rpc-rquotad</literal> for the changes in the <literal role="filename">/etc/sysconfig/rpc-rquotad</literal> file to take effect:</simpara>
<screen># systemctl restart rpc-rquotad</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="enabling-nfs-over-rdma-nfsordma_exporting-nfs-shares">
<title>Enabling NFS over RDMA (NFSoRDMA)</title>
<simpara>The remote direct memory access (RDMA) service works automatically in Red Hat Enterprise Linux 8 if there is RDMA-capable hardware present.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the <literal role="package">rdma-core</literal> package:</simpara>
<screen># yum install rdma-core</screen>
</listitem>
<listitem>
<simpara>To enable automatic loading of NFSoRDMA <emphasis>server</emphasis> modules, add the <literal role="option">SVCRDMA_LOAD=yes</literal> option on a new line in the <literal role="filename">/etc/rdma/rdma.conf</literal> configuration file.</simpara>
<simpara>The <literal>rdma=20049</literal> option in the <literal>[nfsd]</literal> section of the <literal role="filename">/etc/nfs.conf</literal> file specifies the port number on which the NFSoRDMA service listens for clients. The RFC 5667 standard specifies that servers must listen on port <literal>20049</literal> when providing NFSv4 services over RDMA.</simpara>
<simpara>The <literal role="filename">/etc/rdma/rdma.conf</literal> file contains a line that sets the <literal role="option">XPRTRDMA_LOAD=yes</literal> option by default, which requests the <literal>rdma</literal> service to load the NFSoRDMA <emphasis>client</emphasis> module.</simpara>
</listitem>
<listitem>
<simpara>Restart the <literal>nfs-server</literal> service:</simpara>
<screen># systemctl restart nfs-server</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The RFC 5667 standard: <link xlink:href="https://tools.ietf.org/html/rfc5667">https://tools.ietf.org/html/rfc5667</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-an-nfsv4-only-server_exporting-nfs-shares">
<title>Configuring an NFSv4-only server</title>
<simpara>As an NFS server administrator, you can configure the NFS server to support only NFSv4, which minimizes the number of open ports and running services on the system.</simpara>
<section xml:id="benefits-and-drawbacks-of-an-nfsv4-only-server_configuring-an-nfsv4-only-server">
<title>Benefits and drawbacks of an NFSv4-only server</title>
<simpara>This section explains the benefits and drawbacks of configuring the NFS server to only support NFSv4.</simpara>
<simpara>By default, the NFS server supports NFSv3 and NFSv4 connections in Red Hat Enterprise Linux 8. However, you can also configure NFS to support only NFS version 4.0 and later. This minimizes the number of open ports and running services on the system, because NFSv4 does not require the <literal>rpcbind</literal> service to listen on the network.</simpara>
<simpara>When your NFS server is configured as NFSv4-only, clients attempting to mount shares using NFSv3 fail with an error like the following:</simpara>
<screen>Requested NFS version or transport protocol is not supported.</screen>
<simpara>Optionally, you can also disable listening for the <literal>RPCBIND</literal>, <literal>MOUNT</literal>, and <literal>NSM</literal> protocol calls, which are not necessary in the NFSv4-only case.</simpara>
<simpara>The effects of disabling these additional options are:</simpara>
<itemizedlist>
<listitem>
<simpara>Clients that attempt to mount shares from your server using NFSv3 become unresponsive.</simpara>
</listitem>
<listitem>
<simpara>The NFS server itself is unable to mount NFSv3 file systems.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-the-nfs-server-to-support-only-nfsv4_configuring-an-nfsv4-only-server">
<title>Configuring the NFS server to support only NFSv4</title>
<simpara>This procedure describes how to configure your NFS server to support only NFS version 4.0 and later.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Disable NFSv3 by adding the following lines to the <literal>[nfsd]</literal> section of the <literal role="filename">/etc/nfs.conf</literal> configuration file:</simpara>
<screen>[nfsd]

vers3=no</screen>
</listitem>
<listitem>
<simpara>Optionally, disable listening for the <literal>RPCBIND</literal>, <literal>MOUNT</literal>, and <literal>NSM</literal> protocol calls, which are not necessary in the NFSv4-only case. Disable related services:</simpara>
<screen># systemctl mask --now rpc-statd.service rpcbind.service rpcbind.socket</screen>
</listitem>
<listitem>
<simpara>Restart the NFS server:</simpara>
<screen># systemctl restart nfs-server</screen>
</listitem>
</orderedlist>
<simpara>The changes take effect as soon as you start or restart the NFS server.</simpara>
</section>
<section xml:id="verifying-the-nfsv4-only-configuration_configuring-an-nfsv4-only-server">
<title>Verifying the NFSv4-only configuration</title>
<simpara>This procedure describes how to verify that your NFS server is configured in the NFSv4-only mode by using the <literal>netstat</literal> utility.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>netstat</literal> utility to list services listening on the TCP and UDP protocols:</simpara>
<screen># netstat --listening --tcp --udp</screen>
<example>
<title>Output on an NFSv4-only server</title>
<simpara>The following is an example <literal>netstat</literal> output on an NFSv4-only server; listening for <literal>RPCBIND</literal>, <literal>MOUNT</literal>, and <literal>NSM</literal> is also disabled. Here, <literal>nfs</literal> is the only listening NFS service:</simpara>
<screen># netstat --listening --tcp --udp

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 0.0.0.0:ssh             0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:nfs             0.0.0.0:*               LISTEN
tcp6       0      0 [::]:ssh                [::]:*                  LISTEN
tcp6       0      0 [::]:nfs                [::]:*                  LISTEN
udp        0      0 localhost.locald:bootpc 0.0.0.0:*</screen>
</example>
<example>
<title>Output before configuring an NFSv4-only server</title>
<simpara>In comparison, the <literal>netstat</literal> output before configuring an NFSv4-only server includes the <literal>sunrpc</literal> and <literal>mountd</literal> services:</simpara>
<screen># netstat --listening --tcp --udp

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address State
tcp        0      0 0.0.0.0:ssh             0.0.0.0:*       LISTEN
tcp        0      0 0.0.0.0:40189           0.0.0.0:*       LISTEN
tcp        0      0 0.0.0.0:46813           0.0.0.0:*       LISTEN
tcp        0      0 0.0.0.0:nfs             0.0.0.0:*       LISTEN
tcp        0      0 0.0.0.0:sunrpc          0.0.0.0:*       LISTEN
tcp        0      0 0.0.0.0:mountd          0.0.0.0:*       LISTEN
tcp6       0      0 [::]:ssh                [::]:*          LISTEN
tcp6       0      0 [::]:51227              [::]:*          LISTEN
tcp6       0      0 [::]:nfs                [::]:*          LISTEN
tcp6       0      0 [::]:sunrpc             [::]:*          LISTEN
tcp6       0      0 [::]:mountd             [::]:*          LISTEN
tcp6       0      0 [::]:45043              [::]:*          LISTEN
udp        0      0 localhost:1018          0.0.0.0:*
udp        0      0 localhost.locald:bootpc 0.0.0.0:*
udp        0      0 0.0.0.0:mountd          0.0.0.0:*
udp        0      0 0.0.0.0:46672           0.0.0.0:*
udp        0      0 0.0.0.0:sunrpc          0.0.0.0:*
udp        0      0 0.0.0.0:33494           0.0.0.0:*
udp6       0      0 [::]:33734              [::]:*
udp6       0      0 [::]:mountd             [::]:*
udp6       0      0 [::]:sunrpc             [::]:*
udp6       0      0 [::]:40243              [::]:*</screen>
</example>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="related-information-exporting-nfs-shares">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The Linux NFS wiki: <link xlink:href="https://linux-nfs.org/wiki/index.php/Main_Page">https://linux-nfs.org/wiki/index.php/Main_Page</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="securing-nfs_managing-file-systems">
<title>Securing NFS</title>
<simpara>To minimize NFS security risks and protect data on the server, consider the following sections when exporting NFS file systems on a server or mounting them on a client.</simpara>
<section xml:id="nfs-security-with-auth_sys-and-export-controls_securing-nfs">
<title>NFS security with AUTH_SYS and export controls</title>
<simpara>NFS provides the following traditional options in order to control access to exported files:</simpara>
<itemizedlist>
<listitem>
<simpara>The server restricts which hosts are allowed to mount which file systems either by IP address or by host name.</simpara>
</listitem>
<listitem>
<simpara>The server enforces file system permissions for users on NFS clients in the same way it does for local users. Traditionally, NFS does this using the <literal>AUTH_SYS</literal> call message (also called <literal>AUTH_UNIX</literal>), which relies on the client to state the UID and GIDs of the user. Be aware that this means that a malicious or misconfigured client might easily get this wrong and allow a user access to files that it should not.</simpara>
</listitem>
</itemizedlist>
<simpara>To limit the potential risks, administrators often limits the access to read-only or squash user permissions to a common user and group ID. Unfortunately, these solutions prevent the NFS share from being used in the    way it was originally intended.</simpara>
<simpara>Additionally, if an attacker gains control of the DNS server used by the system exporting the NFS file system, they can point the system associated with a particular hostname or fully qualified domain name to an unauthorized machine. At this point, the unauthorized machine <emphasis>is</emphasis> the system permitted to mount the NFS share, because no username or password information is exchanged to provide additional security for the NFS mount.</simpara>
<simpara>Wildcards should be used sparingly when exporting directories through NFS, as it is possible for the scope of the wildcard to encompass more systems than intended.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>To secure NFS and <literal role="command">rpcbind</literal>, use, for example, <literal role="systemitem">nftables</literal> and <literal role="systemitem">firewalld</literal>. For details about configuring these frameworks, see the <literal role="citetitle">nft(8)</literal> and <literal role="citetitle">firewalld-cmd(1)</literal> man pages.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nfs-security-with-auth_gss_securing-nfs">
<title>NFS security with <literal>AUTH_GSS</literal></title>
<simpara>All version of NFS support RPCSEC_GSS and the Kerberos mechanism.</simpara>
<simpara>Unlike AUTH_SYS, with the RPCSEC_GSS Kerberos mechanism, the server does not depend on the client to correctly represent which user is accessing the file. Instead, cryptography is used to authenticate users to  the server, which prevents a malicious client from impersonating a user without having that user’s Kerberos credentials. Using the RPCSEC_GSS Kerberos mechanism is the most straightforward way to secure mounts  because after configuring Kerberos, no additional setup is needed.</simpara>
</section>
<section xml:id="configuring-an-nfs-server-and-client-to-use-kerberos_securing-nfs">
<title>Configuring an NFS server and client to use Kerberos</title>
<simpara>Kerberos is a network authentication system that allows clients and servers to authenticate to each other by using symmetric encryption and a trusted third party, the KDC. Red Hat recommends using Identity Management (IdM) for setting up Kerberos.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The Kerberos Key Distribution Centre (<literal>KDC</literal>) is installed and configured.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara/>
<itemizedlist>
<listitem>
<simpara>Create the <literal>nfs/hostname.<emphasis>domain@REALM</emphasis></literal> principal on the NFS server side.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>host/hostname.<emphasis>domain@REALM</emphasis></literal> principal on both the server and the client side.</simpara>
</listitem>
<listitem>
<simpara>Add the corresponding keys to keytabs for the client and server.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On the server side, use the <literal role="option">sec=</literal> option to enable the wanted security flavors. To enable all security flavors as well as non-cryptographic mounts:</simpara>
<screen>/export *(sec=sys:krb5:krb5i:krb5p)</screen>
<simpara>Valid security flavors to use with the <literal role="option">sec=</literal> option are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>sys</literal>: no cryptographic protection, the default</simpara>
</listitem>
<listitem>
<simpara><literal>krb5</literal>: authentication only</simpara>
</listitem>
<listitem>
<simpara><literal>krb5i</literal>: integrity protection</simpara>
</listitem>
<listitem>
<simpara><literal>krb5p</literal>: privacy protection</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On the client side, add <literal>sec=krb5</literal> (or <literal>sec=krb5i</literal>, or <literal>sec=krb5p</literal>, depending on the setup) to the mount options:</simpara>
<screen># mount -o sec=krb5 server:/export /mnt</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>If you need to write files as root on the Kerberos-secured NFS share and keep root ownership on these files, see <link xlink:href="https://access.redhat.com/articles/4040141">https://access.redhat.com/articles/4040141</link>. Note that this configuration is not recommended.</simpara>
</listitem>
<listitem>
<simpara>For more information on NFS configuration, see the <emphasis role="strong">exports</emphasis>(5) and <emphasis role="strong">nfs</emphasis>(5) man pages.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nfsv4-security-options_securing-nfs">
<title>NFSv4 security options</title>
<simpara>NFSv4 includes ACL support based on the Microsoft Windows NT model, not the POSIX model, because of the Microsoft Windows NT model’s features and wide deployment.</simpara>
<simpara>Another important security feature of NFSv4 is the removal of the use of the <literal>MOUNT</literal> protocol for mounting file systems. The <literal>MOUNT</literal> protocol presented a security risk because of the way the protocol processed  file handles.</simpara>
</section>
<section xml:id="file-permissions-on-mounted-nfs-exports_securing-nfs">
<title>File permissions on mounted NFS exports</title>
<simpara>Once the NFS file system is mounted as either read or read and write by a remote host, the only protection each shared file has is its permissions. If two users that share the same user ID value mount the same  NFS file system on different client systems, they can modify each others' files.
Additionally, anyone logged in as root on the client system can use the <literal role="command">su -</literal> command to access any files with the NFS share.</simpara>
<simpara>By default, access control lists (ACLs) are supported by NFS under Red Hat Enterprise Linux. Red Hat recommends to keep this feature enabled.</simpara>
<simpara>By default, NFS uses <emphasis>root squashing</emphasis> when exporting a file system. This sets the user ID of anyone accessing the NFS share as the root user on their local machine to <literal>nobody</literal>. Root squashing is controlled by the default option <literal role="command">root_squash</literal>; for more information about this option, see <xref linkend="nfs-server-configuration_exporting-nfs-shares"/>.</simpara>
<simpara>When exporting an NFS share as read-only, consider using the <literal role="option">all_squash</literal> option. This option makes every user accessing the exported file system take the user ID of the <literal>nobody</literal> user.</simpara>
</section>
</chapter>
<chapter xml:id="enabling-pnfs-scsi-layouts-in-nfs_managing-file-systems">
<title>Enabling pNFS SCSI layouts in NFS</title>
<simpara>You can configure the NFS server and client to use the pNFS SCSI layout for accessing data. pNFS SCSI is beneficial in use cases that involve longer-duration single-client access to a file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Both the client and the server must be able to send SCSI commands to the same block device. That is, the block device must be on a shared SCSI bus.</simpara>
</listitem>
<listitem>
<simpara>The block device must contain an XFS file system.</simpara>
</listitem>
<listitem>
<simpara>The SCSI device must support SCSI Persistent Reservations as described in the SCSI-3 Primary Commands specification.</simpara>
</listitem>
</itemizedlist>
<section xml:id="the-pnfs-technology_enabling-pnfs-scsi-layouts-in-nfs">
<title>The pNFS technology</title>
<simpara>The pNFS architecture improves the scalability of NFS. When a server implements pNFS, the client is able to access data through multiple servers concurrently. This can lead to performance improvements.</simpara>
<simpara>pNFS supports the following storage protocols or layouts on RHEL:</simpara>
<itemizedlist>
<listitem>
<simpara>Files</simpara>
</listitem>
<listitem>
<simpara>Flexfiles</simpara>
</listitem>
<listitem>
<simpara>SCSI</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="pnfs-scsi-layouts_enabling-pnfs-scsi-layouts-in-nfs">
<title>pNFS SCSI layouts</title>
<simpara>The SCSI layout builds on the work of pNFS block layouts. The layout is defined across SCSI devices. It contains a sequential series of fixed-size blocks as logical units (LUs) that must be capable of supporting SCSI persistent reservations. The LU devices are identified by their SCSI device identification.</simpara>
<simpara>pNFS SCSI performs well in use cases that involve longer-duration single-client access to a file. An example might be a mail server or a virtual machine housing a cluster.</simpara>
<bridgehead xml:id="operations_between_the_client_and_the_server" renderas="sect3" remap="_operations_between_the_client_and_the_server">Operations between the client and the server</bridgehead>
<simpara>When an NFS client reads from a file or writes to it, the client performs a <literal>LAYOUTGET</literal> operation. The server responds with the location of the file on the SCSI device. The client might need to perform an additional operation of <literal>GETDEVICEINFO</literal> to determine which SCSI device to use. If these operations work correctly, the client can issue I/O requests directly to the SCSI device instead of sending <literal>READ</literal> and <literal>WRITE</literal> operations to the server.</simpara>
<simpara>Errors or contention between clients might cause the server to recall layouts or not issue them to the clients. In those cases, the clients fall back to issuing <literal>READ</literal> and <literal>WRITE</literal> operations to the server instead of sending I/O requests directly to the SCSI device.</simpara>
<simpara>To monitor the operations, see <xref linkend="monitoring-pnfs-scsi-layouts-functionality_enabling-pnfs-scsi-layouts-in-nfs"/>.</simpara>
<bridgehead xml:id="device_reservations" renderas="sect3" remap="_device_reservations">Device reservations</bridgehead>
<simpara>pNFS SCSI handles fencing through the assignment of reservations. Before the server issues layouts to clients, it reserves the SCSI device to ensure that only registered clients may access the device. If a client can issue commands to that SCSI device but is not registered with the device, many operations from the client on that device fail. For example, the <literal>blkid</literal> command on the client fails to show the UUID of the XFS file system if the server has not given a layout for that device to the client.</simpara>
<simpara>The server does not remove its own persistent reservation. This protects the data within the file system on the device across restarts of clients and servers. In order to repurpose the SCSI device, you might need to manually remove the persistent reservation on the NFS server.</simpara>
</section>
<section xml:id="checking-for-a-scsi-device-compatible-with-pnfs_enabling-pnfs-scsi-layouts-in-nfs">
<title>Checking for a SCSI device compatible with pNFS</title>
<simpara>This procedure checks if a SCSI device supports the pNFS SCSI layout.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal role="package">sg3_utils</literal> package:</simpara>
<screen># yum install sg3_utils</screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On both the server and client, check for the proper SCSI device support:</simpara>
<screen># sg_persist --in --report-capabilities --verbose <emphasis><phrase role="replaceable">path-to-scsi-device</phrase></emphasis></screen>
<simpara>Ensure that the <emphasis>Persist Through Power Loss Active</emphasis> (<literal>PTPL_A</literal>) bit is set.</simpara>
<example>
<title>A SCSI device that supports pNFS SCSI</title>
<simpara>The following is an example of <literal>sg_persist</literal> output for a SCSI device that supports pNFS SCSI. The <literal>PTPL_A</literal> bit reports <literal>1</literal>.</simpara>
<screen>    inquiry cdb: 12 00 00 00 24 00
    Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00
  LIO-ORG   block11           4.0
  Peripheral device type: disk
Report capabilities response:
  Compatible Reservation Handling(CRH): 1
  Specify Initiator Ports Capable(SIP_C): 1
  All Target Ports Capable(ATP_C): 1
  Persist Through Power Loss Capable(PTPL_C): 1
  Type Mask Valid(TMV): 1
  Allow Commands: 1
  <emphasis role="strong">Persist Through Power Loss Active(PTPL_A): 1</emphasis>
    Support indicated in Type mask:
      Write Exclusive, all registrants: 1
      Exclusive Access, registrants only: 1
      Write Exclusive, registrants only: 1
      Exclusive Access: 1
      Write Exclusive: 1
      Exclusive Access, all registrants: 1</screen>
</example>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>sg_persist(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-up-pnfs-scsi-on-the-server_enabling-pnfs-scsi-layouts-in-nfs">
<title>Setting up pNFS SCSI on the server</title>
<simpara>This procedure configures an NFS server to export a pNFS SCSI layout.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On the server, mount the XFS file system created on the SCSI device.</simpara>
</listitem>
<listitem>
<simpara>Configure the NFS server to export NFS version 4.1 or higher. Set the following option in the <literal>[nfsd]</literal> section of the <literal role="filename">/etc/nfs.conf</literal> file:</simpara>
<screen>[nfsd]

vers4.1=y</screen>
</listitem>
<listitem>
<simpara>Configure the NFS server to export the XFS file system over NFS with the <literal>pnfs</literal> option:</simpara>
<example>
<title>An entry in /etc/exports to export pNFS SCSI</title>
<simpara>The following entry in the <literal role="filename">/etc/exports</literal> configuration file exports the file system mounted at <literal role="filename">/exported/directory/</literal> to the <literal>allowed.example.com</literal> client as a pNFS SCSI layout:</simpara>
<screen>/exported/directory allowed.example.com(pnfs)</screen>
</example>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information on configuring an NFS server, see <xref linkend="exporting-nfs-shares_managing-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-up-pnfs-scsi-on-the-client_enabling-pnfs-scsi-layouts-in-nfs">
<title>Setting up pNFS SCSI on the client</title>
<simpara>This procedure configures an NFS client to mount a pNFS SCSI layout.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The NFS server is configured to export an XFS file system over pNFS SCSI. See <xref linkend="setting-up-pnfs-scsi-on-the-server_enabling-pnfs-scsi-layouts-in-nfs"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On the client, mount the exported XFS file system using NFS version 4.1 or higher:</simpara>
<screen># mount -t nfs -o nfsvers=4.1 <emphasis><phrase role="replaceable">host:/remote/export</phrase></emphasis> <emphasis><phrase role="replaceable">/local/directory</phrase></emphasis></screen>
<simpara>Do not mount the XFS file system directly without NFS.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information on mounting NFS shares, see <xref linkend="mounting-nfs-shares_managing-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="releasing-the-pnfs-scsi-reservation-on-the-server_enabling-pnfs-scsi-layouts-in-nfs">
<title>Releasing the pNFS SCSI reservation on the server</title>
<simpara>This procedure releases the persistent reservation that an NFS server holds on a SCSI device. This enables you to repurpose the SCSI device when you no longer need to export pNFS SCSI.</simpara>
<simpara>You must remove the reservation from the server. It cannot be removed from a different IT Nexus.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal role="package">sg3_utils</literal> package:</simpara>
<screen># yum install sg3_utils</screen>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Query an existing reservation on the server:</simpara>
<screen># sg_persist --read-reservation <emphasis><phrase role="replaceable">path-to-scsi-device</phrase></emphasis></screen>
<example>
<title>Querying a reservation on /dev/sda</title>
<screen># sg_persist --read-reservation /dev/sda

  LIO-ORG   block_1           4.0
  Peripheral device type: disk
  PR generation=0x8, Reservation follows:
    Key=0x100000000000000
    scope: LU_SCOPE,  type: Exclusive Access, registrants only</screen>
</example>
</listitem>
<listitem>
<simpara>Remove the existing registration on the server:</simpara>
<screen># sg_persist --out \
             --release \
             --param-rk=<emphasis><phrase role="replaceable">reservation-key</phrase></emphasis> \
             --prout-type=6 \
             <emphasis><phrase role="replaceable">path-to-scsi-device</phrase></emphasis></screen>
<example>
<title>Removing a reservation on /dev/sda</title>
<screen># sg_persist --out \
             --release \
             --param-rk=0x100000000000000 \
             --prout-type=6 \
             /dev/sda

  LIO-ORG   block_1           4.0
  Peripheral device type: disk</screen>
</example>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>sg_persist(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="monitoring-pnfs-scsi-layouts-functionality_enabling-pnfs-scsi-layouts-in-nfs">
<title>Monitoring pNFS SCSI layouts functionality</title>
<simpara>You can monitor if the pNFS client and server exchange proper pNFS SCSI operations or if they fall back on regular NFS operations.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A pNFS SCSI client and server are configured.</simpara>
</listitem>
</itemizedlist>
<section xml:id="checking-pnfs-scsi-operations-from-the-server-using-nfsstat_monitoring-pnfs-scsi-layouts-functionality">
<title>Checking pNFS SCSI operations from the server using nfsstat</title>
<simpara>This procedure uses the <literal>nfsstat</literal> utility to monitor pNFS SCSI operations from the server.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Monitor the operations serviced from the server:</simpara>
<screen># watch --differences \
        "nfsstat --server | egrep --after-context=1 read\|write\|layout"

Every 2.0s: nfsstat --server | egrep --after-context=1 read\|write\|layout

putrootfh    read         readdir      readlink     remove	 rename
2         0% 0         0% 1         0% 0         0% 0         0% 0         0%
--
setcltidconf verify	  write        rellockowner bc_ctl	 bind_conn
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
--
getdevlist   layoutcommit layoutget    layoutreturn secinfononam sequence
0         0% 29        1% 49        1% 5         0% 0         0% 2435     86%</screen>
</listitem>
<listitem>
<simpara>The client and server use pNFS SCSI operations when:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>layoutget</literal>, <literal>layoutreturn</literal>, and <literal>layoutcommit</literal> counters increment. This means that the server is serving layouts.</simpara>
</listitem>
<listitem>
<simpara>The server <literal>read</literal> and <literal>write</literal> counters do not increment. This means that the clients are performing I/O requests directly to the SCSI devices.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="checking-pnfs-scsi-operations-from-the-client-using-mountstats_monitoring-pnfs-scsi-layouts-functionality">
<title>Checking pNFS SCSI operations from the client using mountstats</title>
<simpara>This procedure uses the <literal role="filename">/proc/self/mountstats</literal> file to monitor pNFS SCSI operations from the client.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List the per-mount operation counters:</simpara>
<screen># cat /proc/self/mountstats \
      | awk /scsi_lun_0/,/^$/ \
      | egrep device\|READ\|WRITE\|LAYOUT

device 192.168.122.73:/exports/scsi_lun_0 mounted on /mnt/rhel7/scsi_lun_0 with fstype nfs4 statvers=1.1
    nfsv4:  bm0=0xfdffbfff,bm1=0x40f9be3e,bm2=0x803,acl=0x3,sessions,pnfs=LAYOUT_SCSI
            READ: 0 0 0 0 0 0 0 0
           WRITE: 0 0 0 0 0 0 0 0
        READLINK: 0 0 0 0 0 0 0 0
         READDIR: 0 0 0 0 0 0 0 0
       LAYOUTGET: 49 49 0 11172 9604 2 19448 19454
    LAYOUTCOMMIT: 28 28 0 7776 4808 0 24719 24722
    LAYOUTRETURN: 0 0 0 0 0 0 0 0
     LAYOUTSTATS: 0 0 0 0 0 0 0 0</screen>
</listitem>
<listitem>
<simpara>In the results:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>LAYOUT</literal> statistics indicate requests where the client and server use pNFS SCSI operations.</simpara>
</listitem>
<listitem>
<simpara>The <literal>READ</literal> and <literal>WRITE</literal> statistics indicate requests where the client and server fall back to NFS operations.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="getting-started-with-fs-cache_managing-file-systems">
<title>Getting started with FS-Cache</title>
<simpara>FS-Cache is a persistent local cache that file systems can use to take data retrieved from over the network and cache it on local disk. This helps minimize network traffic for users accessing data from a file system mounted over the network (for example, NFS).</simpara>
<section xml:id="overview-of-the-fs-cache_getting-started-with-fs-cache">
<title>Overview of the FS-Cache</title>
<simpara>The following diagram is a high-level illustration of how FS-Cache works:</simpara>
<figure xml:id="fig-fscachemain">
<title>FS-Cache Overview</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/fs-cache.png"/>
</imageobject>
<textobject><phrase>FS-Cache Overview</phrase></textobject>
</mediaobject>
</figure>
<simpara>FS-Cache is designed to be as transparent as possible to the users and administrators of a system. Unlike <literal role="command">cachefs</literal> on Solaris, FS-Cache allows a file system on a server to interact directly with a client’s local cache without creating an overmounted file system. With NFS, a mount option instructs the client to mount the NFS share with FS-cache enabled. The mount point will cause automatic upload for two kernel modules: <literal>fscache</literal> and <literal>cachefiles</literal>. The <literal>cachefilesd</literal> daemon communicates with the kernel modules to implement the cache.</simpara>
<simpara>FS-Cache does not alter the basic operation of a file system that works over the network - it merely provides that file system with a persistent place in which it can cache data. For instance, a client can still mount an NFS share whether or not FS-Cache is enabled. In addition, cached NFS can handle files that will not fit into the cache (whether individually or collectively) as files can be partially cached and do not have to be read completely up front. FS-Cache also hides all I/O errors that occur in the cache from the client file system driver.</simpara>
<simpara>To provide caching services, FS-Cache needs a <emphasis>cache back end</emphasis>. A cache back end is a storage driver configured to provide caching services, which is <literal role="command">cachefiles</literal>. In this case, FS-Cache requires a mounted block-based file system that supports <literal role="command">bmap</literal> and extended attributes (e.g. ext3) as its cache back end.</simpara>
<simpara>File systems that support functionalities required by FS-Cache cache back end include the Red Hat Enterprise Linux 8 implementations of the following file systems:</simpara>
<itemizedlist>
<listitem>
<simpara>ext3 (with extended attributes enabled)</simpara>
</listitem>
<listitem>
<simpara>ext4</simpara>
</listitem>
<listitem>
<simpara>XFS</simpara>
</listitem>
</itemizedlist>
<simpara>FS-Cache cannot arbitrarily cache any file system, whether through the network or otherwise: the shared file system’s driver must be altered to allow interaction with FS-Cache, data storage/retrieval, and metadata setup and validation. FS-Cache needs <emphasis>indexing keys</emphasis> and <emphasis>coherency data</emphasis> from the cached file system to support persistence: indexing keys to match file system objects to cache objects, and coherency data to determine whether the cache objects are still valid.</simpara>
<note>
<simpara>In Red Hat Enterprise Linux 8, the <emphasis role="strong"><phrase role="package">cachefilesd</phrase></emphasis> package is not installed by default and needs to be installed manually.</simpara>
</note>
</section>
<section xml:id="performance-guarantee_getting-started-with-fs-cache">
<title>Performance guarantee</title>
<simpara>FS-Cache does <emphasis>not</emphasis> guarantee increased performance. Using a cache incurs a performance penalty: for example, cached NFS shares add disk accesses to cross-network lookups. While FS-Cache tries to be as asynchronous as possible, there are synchronous paths (e.g. reads) where this isn’t possible.</simpara>
<simpara>For example, using FS-Cache to cache an NFS share between two computers over an otherwise unladen GigE network likely will not demonstrate any performance improvements on file access. Rather, NFS requests would be satisfied faster from server memory rather than from local disk.</simpara>
<simpara>The use of FS-Cache, therefore, is a <emphasis>compromise</emphasis> between various factors. If FS-Cache is being used to cache NFS traffic, for instance, it may slow the client down a little, but massively reduce the network and server loading by satisfying read requests locally without consuming network bandwidth.</simpara>
</section>
<section xml:id="setting-up-a-cache_getting-started-with-fs-cache">
<title>Setting up a cache</title>
<simpara>Currently, Red Hat Enterprise Linux 8 only provides the <literal role="command">cachefiles</literal> caching back end. The <literal role="command">cachefilesd</literal> daemon initiates and manages <literal role="command">cachefiles</literal>. The <literal>/etc/cachefilesd.conf</literal> file controls how <literal role="command">cachefiles</literal> provides caching services.</simpara>
<simpara>The cache back end works by maintaining a certain amount of free space on the partition hosting the cache. It grows and shrinks the cache in response to other elements of the system using up free space, making it safe to use on the root file system (for example, on a laptop). FS-Cache sets defaults on this behavior, which can be configured via <emphasis>cache cull limits</emphasis>. For more information about configuring cache cull limits, see <xref linkend="cache-cull-limits-configuration_getting-started-with-fs-cache"/>.</simpara>
<simpara>This procedure shows how to set up a cache.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <emphasis role="strong"><phrase role="package">cachefilesd</phrase></emphasis> package is installed and service has started successfully. To be sure the service is running, use the following command:</simpara>
<literallayout class="monospaced"># systemctl start cachefilesd
# systemctl status cachefilesd</literallayout>
<simpara>The status must be <emphasis>active (running)</emphasis>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure in a cache back end which directory to use as a cache, use the following parameter:</simpara>
<literallayout class="monospaced">$ dir <emphasis>/path/to/cache</emphasis></literallayout>
</listitem>
<listitem>
<simpara>Typically, the cache back end directory is set in <literal>/etc/cachefilesd.conf</literal> as <literal>/var/cache/fscache</literal>, as in:</simpara>
<literallayout class="monospaced">$ dir /var/cache/fscache</literallayout>
</listitem>
<listitem>
<simpara>If you want to change the cache back end directory, the selinux context must be same as <literal>/var/cache/fscache</literal>:</simpara>
<literallayout class="monospaced"># semanage fcontext -a -e /var/cache/fscache /path/to/cache
# restorecon -Rv /path/to/cache</literallayout>
</listitem>
<listitem>
<simpara>Replace <emphasis>/path/to/cache</emphasis> with the directory name while setting up cache.</simpara>
</listitem>
<listitem>
<simpara>If the given commands for setting selinux context did not work, use the following commands:</simpara>
<literallayout class="monospaced"># semanage permissive -a cachefilesd_t
# semanage permissive -a cachefiles_kernel_t</literallayout>
<simpara>FS-Cache will store the cache in the file system that hosts <literal role="command"><emphasis>/path/to/cache</emphasis></literal>. On a laptop, it is advisable to use the root file system (<literal>/</literal>) as the host file system, but for a desktop machine it would be more prudent to mount a disk partition specifically for the cache.</simpara>
</listitem>
<listitem>
<simpara>The host file system must support user-defined extended attributes; FS-Cache uses these attributes to store coherency maintenance information. To enable user-defined extended attributes for ext3 file systems (i.e. <literal role="command"><emphasis>device</emphasis></literal>), use:</simpara>
<literallayout class="monospaced"># tune2fs -o user_xattr /dev/device</literallayout>
</listitem>
<listitem>
<simpara>To enable extended attributes for a file system at the mount time, as an alternative, use the following command:</simpara>
<literallayout class="monospaced"># mount /dev/device /path/to/cache -o user_xattr</literallayout>
</listitem>
<listitem>
<simpara>Once the configuration file is in place, start up the <literal>cachefilesd</literal> service:</simpara>
<literallayout class="monospaced"># systemctl start cachefilesd</literallayout>
</listitem>
<listitem>
<simpara>To configure <literal role="command">cachefilesd</literal> to start at boot time, execute the following command as root:</simpara>
<literallayout class="monospaced"># systemctl enable cachefilesd</literallayout>
</listitem>
</orderedlist>
</section>
<section xml:id="using-the-cache-with-nfs_getting-started-with-fs-cache">
<title>Using the cache with NFS</title>
<simpara>NFS will not use the cache unless explicitly instructed. This paragraph shows how to configure an NFS mount by using FS-Cache.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <emphasis role="strong"><phrase role="package">cachefilesd</phrase></emphasis> package is installed and running. To ensure it is running, use the following command:</simpara>
<literallayout class="monospaced"># systemctl start cachefilesd
# systemctl status cachefilesd</literallayout>
<simpara>The status must be <emphasis>active (running)</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Mount NFS shares with the following option:</simpara>
<literallayout class="monospaced"># mount nfs-share:/ <emphasis>/mount/point</emphasis> -o fsc</literallayout>
<simpara>All access to files under <literal role="command"><emphasis>/mount/point</emphasis></literal> will go through the cache, unless the file is opened for direct I/O or writing. For more information, see <xref linkend="cache-limitations-with-nfs_using-the-cache-with-nfs"/>. NFS indexes cache contents using NFS file handle, <emphasis>not</emphasis> the file name, which means hard-linked files share the cache correctly.</simpara>
</listitem>
</itemizedlist>
<simpara>NFS versions 3, 4.0, 4.1 and 4.2 support caching. However, each version uses different branches for caching.</simpara>
<section xml:id="configuring-nfs-cache-sharing_using-the-cache-with-nfs">
<title>Configuring NFS cache sharing</title>
<simpara>There are several potential issues to do with NFS cache sharing. Because the cache is persistent, blocks of data in the cache are indexed on a sequence of four keys:</simpara>
<itemizedlist>
<listitem>
<simpara>Level 1: Server details</simpara>
</listitem>
<listitem>
<simpara>Level 2: Some mount options; security type; FSID; uniquifier</simpara>
</listitem>
<listitem>
<simpara>Level 3: File Handle</simpara>
</listitem>
<listitem>
<simpara>Level 4: Page number in file</simpara>
</listitem>
</itemizedlist>
<simpara>To avoid coherency management problems between superblocks, all NFS superblocks that require to cache the data have unique Level 2 keys. Normally, two NFS mounts with same source volume and options share a superblock, and thus share the caching, even if they mount different directories within that volume.</simpara>
<simpara>This is an example how to configure cache sharing with different options.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Mount NFS shares with the following commands:</simpara>
<literallayout class="monospaced">mount home0:/disk0/fred /home/fred -o fsc
mount home0:/disk0/jim /home/jim -o fsc</literallayout>
<simpara>Here, <literal>/home/fred</literal> and <literal>/home/jim</literal> likely share the superblock as they have the same options, especially if they come from the same volume/partition on the NFS server (<literal>home0</literal>).</simpara>
</listitem>
<listitem>
<simpara>To not share the superblock, use the <literal role="command">mount</literal> command with the following options:</simpara>
<literallayout class="monospaced">mount home0:/disk0/fred /home/fred -o fsc,rsize=8192
mount home0:/disk0/jim /home/jim -o fsc,rsize=65536</literallayout>
<simpara>In this case, <literal>/home/fred</literal> and <literal>/home/jim</literal> will not share the superblock as they have different network access parameters, which are part of the Level 2 key.</simpara>
</listitem>
<listitem>
<simpara>To cache the contents of the two subtrees (<literal>/home/fred1</literal> and <literal>/home/fred2</literal>) <emphasis>twice</emphasis> with not sharing the superblock, use the following command:</simpara>
<literallayout class="monospaced">mount home0:/disk0/fred /home/fred1 -o fsc,rsize=8192
mount home0:/disk0/fred /home/fred2 -o fsc,rsize=65536</literallayout>
</listitem>
<listitem>
<simpara>Another way to avoid superblock sharing is to suppress it explicitly with the <literal role="command">nosharecache</literal> parameter. Using the same example:</simpara>
<literallayout class="monospaced">mount home0:/disk0/fred /home/fred -o nosharecache,fsc
mount home0:/disk0/jim /home/jim -o nosharecache,fsc</literallayout>
<simpara>However, in this case only one of the superblocks is permitted to use cache since there is nothing to distinguish the Level 2 keys of <literal>home0:/disk0/fred</literal> and <literal>home0:/disk0/jim</literal>.</simpara>
</listitem>
<listitem>
<simpara>To specify the addressing to the superblock, add a <emphasis>unique identifier</emphasis> on at least one of the mounts, i.e. <literal role="command">fsc=<emphasis>unique-identifier</emphasis></literal>:</simpara>
<literallayout class="monospaced">mount home0:/disk0/fred /home/fred -o nosharecache,fsc
mount home0:/disk0/jim /home/jim -o nosharecache,fsc=jim</literallayout>
<simpara>Here, the unique identifier <literal role="command">jim</literal> is added to the Level 2 key used in the cache for <literal>/home/jim</literal>.</simpara>
</listitem>
</orderedlist>
<important>
<simpara>The user can not share caches between superblocks that have different communications or protocol parameters. For example, it is not possible to share between NFSv4.0 and NFSv3 or between NFSv4.1 and NFSv4.2 because they force different superblocks. Also setting parameters, such as the read size (rsize), prevents cache sharing because, again, it forces a different superblock.</simpara>
</important>
</section>
<section xml:id="cache-limitations-with-nfs_using-the-cache-with-nfs">
<title>Cache limitations with NFS</title>
<simpara>There are some cache limitations with NFS:</simpara>
<itemizedlist>
<listitem>
<simpara>Opening a file from a shared file system for direct I/O automatically bypasses the cache. This is because this type of access must be direct to the server.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>Opening a file from a shared file system for either direct I/O or writing flushes the cached copy of the file. FS-Cache will not cache the file again until it is no longer opened for direct I/O or writing.</simpara>
</listitem>
<listitem>
<simpara>Furthermore, this release of FS-Cache only caches regular NFS files. FS-Cache will <emphasis>not</emphasis> cache directories, symlinks, device files, FIFOs and sockets.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cache-cull-limits-configuration_getting-started-with-fs-cache">
<title>Cache cull limits configuration</title>
<simpara>The <literal role="command">cachefilesd</literal> daemon works by caching remote data from shared file systems to free space on the disk. This could potentially consume all available free space, which could be bad if the disk also housed the root partition. To control this, <literal role="command">cachefilesd</literal> tries to maintain a certain amount of free space by discarding old objects (i.e. accessed less recently) from the cache. This behavior is known as <emphasis>cache culling</emphasis>.</simpara>
<simpara>Cache culling is done on the basis of the percentage of blocks and the percentage of files available in the underlying file system. There are settings in <literal>/etc/cachefilesd.conf</literal> which control six limits:</simpara>
<variablelist>
<varlistentry>
<term>brun <emphasis>N</emphasis>% (percentage of blocks), frun <emphasis>N</emphasis>% (percentage of files)</term>
<listitem>
<simpara>If the amount of free space and the number of available files in the cache rises above both these limits, then culling is turned off.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>bcull <emphasis>N</emphasis>% (percentage of blocks), fcull <emphasis>N</emphasis>% (percentage of files)</term>
<listitem>
<simpara>If the amount of available space or the number of files in the cache falls below either of these limits, then culling is started.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>bstop <emphasis>N</emphasis>% (percentage of blocks), fstop <emphasis>N</emphasis>% (percentage of files)</term>
<listitem>
<simpara>If the amount of available space or the number of available files in the cache falls below either of these limits, then no further allocation of disk space or files is permitted until culling has raised things above these limits again.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The default value of <literal>N</literal> for each setting is as follows:</simpara>
<itemizedlist>
<listitem>
<simpara><literal role="command">brun</literal>/<literal role="command">frun</literal> - 10%</simpara>
</listitem>
<listitem>
<simpara><literal role="command">bcull</literal>/<literal role="command">fcull</literal> - 7%</simpara>
</listitem>
<listitem>
<simpara><literal role="command">bstop</literal>/<literal role="command">fstop</literal> - 3%</simpara>
</listitem>
</itemizedlist>
<simpara>When configuring these settings, the following must hold true:</simpara>
<itemizedlist>
<listitem>
<simpara>0 ≤ <literal role="command">bstop</literal> &lt; <literal role="command">bcull</literal> &lt; <literal role="command">brun</literal> &lt; 100</simpara>
</listitem>
<listitem>
<simpara>0 ≤ <literal role="command">fstop</literal> &lt; <literal role="command">fcull</literal> &lt; <literal role="command">frun</literal> &lt; 100</simpara>
</listitem>
</itemizedlist>
<simpara>These are the percentages of available space and available files and do not appear as 100 minus the percentage displayed by the <literal role="command">df</literal> program.</simpara>
<important>
<simpara>Culling depends on both b<emphasis>xxx</emphasis> and f<emphasis>xxx</emphasis> pairs simultaneously; the user can not treat them separately.</simpara>
</important>
</section>
<section xml:id="retrieving-statistical-information_getting-started-with-fs-cache">
<title>Retrieving statistical information from the fscache kernel module</title>
<simpara>FS-Cache also keeps track of general statistical information. This procedure shows how to get this information.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To view the statistical information on FS-Cache, use the following command:</simpara>
<literallayout class="monospaced"># cat /proc/fs/fscache/stats</literallayout>
</listitem>
</orderedlist>
<simpara>FS-Cache statistics includes information on decision points and object counters. For more information, see the following kernel document:</simpara>
<simpara><literal>/usr/share/doc/kernel-doc-4.18.0/Documentation/filesystems/caching/fscache.txt</literal></simpara>
</section>
<section xml:id="fs-cache-references_getting-started-with-fs-cache">
<title>FS-Cache references</title>
<simpara>This section provides reference information for FS-Cache.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>For more information on <literal role="command">cachefilesd</literal> and how to configure it, see <literal role="command">man cachefilesd</literal> and <literal role="command">man cachefilesd.conf</literal>. The following kernel documents also provide additional information:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/usr/share/doc/cachefilesd/README</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/usr/share/man/man5/cachefilesd.conf.5.gz</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/usr/share/man/man8/cachefilesd.8.gz</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>For general information about FS-Cache, including details on its design constraints, available statistics, and capabilities, see the following kernel document:</simpara>
<simpara><literal>/usr/share/doc/kernel-doc-4.18.0/Documentation/filesystems/caching/fscache.txt</literal></simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_mounting-an-smb-share-on-red-hat-enterprise-linux_managing-file-systems">
<title>Mounting an SMB Share on Red Hat Enterprise Linux</title>
<simpara>The Server Message Block (SMB) protocol implements an application-layer network protocol used to access resources on a server, such as file shares and shared printers.</simpara>
<note>
<simpara>In the context of SMB, you can find mentions about the Common Internet File System (CIFS) protocol, which is a dialect of SMB. Both the SMB and CIFS protocol are supported, and the kernel module and utilities involved in mounting SMB and CIFS shares both use the name <literal>cifs</literal>.</simpara>
</note>
<simpara>This section describes how to mount shares from an SMB server. For details about setting up an SMB server on Red Hat Enterprise Linux using Samba, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/assembly_using-samba-as-a-server_deploying-different-types-of-servers">Using Samba as a server</link>.</simpara>
<formalpara>
<title>Prerequisites</title>
<para>On Microsoft Windows, SMB is implemented by default. On Red Hat Enterprise Linux, the <literal>cifs.ko</literal> file system module of the kernel provides support for mounting SMB shares. Therefor install the <literal>cifs-utils</literal> package:</para>
</formalpara>
<literallayout class="monospaced"># yum install cifs-utils</literallayout>
<simpara>The <literal>cifs-utils</literal> package provides utilities to:</simpara>
<itemizedlist>
<listitem>
<simpara>Mount SMB and CIFS shares</simpara>
</listitem>
<listitem>
<simpara>Manage NT Lan Manager (NTLM) credentials in the kernel’s keyring</simpara>
</listitem>
<listitem>
<simpara>Set and display Access Control Lists (ACL) in a security descriptor on SMB and CIFS shares</simpara>
</listitem>
</itemizedlist>
<section xml:id="con_supported-smb-protocol-versions_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Supported SMB protocol versions</title>
<simpara>The <literal>cifs.ko</literal> kernel module supports the following SMB protocol versions:</simpara>
<itemizedlist>
<listitem>
<simpara>SMB 1</simpara>
<warning>
<simpara>The SMB1 protocol is deprecated due to known security issues, and is only <emphasis role="strong">safe to use on a private network</emphasis>.
The main reason that SMB1 is still provided as a supported option is that currently it is the only SMB protocol version that supports UNIX extensions.
If you do not need to use UNIX extensions on SMB, Red Hat strongly recommends using SMB2 or later.</simpara>
</warning>
</listitem>
<listitem>
<simpara>SMB 2.0</simpara>
</listitem>
<listitem>
<simpara>SMB 2.1</simpara>
</listitem>
<listitem>
<simpara>SMB 3.0</simpara>
</listitem>
<listitem>
<simpara>SMB 3.1.1</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Depending on the protocol version, not all SMB features are implemented.</simpara>
</note>
</section>
<section xml:id="con_unix-extensions-support_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>UNIX extensions support</title>
<simpara>Samba uses the <literal>CAP_UNIX</literal> capability bit in the SMB protocol to provide the UNIX extensions feature. These extensions are also supported by the <literal>cifs.ko</literal> kernel module. However, both Samba and the kernel module support UNIX extensions only in the SMB 1 protocol.</simpara>
<simpara>To use UNIX extensions:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>server min protocol</literal> parameter in the <literal>[global]</literal> section in the <literal role="filename">/etc/samba/smb.conf</literal> file to <literal>NT1</literal>.</simpara>
</listitem>
<listitem>
<simpara>Mount the share using the SMB 1 protocol by providing the <literal>-o vers=1.0</literal> option to the mount command. For example:</simpara>
<literallayout class="monospaced"># mount -t cifs <emphasis role="strong">-o vers=1.0</emphasis>,username=<emphasis>user_name</emphasis> <emphasis>//server_name/share_name</emphasis> <emphasis>/mnt/</emphasis></literallayout>
<simpara>By default, the kernel module uses SMB 2 or the highest later protocol version supported by the server. Passing the <literal>-o vers=1.0</literal> option to the <literal>mount</literal> command forces that the kernel module uses the SMB 1 protocol that is required for using UNIX extensions.</simpara>
</listitem>
</orderedlist>
<simpara>To verify if UNIX extensions are enabled, display the options of the mounted share:</simpara>
<literallayout class="monospaced"># mount
...
<emphasis>//server/share</emphasis> on <emphasis>/mnt</emphasis> type cifs (...,<literal>unix</literal>,...)</literallayout>
<simpara>If the <literal>unix</literal> entry is displayed in the list of mount options, UNIX extensions are enabled.</simpara>
</section>
<section xml:id="proc_manually-mounting-an-smb-share_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Manually mounting an SMB share</title>
<simpara>If you only require an SMB share to be temporary mounted, you can mount it manually using the <literal>mount</literal> utility.</simpara>
<note>
<simpara>Manually mounted shares are not mounted automatically again when you reboot the system. To configure that Red Hat Enterprise Linux automatically mounts the share when the system boots, see <xref linkend="proc_mounting-an-smb-share-automatically-when-the-system-boots_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/>.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <literal role="package">cifs-utils</literal> package is installed.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To manually mount an SMB share, use the <literal>mount</literal> utility with the <literal>-t cifs</literal> parameter:</para>
</formalpara>
<literallayout class="monospaced"># mount -t cifs -o username=<emphasis>user_name</emphasis> <emphasis>//server_name/share_name</emphasis> <emphasis>/mnt/</emphasis>
Password for <emphasis>user_name@//server_name/share_name</emphasis>:  <emphasis>password</emphasis></literallayout>
<simpara>In the <literal>-o</literal> parameter, you can specify options that are used to mount the share. For details, see <xref linkend="con_frequently-used-mount-options_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/> and the <literal>OPTIONS</literal> section in the <literal>mount.cifs(8)</literal> man page.</simpara>
<example xml:id="example_mounting-a-share-using-an-encrypted-smb30-connection_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Mounting a share using an encrypted SMB 3.0 connection</title>
<simpara>To mount the <literal>\\server\example\</literal> share as the <literal><emphasis>DOMAIN</emphasis>\Administrator</literal> user over an encrypted SMB 3.0 connection into the <literal>/mnt/</literal> directory:</simpara>
<literallayout class="monospaced"># mount -t cifs -o username=<emphasis>DOMAIN</emphasis>\Administrator,seal,vers=3.0 //server/example /mnt/
Password for <emphasis>DOMAIN</emphasis>\Administrator@//server_name/share_name:  <emphasis>password</emphasis></literallayout>
</example>
</section>
<section xml:id="proc_mounting-an-smb-share-automatically-when-the-system-boots_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Mounting an SMB share automatically when the system boots</title>
<simpara>If access to a mounted SMB share is permanently required on a server, mount the share automatically at boot time.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <literal role="package">cifs-utils</literal> package is installed.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To mount an SMB share automatically when the system boots, add an entry for the share to the <literal role="filename">/etc/fstab</literal> file. For example:</para>
</formalpara>
<literallayout class="monospaced"><emphasis>//server_name/share_name</emphasis>  <emphasis>/mnt</emphasis>  cifs  credentials=<emphasis>/root/smb.cred</emphasis>  0 0</literallayout>
<important>
<simpara>To enable the system to mount a share automatically, you must store the user name, password, and domain name in a credentials file. For details, see <xref linkend="proc_authenticating-to-an-smb-share-using-a-credentials-file_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/>.</simpara>
</important>
<simpara>In the fourth field of the row in the <literal role="filename">/etc/fstab</literal>, specify mount options, such as the path to the credentials file. For details, see <xref linkend="con_frequently-used-mount-options_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/> and the <literal>OPTIONS</literal> section in the <literal>mount.cifs(8)</literal> man page.</simpara>
<simpara>To verify that the share mounts successfully, enter:</simpara>
<literallayout class="monospaced"># mount /mnt/</literallayout>
</section>
<section xml:id="proc_authenticating-to-an-smb-share-using-a-credentials-file_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Authenticating to an SMB share using a credentials file</title>
<simpara>In certain situations, such as when mounting a share automatically at boot time, a share should be mounted without entering the user name and password. To implement this, create a credentials file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <literal role="package">cifs-utils</literal> package is installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal role="filename">/root/smb.cred</literal>, and specify the user name, password, and domain name that file:</simpara>
<literallayout class="monospaced">username=<emphasis>user_name</emphasis>
password=<emphasis>password</emphasis>
domain=<emphasis>domain_name</emphasis></literallayout>
</listitem>
<listitem>
<simpara>Set the permissions to only allow the owner to access the file:</simpara>
<literallayout class="monospaced"># chown user_name /root/smb.cred
# chmod 600 /root/smb.cred</literallayout>
</listitem>
</orderedlist>
<simpara>You can now pass the <literal>credentials=<emphasis>file_name</emphasis></literal> mount option to the <literal>mount</literal> utility or use it in the <literal role="filename">/etc/fstab</literal> file to mount the share without being prompted for the user name and password.</simpara>
</section>
<section xml:id="assembly_performing-a-multi-user-smb-mount_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Performing a multi-user SMB mount</title>
<simpara>The credentials you provide to mount a share determine the access permissions on the mount point by default. For example, if you use the <literal><emphasis>DOMAIN</emphasis>\example</literal> user when you mount a share, all operations on the share will be executed as this user, regardless which local user performs the operation.</simpara>
<simpara>However, in certain situations, the administrator wants to mount a share automatically when the system boots, but users should perform actions on the share’s content using their own credentials. The <literal>multiuser</literal> mount options lets you configure this scenario.</simpara>
<important>
<simpara>To use the <literal>multiuser</literal> mount option, you must additionally set the <literal>sec</literal> mount option to a security type that supports providing credentials in a non-interactive way, such as <literal>krb5</literal> or the <literal>ntlmssp</literal> option with a credentials file. For details, see <xref linkend="proc_accessing-a-share-as-a-user_assembly_performing-a-multi-user-smb-mount"/>.</simpara>
</important>
<simpara>The <literal>root</literal> user mounts the share using the <literal>multiuser</literal> option and an account that has minimal access to the contents of the share. Regular users can then provide their user name and password to the current session’s kernel keyring using the <literal>cifscreds</literal> utility. If the user accesses the content of the mounted share, the kernel uses the credentials from the kernel keyring instead of the one initially used to mount the share.</simpara>
<simpara>Using this feature consists of the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="proc_mounting-a-share-with-the-multiuser-option_assembly_performing-a-multi-user-smb-mount">Mount a share with the <literal>multiuser</literal> option</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="proc_verifying-if-an-smb-share-is-mounted-with-the-multiuser-option_assembly_performing-a-multi-user-smb-mount">Optionally, verify if the share was successfully mounted with the <literal>multiuser</literal> option</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="proc_accessing-a-share-as-a-user_assembly_performing-a-multi-user-smb-mount">Access the share as a user</link>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <literal role="package">cifs-utils</literal> package is installed.</simpara>
</listitem>
</itemizedlist>
<section xml:id="proc_mounting-a-share-with-the-multiuser-option_assembly_performing-a-multi-user-smb-mount">
<title>Mounting a share with the multiuser option</title>
<simpara>Before users can access the share with their own credentials, mount the share as the <literal>root</literal> user using an account with limited permissions.</simpara>
<formalpara>
<title>Procedure</title>
<para>To mount a share automatically with the <literal>multiuser</literal> option when the system boots:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the entry for the share in the <literal role="filename">/etc/fstab</literal> file. For example:</simpara>
<literallayout class="monospaced"><emphasis>//server_name/share_name</emphasis>  <emphasis>/mnt</emphasis>  cifs  <literal>multiuser,sec=ntlmssp</literal>,credentials=<emphasis>/root/smb.cred</emphasis>  0 0</literallayout>
</listitem>
<listitem>
<simpara>Mount the share:</simpara>
<literallayout class="monospaced"># mount /mnt/</literallayout>
</listitem>
</orderedlist>
<simpara>If you do not want to mount the share automatically when the system boots, mount it manually by passing <literal>-o multiuser,sec=security_type</literal> to the <literal>mount</literal> command. For details about mounting an SMB share manually, see <xref linkend="proc_manually-mounting-an-smb-share_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/>.</simpara>
</section>
<section xml:id="proc_verifying-if-an-smb-share-is-mounted-with-the-multiuser-option_assembly_performing-a-multi-user-smb-mount">
<title>Verifying if an SMB share is mounted with the multiuser option</title>
<simpara>To verify if a share is mounted with the <literal>multiuser</literal> option, display the mount options.</simpara>
<formalpara>
<title>Procedure</title>
<para>
<literallayout class="monospaced"># mount
...
<emphasis>//server_name/share_name</emphasis> on <emphasis>/mnt</emphasis> type cifs (sec=ntlmssp,<literal>multiuser</literal>,...)</literallayout>
</para>
</formalpara>
<simpara>If the <literal>multiuser</literal> entry is displayed in the list of mount options, the feature is enabled.</simpara>
</section>
<section xml:id="proc_accessing-a-share-as-a-user_assembly_performing-a-multi-user-smb-mount">
<title>Accessing a share as a user</title>
<simpara>If an SMB share is mounted with the <literal>multiuser</literal>  option, users can provide their credentials for the server to the kernel’s keyring:</simpara>
<literallayout class="monospaced"># cifscreds add -u <emphasis>SMB_user_name</emphasis> <emphasis>server_name</emphasis>
Password: <emphasis>password</emphasis></literallayout>
<simpara>When the user performs operations in the directory that contains the mounted SMB share, the server applies the file system permissions for this user, instead of the one initially used when the share was mounted.</simpara>
<note>
<simpara>Multiple users can perform operations using their own credentials on the mounted share at the same time.</simpara>
</note>
</section>
</section>
<section xml:id="con_frequently-used-mount-options_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux">
<title>Frequently used mount options</title>
<simpara>When you mount an SMB share, the mount options determine:</simpara>
<itemizedlist>
<listitem>
<simpara>How the connection will be established with the server. For example, which SMB protocol version is used when connecting to the server.</simpara>
</listitem>
<listitem>
<simpara>How the share will be mounted into the local file system. For example, if the system overrides the remote file and directory permissions to enable multiple local users to access the content on the server.</simpara>
</listitem>
</itemizedlist>
<simpara>To set multiple options in the fourth field of the <literal role="filename">/etc/fstab</literal> file or in the <literal>-o</literal> parameter of a mount command, separate them with commas. For example, see <xref linkend="proc_mounting-a-share-with-the-multiuser-option_assembly_performing-a-multi-user-smb-mount"/>.</simpara>
<simpara>The following list gives frequently used mount options:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="71*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>credentials=<emphasis>file_name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the path to the credentials file. See <xref linkend="proc_authenticating-to-an-smb-share-using-a-credentials-file_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>dir_mode=<emphasis>mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the directory mode if the server does not support CIFS UNIX extensions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>file_mode=<emphasis>mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the file mode if the server does not support CIFS UNIX extensions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>password=<emphasis>password</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the password used to authenticate to the SMB server. Alternatively, specify a credentials file using the <literal>credentials</literal> option.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>seal</simpara></entry>
<entry align="left" valign="top"><simpara>Enables encryption support for connections using SMB 3.0 or a later protocol version. Therefore, use <literal>seal</literal> together with the <literal>vers</literal> mount option set to <literal>3.0</literal> or later. See <xref linkend="example_mounting-a-share-using-an-encrypted-smb30-connection_assembly_mounting-an-smb-share-on-red-hat-enterprise-linux"/>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>sec=<emphasis>security_mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the security mode, such as <literal>ntlmsspi</literal>, to enable NTLMv2 password hashing and enabled packet signing. For a list of supported values, see the option’s description in the <literal>mount.cifs(8)</literal> man page.</simpara>
<simpara>If the server does not support the <literal>ntlmv2</literal> security mode, use <literal>sec=ntlmssp</literal>, which is the default.</simpara>
<simpara>For security reasons, do not use the insecure <literal>ntlm</literal> security mode.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>username=<emphasis>user_name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the user name used to authenticate to the SMB server. Alternatively, specify a credentials file using the <literal>credentials</literal> option.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>vers=<emphasis>SMB_protocol_version</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the SMB protocol version used for the communication with the server.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>For a complete list, see the <literal>OPTIONS</literal> section in the <literal>mount.cifs(8)</literal> man page.</simpara>
</section>
</chapter>
<chapter xml:id="assembly_overview-of-persistent-naming-attributes_managing-file-systems">
<title>Overview of persistent naming attributes</title>
<simpara>As a system administrator, you need to refer to storage volumes using persistent naming attributes to build storage setups that are reliable over multiple system boots.</simpara>
<section xml:id="con_disadvantages-of-non-persistent-naming-attributes_assembly_overview-of-persistent-naming-attributes">
<title>Disadvantages of non-persistent naming attributes</title>
<simpara>Red Hat Enterprise Linux provides a number of ways to identify storage devices. It is important to use the correct option to identify each device when used in order to avoid inadvertently accessing the wrong device, particularly when installing to or reformatting drives.</simpara>
<simpara>Traditionally, non-persistent names in the form of <literal>/dev/sd<emphasis><phrase role="replaceable">(major number)</phrase></emphasis><emphasis><phrase role="replaceable">(minor number)</phrase></emphasis></literal> are used on Linux to refer to storage devices. The major and minor number range and associated <literal>sd</literal> names are allocated for each device when it is detected. This means that the association between the major and minor number range and associated <literal>sd</literal> names can change if the order of device detection changes.</simpara>
<simpara>Such a change in the ordering might occur in the following situations:</simpara>
<itemizedlist>
<listitem>
<simpara>The parallelization of the system boot process detects storage devices in a different order with each system boot.</simpara>
</listitem>
<listitem>
<simpara>A disk fails to power up or respond to the SCSI controller. This results in it not being detected by the normal device probe. The disk is not accessible to the system and subsequent devices will have their major and minor number range, including the associated <literal>sd</literal> names shifted down. For example, if a disk normally referred to as <literal>sdb</literal> is not detected, a disk that is normally referred to as <literal>sdc</literal> would instead appear as <literal>sdb</literal>.</simpara>
</listitem>
<listitem>
<simpara>A SCSI controller (host bus adapter, or HBA) fails to initialize, causing all disks connected to that HBA to not be detected. Any disks connected to subsequently probed HBAs are assigned different major and minor number ranges, and different associated <literal>sd</literal> names.</simpara>
</listitem>
<listitem>
<simpara>The order of driver initialization changes if different types of HBAs are present in the system. This causes the disks connected to those HBAs to be detected in a different order. This might also occur if HBAs are moved to different PCI slots on the system.</simpara>
</listitem>
<listitem>
<simpara>Disks connected to the system with Fibre Channel, iSCSI, or FCoE adapters might be inaccessible at the time the storage devices are probed, due to a storage array or intervening switch being powered off, for example. This might occur when a system reboots after a power failure, if the storage array takes longer to come online than the system take to boot. Although some Fibre Channel drivers support a mechanism to specify a persistent SCSI target ID to WWPN mapping, this does not cause the major and minor number ranges, and the associated <literal>sd</literal> names to be reserved; it only provides consistent SCSI target ID numbers.</simpara>
</listitem>
</itemizedlist>
<simpara>These reasons make it undesirable to use the major and minor number range or the associated <literal>sd</literal> names when referring to devices, such as in the <literal role="filename">/etc/fstab</literal> file. There is the possibility that the wrong device will be mounted and data corruption might result.</simpara>
<simpara>Occasionally, however, it is still necessary to refer to the <literal>sd</literal> names even when another mechanism is used, such as when errors are reported by a device. This is because the Linux kernel uses <literal>sd</literal> names (and also SCSI host/channel/target/LUN tuples) in kernel messages regarding the device.</simpara>
</section>
<section xml:id="file-system-and-device-identifiers_assembly_overview-of-persistent-naming-attributes">
<title>File system and device identifiers</title>
<simpara>This sections explains the difference between persistent attributes identifying file systems and block devices.</simpara>
<bridgehead xml:id="file_system_identifiers" renderas="sect3" remap="_file_system_identifiers">File system identifiers</bridgehead>
<simpara>File system identifiers are tied to a particular file system created on a block device. The identifier is also stored as part of the file system. If you copy the file system to a different device, it still carries the same file system identifier. On the other hand, if you rewrite the device, such as by formatting it with the <literal>mkfs</literal> utility, the device loses the attribute.</simpara>
<simpara>File system identifiers include:</simpara>
<itemizedlist>
<listitem>
<simpara>Unique identifier (UUID)</simpara>
</listitem>
<listitem>
<simpara>Label</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="device_identifiers" renderas="sect3" remap="_device_identifiers">Device identifiers</bridgehead>
<simpara>Device identifiers are tied to a block device: for example, a disk or a partition. If you rewrite the device, such as by formatting it with the <literal>mkfs</literal> utility, the device keeps the attribute, because it is not stored in the file system.</simpara>
<simpara>Device identifiers include:</simpara>
<itemizedlist>
<listitem>
<simpara>World Wide Identifier (WWID)</simpara>
</listitem>
<listitem>
<simpara>Partition UUID</simpara>
</listitem>
<listitem>
<simpara>Serial number</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="recommendations" renderas="sect3" remap="_recommendations">Recommendations</bridgehead>
<itemizedlist>
<listitem>
<simpara>Some file systems, such as logical volumes, span multiple devices. Red Hat recommends accessing these file systems using file system identifiers rather than device identifiers.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="con_device-names-managed-by-the-udev-mechanism-in-dev-disk-_assembly_overview-of-persistent-naming-attributes">
<title>Device names managed by the udev mechanism in /dev/disk/</title>
<simpara>This section lists different kinds of persistent naming attributes that the <literal>udev</literal> service provides in the <literal role="filename">/dev/disk/</literal> directory.</simpara>
<simpara>The <literal>udev</literal> mechanism is used for all types of devices in Linux, not just for storage devices. In the case of storage devices, Red Hat Enterprise Linux contains <literal>udev</literal> rules that create symbolic links in the <literal role="filename">/dev/disk/</literal> directory.  This enables you to refer to storage devices by:</simpara>
<itemizedlist>
<listitem>
<simpara>Their content</simpara>
</listitem>
<listitem>
<simpara>A unique identifier</simpara>
</listitem>
<listitem>
<simpara>Their serial number.</simpara>
</listitem>
</itemizedlist>
<simpara>Although <literal>udev</literal> naming attributes are persistent, in that they do not change on their own across system reboots, some are also configurable.</simpara>
<section xml:id="file-system-identifiers_assembly_overview-of-persistent-naming-attributes">
<title>File system identifiers</title>
<bridgehead xml:id="the_uuid_attribute_in_dev_disk_by_uuid" renderas="sect3" remap="_the_uuid_attribute_in_dev_disk_by_uuid">The UUID attribute in /dev/disk/by-uuid/</bridgehead>
<simpara>Entries in this directory provide a symbolic name that refers to the storage device by a <emphasis role="strong">unique identifier</emphasis> (UUID) in the content (that is, the data) stored on the device. For example:</simpara>
<screen>/dev/disk/by-uuid/<emphasis>3e6be9de-8139-11d1-9106-a43f08d823a6</emphasis></screen>
<simpara>You can use the UUID to refer to the device in the <literal role="filename">/etc/fstab</literal> file using the following syntax:</simpara>
<screen>UUID=<emphasis>3e6be9de-8139-11d1-9106-a43f08d823a6</emphasis></screen>
<simpara>You can configure the UUID attribute when creating a file system, and you can also change it later on.</simpara>
<bridgehead xml:id="the_label_attribute_in_dev_disk_by_label" renderas="sect3" remap="_the_label_attribute_in_dev_disk_by_label">The Label attribute in /dev/disk/by-label/</bridgehead>
<simpara>Entries in this directory provide a symbolic name that refers to the storage device by a <emphasis role="strong">label</emphasis> in the content (that is, the data) stored on the device.</simpara>
<simpara>For example:</simpara>
<screen>/dev/disk/by-label/<emphasis>Boot</emphasis></screen>
<simpara>You can use the label to refer to the device in the <literal role="filename">/etc/fstab</literal> file using the following syntax:</simpara>
<screen>LABEL=<emphasis>Boot</emphasis></screen>
<simpara>You can configure the Label attribute when creating a file system, and you can also change it later on.</simpara>
</section>
<section xml:id="device-identifiers_assembly_overview-of-persistent-naming-attributes">
<title>Device identifiers</title>
<bridgehead xml:id="the_wwid_attribute_in_dev_disk_by_id" renderas="sect3" remap="_the_wwid_attribute_in_dev_disk_by_id">The WWID attribute in /dev/disk/by-id/</bridgehead>
<simpara>The World Wide Identifier (WWID) is a persistent, <emphasis role="strong">system-independent identifier</emphasis> that the SCSI Standard requires from all SCSI devices. The WWID identifier is guaranteed to be unique for every storage device, and independent of the path that is used to access the device. The identifier is a property of the device but is not stored in the content (that is, the data) on the devices.</simpara>
<simpara>This identifier can be obtained by issuing a SCSI Inquiry to retrieve the Device Identification Vital Product Data (page <literal>0x83</literal>) or Unit Serial Number (page <literal>0x80</literal>).</simpara>
<simpara>Red Hat Enterprise Linux automatically maintains the proper mapping from the WWID-based device name to a current <literal>/dev/sd</literal> name on that system. Applications can use the <literal>/dev/disk/by-id/</literal> name to reference the data on the disk, even if the path to the device changes, and even when accessing the device from different systems.</simpara>
<example>
<title>WWID mappings</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">WWID symlink</entry>
<entry align="left" valign="top">Non-persistent device</entry>
<entry align="left" valign="top">Note</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-id/scsi-3600508b400105e210000900000490000</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sda</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A device with a page <literal>0x83</literal> identifier</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-id/scsi-SSEAGATE_ST373453LW_3HW1RHM6</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sdb</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A device with a page <literal>0x80</literal> identifier</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-id/ata-SAMSUNG_MZNLN256HMHQ-000L7_S2WDNX0J336519-part3</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sdc3</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A disk partition</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>In addition to these persistent names provided by the system, you can also use <literal>udev</literal> rules to implement persistent names of your own, mapped to the WWID of the storage.</simpara>
<bridgehead xml:id="the_partition_uuid_attribute_in_dev_disk_by_partuuid" renderas="sect3" remap="_the_partition_uuid_attribute_in_dev_disk_by_partuuid">The Partition UUID attribute in /dev/disk/by-partuuid</bridgehead>
<simpara>The Partition UUID (PARTUUID) attribute identifies partitions as defined by GPT partition table.</simpara>
<example>
<title>Partition UUID mappings</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="60*"/>
<colspec colname="col_2" colwidth="40*"/>
<thead>
<row>
<entry align="left" valign="top">PARTUUID symlink</entry>
<entry align="left" valign="top">Non-persistent device</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-partuuid/4cd1448a-01</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sda1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-partuuid/4cd1448a-02</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sda2</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/disk/by-partuuid/4cd1448a-03</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal role="filename">/dev/sda3</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<bridgehead xml:id="the_path_attribute_in_dev_disk_by_path" renderas="sect3" remap="_the_path_attribute_in_dev_disk_by_path">The Path attribute in /dev/disk/by-path/</bridgehead>
<simpara>This attribute provides a symbolic name that refers to the storage device by the <emphasis role="strong">hardware path</emphasis> used to access the device.</simpara>
<simpara>The Path attribute fails if any part of the hardware path (for example, the PCI ID, target port, or LUN number) changes. The Path attribute is therefore unreliable. However, the Path attribute may be useful in one of the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>You need to identify a disk that you are planning to replace later.</simpara>
</listitem>
<listitem>
<simpara>You plan to install a storage service on a disk in a specific location.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="con_the-world-wide-identifier-with-dm-multipath_assembly_overview-of-persistent-naming-attributes">
<title>The World Wide Identifier with DM Multipath</title>
<simpara>This section describes the mapping between the World Wide Identifier (WWID) and non-persistent device names in a Device Mapper Multipath configuration.</simpara>
<simpara>If there are multiple paths from a system to a device, DM Multipath uses the WWID to detect this. DM Multipath then presents a single "pseudo-device" in the <literal role="filename">/dev/mapper/wwid</literal> directory, such as <literal role="filename">/dev/mapper/3600508b400105df70000e00000ac0000</literal>.</simpara>
<simpara>The command <literal role="command">multipath -l</literal> shows the mapping to the non-persistent identifiers:</simpara>
<itemizedlist>
<listitem>
<simpara><literal><emphasis>Host</emphasis>:<emphasis>Channel</emphasis>:<emphasis>Target</emphasis>:<emphasis>LUN</emphasis></literal></simpara>
</listitem>
<listitem>
<simpara><literal>/dev/sd</literal> name</simpara>
</listitem>
<listitem>
<simpara><literal><emphasis>major</emphasis>:<emphasis>minor</emphasis></literal> number</simpara>
</listitem>
</itemizedlist>
<example>
<title>WWID mappings in a multipath configuration</title>
<simpara>An example output of the <literal role="command">multipath -l</literal> command:</simpara>
<screen>3600508b400105df70000e00000ac0000 dm-2 vendor,product
[size=20G][features=1 queue_if_no_path][hwhandler=0][rw]
\_ round-robin 0 [prio=0][active]
 \_ 5:0:1:1 sdc 8:32  [active][undef]
 \_ 6:0:1:1 sdg 8:96  [active][undef]
\_ round-robin 0 [prio=0][enabled]
 \_ 5:0:0:1 sdb 8:16  [active][undef]
 \_ 6:0:0:1 sdf 8:80  [active][undef]</screen>
</example>
<simpara>DM Multipath automatically maintains the proper mapping of each WWID-based device name to its corresponding <literal>/dev/sd</literal> name on the system. These names are persistent across path changes, and they are consistent when accessing the device from different systems.</simpara>
<simpara>When the <literal role="option">user_friendly_names</literal> feature of DM Multipath is used, the WWID is mapped to a name of the form <literal>/dev/mapper/mpath<emphasis>N</emphasis></literal>. By default, this mapping is maintained in the file <literal role="filename">/etc/multipath/bindings</literal>. These <literal>mpath<emphasis>N</emphasis></literal> names are persistent as long as that file is maintained.</simpara>
<important>
<simpara>If you use <literal role="option">user_friendly_names</literal>, then additional steps are required to obtain consistent names in a cluster.</simpara>
</important>
</section>
<section xml:id="con_limitations-of-the-udev-device-naming-convention_assembly_overview-of-persistent-naming-attributes">
<title>Limitations of the udev device naming convention</title>
<simpara>The following are some limitations of the <literal>udev</literal> naming convention:</simpara>
<itemizedlist>
<listitem>
<simpara>It is possible that the device might not be accessible at the time the query is performed because the <literal>udev</literal> mechanism might rely on the ability to query the storage device when the <literal>udev</literal> rules are processed for a <literal>udev</literal> event. This is more likely to occur with Fibre Channel, iSCSI or FCoE storage devices when the device is not located in the server chassis.</simpara>
</listitem>
<listitem>
<simpara>The kernel might send <literal>udev</literal> events at any time, causing the rules to be processed and possibly causing the <literal role="filename">/dev/disk/by-*/</literal> links to be removed if the device is not accessible.</simpara>
</listitem>
<listitem>
<simpara>There might be a delay between when the <literal>udev</literal> event is generated and when it is processed, such as when a large number of devices are detected and the user-space <literal>udevd</literal> service takes some amount of time to process the rules for each one. This might cause a delay between when the kernel detects the device and when the <literal role="filename">/dev/disk/by-*/</literal> names are available.</simpara>
</listitem>
<listitem>
<simpara>External programs such as <literal>blkid</literal> invoked by the rules might open the device for a brief period of time, making the device inaccessible for other uses.</simpara>
</listitem>
<listitem>
<simpara>The device names managed by the <literal>udev</literal> mechanism in /dev/disk/ may change between major releases, requiring you to update the links.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_listing-persistent-naming-attributes_assembly_overview-of-persistent-naming-attributes">
<title>Listing persistent naming attributes</title>
<simpara>This procedure describes how to find out the persistent naming attributes of non-persistent storage devices.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To list the UUID and Label attributes, use the <literal>lsblk</literal> utility:</simpara>
<screen>$ lsblk --fs <emphasis><phrase role="replaceable">storage-device</phrase></emphasis></screen>
<simpara>For example:</simpara>
<example>
<title>Viewing the UUID and Label of a file system</title>
<screen>$ lsblk --fs /dev/sda1

NAME FSTYPE <emphasis role="strong">LABEL</emphasis> <emphasis role="strong">UUID</emphasis>                                 MOUNTPOINT
sda1 xfs    <emphasis role="strong">Boot</emphasis>  <emphasis role="strong">afa5d5e3-9050-48c3-acc1-bb30095f3dc4</emphasis> /boot</screen>
</example>
</listitem>
<listitem>
<simpara>To list the PARTUUID attribute, use the <literal>lsblk</literal> utility with the <literal role="option">--output +PARTUUID</literal> option:</simpara>
<screen>$ lsblk --output +PARTUUID</screen>
<simpara>For example:</simpara>
<example>
<title>Viewing the PARTUUID attribute of a partition</title>
<screen>$ lsblk --output +PARTUUID /dev/sda1

NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT <emphasis role="strong">PARTUUID</emphasis>
sda1   8:1    0  512M  0 part /boot      <emphasis role="strong">4cd1448a-01</emphasis></screen>
</example>
</listitem>
<listitem>
<simpara>To list the WWID attribute, examine the targets of symbolic links in the <literal role="filename">/dev/disk/by-id/</literal> directory. For example:</simpara>
<example>
<title>Viewing the WWID of all storage devices on the system</title>
<screen>$ file /dev/disk/by-id/*

/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001
symbolic link to ../../sda
/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001-part1
symbolic link to ../../sda1
/dev/disk/by-id/ata-QEMU_HARDDISK_QM00001-part2
symbolic link to ../../sda2
/dev/disk/by-id/dm-name-rhel_rhel8-root
symbolic link to ../../dm-0
/dev/disk/by-id/dm-name-rhel_rhel8-swap
symbolic link to ../../dm-1
/dev/disk/by-id/dm-uuid-LVM-QIWtEHtXGobe5bewlIUDivKOz5ofkgFhP0RMFsNyySVihqEl2cWWbR7MjXJolD6g
symbolic link to ../../dm-1
/dev/disk/by-id/dm-uuid-LVM-QIWtEHtXGobe5bewlIUDivKOz5ofkgFhXqH2M45hD2H9nAf2qfWSrlRLhzfMyOKd
symbolic link to ../../dm-0
/dev/disk/by-id/lvm-pv-uuid-atlr2Y-vuMo-ueoH-CpMG-4JuH-AhEF-wu4QQm
symbolic link to ../../sda2</screen>
</example>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_modifying-persistent-naming-attributes_assembly_overview-of-persistent-naming-attributes">
<title>Modifying persistent naming attributes</title>
<simpara>This procedure describes how to change the UUID or Label persistent naming attribute of a file system.</simpara>
<note>
<simpara>Changing <literal>udev</literal> attributes happens in the background and might take a long time. The <literal role="command">udevadm settle</literal> command waits until the change is fully registered, which ensures that your next command will be able to utilize the new attribute correctly.</simpara>
</note>
<simpara>In the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">new-uuid</phrase></emphasis> with the UUID you want to set; for example, <literal>1cdfbc07-1c90-4984-b5ec-f61943f5ea50</literal>. You can generate a UUID using the <literal role="command">uuidgen</literal> command.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">new-label</phrase></emphasis> with a label; for example, <literal>backup_data</literal>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you are modifying the attributes of an XFS file system, unmount it first.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change the UUID or Label attributes of an <emphasis role="strong">XFS</emphasis> file system, use the <literal>xfs_admin</literal> utility:</simpara>
<screen># xfs_admin -U <emphasis><phrase role="replaceable">new-uuid</phrase></emphasis> -L <emphasis><phrase role="replaceable">new-label</phrase></emphasis> <emphasis><phrase role="replaceable">storage-device</phrase></emphasis>
# udevadm settle</screen>
</listitem>
<listitem>
<simpara>To change the UUID or Label attributes of an <emphasis role="strong">ext4</emphasis>, <emphasis role="strong">ext3</emphasis>, or <emphasis role="strong">ext2</emphasis> file system, use the <literal>tune2fs</literal> utility:</simpara>
<screen># tune2fs -U <emphasis><phrase role="replaceable">new-uuid</phrase></emphasis> -L <emphasis><phrase role="replaceable">new-label</phrase></emphasis> <emphasis><phrase role="replaceable">storage-device</phrase></emphasis>
# udevadm settle</screen>
</listitem>
<listitem>
<simpara>To change the UUID or Label attributes of a swap volume, use the <literal>swaplabel</literal> utility:</simpara>
<screen># swaplabel --uuid <emphasis><phrase role="replaceable">new-uuid</phrase></emphasis> --label <emphasis><phrase role="replaceable">new-label</phrase></emphasis> <emphasis><phrase role="replaceable">swap-device</phrase></emphasis>
# udevadm settle</screen>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="assembly_getting-started-with-partitions_managing-file-systems">
<title>Getting started with partitions</title>
<simpara>As a system administrator, you can use the following procedures to create, delete, and modify various types of disk partitions.</simpara>
<simpara>For an overview of the advantages and disadvantages to using partitions on block devices, see the following KBase article: <link xlink:href="https://access.redhat.com/solutions/163853">https://access.redhat.com/solutions/163853</link>.</simpara>
<section xml:id="assembly_viewing-the-partition-table_assembly_getting-started-with-partitions">
<title>Viewing the partition table</title>
<simpara>As a system administrator, you can display the partition table of a block device to see the partition layout and details about individual partitions.</simpara>
<section xml:id="proc_viewing-the-partition-table-with-parted_assembly_viewing-the-partition-table">
<title>Viewing the partition table with parted</title>
<simpara>This procedure describes how to view the partition table on a block device using the <literal>parted</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the interactive <literal>parted</literal> shell:</simpara>
<screen># parted <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device you want to examine: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>View the partition table:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Optionally, use the following command to switch to another device you want to examine next:</simpara>
<screen>(parted) select <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>parted(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ref_example-output-of-parted-print_assembly_viewing-the-partition-table">
<title>Example output of <literal>parted print</literal></title>
<simpara>This section provides an example output of the <literal>print</literal> command in the <literal>parted</literal> shell and describes fields in the output.</simpara>
<example>
<title>Output of the <literal>print</literal> command</title>
<screen>Model: ATA SAMSUNG MZNLN256 (scsi)
Disk /dev/sda: 256GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags:

Number  Start   End     Size    Type      File system  Flags
 1      1049kB  269MB   268MB   primary   xfs          boot
 2      269MB   34.6GB  34.4GB  primary
 3      34.6GB  45.4GB  10.7GB  primary
 4      45.4GB  256GB   211GB   extended
 5      45.4GB  256GB   211GB   logical</screen>
<simpara>Following is a description of the fields:</simpara>
<variablelist>
<varlistentry>
<term><literal>Model: ATA SAMSUNG MZNLN256 (scsi)</literal></term>
<listitem>
<simpara>The disk type, manufacturer, model number, and interface.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Disk /dev/sda: 256GB</literal></term>
<listitem>
<simpara>The file path to the block device and the storage capacity.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Partition Table: msdos</literal></term>
<listitem>
<simpara>The disk label type.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Number</literal></term>
<listitem>
<simpara>The partition number. For example, the partition with minor number 1 corresponds to <literal role="filename">/dev/sda1</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Start</literal> and <literal>End</literal></term>
<listitem>
<simpara>The location on the device where the partition starts and ends.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Type</literal></term>
<listitem>
<simpara>Valid types are metadata, free, primary, extended, or logical.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>File system</literal></term>
<listitem>
<simpara>The file system type. If the <literal>File system</literal> field of a device shows no value, this means that its file system type is unknown. The <literal>parted</literal> utility cannot recognize the file system on encrypted devices.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Flags</literal></term>
<listitem>
<simpara>Lists the flags set for the partition. Available flags are <literal>boot</literal>, <literal>root</literal>, <literal>swap</literal>, <literal>hidden</literal>, <literal>raid</literal>, <literal>lvm</literal>, or <literal>lba</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</example>
</section>
</section>
<section xml:id="assembly_creating-a-partition-table-on-a-disk_assembly_getting-started-with-partitions">
<title>Creating a partition table on a disk</title>
<simpara>As a system administrator, you can format a block device with different types of partition tables to enable using partitions on the device.</simpara>
<warning>
<simpara>Formatting a block device with a partition table deletes all data stored on the device.</simpara>
</warning>
<section xml:id="con_considerations-before-modifying-partitions-on-a-disk_assembly_creating-a-partition-table-on-a-disk">
<title>Considerations before modifying partitions on a disk</title>
<simpara>This section lists key points to consider before creating, removing, or resizing partitions.</simpara>
<note>
<simpara>This section does not cover the DASD partition table, which is specific to the IBM Z architecture. For information on DASD, see:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/configuring-a-linux-instance-on-ibm-z_installing-rhel">Configuring a Linux instance on IBM Z</link></simpara>
</listitem>
<listitem>
<simpara>The <link xlink:href="https://www.ibm.com/support/knowledgecenter/linuxonibm/com.ibm.linux.z.lgdd/lgdd_c_dasd_know.html">What you should know about DASD</link> article at the IBM Knowledge Center</simpara>
</listitem>
</itemizedlist>
</note>
<bridgehead xml:id="the_maximum_number_of_partitions" renderas="sect4" remap="_the_maximum_number_of_partitions">The maximum number of partitions</bridgehead>
<simpara>The number of partitions on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, you can have either:</simpara>
<itemizedlist>
<listitem>
<simpara>Up to four primary partitions, or</simpara>
</listitem>
<listitem>
<simpara>Up to three primary partitions, one extended partition, and multiple logical partitions within the extended.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum number of partitions is 128. While the GPT specification allows for more partitions by growing the area reserved for the partition table, common practice used by the <literal>parted</literal> utility is to limit it to enough area for 128 partitions.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Red Hat recommends that, unless you have a reason for doing otherwise, you should <emphasis>at least</emphasis> create the following partitions: <literal>swap</literal>, <literal>/boot/</literal>, and <literal>/</literal> (root).</simpara>
</note>
<bridgehead xml:id="the_maximum_size_of_a_partition" renderas="sect4" remap="_the_maximum_size_of_a_partition">The maximum size of a partition</bridgehead>
<simpara>The size of a partition on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, the maximum size is 2TiB.</simpara>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum size is 8ZiB.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to create a partition larger than 2TiB, the disk must be formatted with GPT.</simpara>
<bridgehead xml:id="size_alignment" renderas="sect4" remap="_size_alignment">Size alignment</bridgehead>
<simpara>The <literal>parted</literal> utility enables you to specify partition size using multiple different suffixes:</simpara>
<variablelist>
<varlistentry>
<term>MiB, GiB, or TiB</term>
<listitem>
<simpara>Size expressed in powers of 2.</simpara>
<itemizedlist>
<listitem>
<simpara>The starting point of the partition is aligned to the exact sector specified by size.</simpara>
</listitem>
<listitem>
<simpara>The ending point is aligned to the specified size minus 1 sector.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>MB, GB, or TB</term>
<listitem>
<simpara>Size expressed in powers of 10.</simpara>
<simpara>The starting and ending point is aligned within one half of the specified unit: for example, ±500KB when using the MB suffix.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="ref_comparison-of-partition-table-types_assembly_creating-a-partition-table-on-a-disk">
<title>Comparison of partition table types</title>
<simpara>This section compares the properties of different types of partition tables that you can create on a block device.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Partition table types</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Partition table</entry>
<entry align="left" valign="top">Maximum number of partitions</entry>
<entry align="left" valign="top">Maximum partition size</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Master Boot Record (MBR)</simpara></entry>
<entry align="left" valign="top"><simpara>4 primary, or 3 primary and 12 logical inside an extended partition</simpara></entry>
<entry align="left" valign="top"><simpara>2TiB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>GUID Partition Table (GPT)</simpara></entry>
<entry align="left" valign="top"><simpara>128</simpara></entry>
<entry align="left" valign="top"><simpara>8ZiB</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="mbr-disk-partitions_assembly_creating-a-partition-table-on-a-disk">
<title>MBR disk partitions</title>
<simpara>The diagrams in this chapter show the partition table as being separate from the actual disk. However, this is not entirely accurate. In reality, the partition table is
stored at the very start of the disk, before any file system or user data, but for clarity, they are separate in the following diagrams.</simpara>
<figure>
<title>Disk with MBR partition table</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/unused-partitioned-drive.svg"/>
</imageobject>
<textobject><phrase>unused partitioned drive</phrase></textobject>
</mediaobject>
</figure>
<simpara>As the previous diagram shows, the partition table is divided into four sections of four primary partitions. A primary partition is a partition on a hard drive that can contain
only one logical drive (or section). Each section can hold the information necessary to define a single partition, meaning that the partition table can define no more
than four partitions.</simpara>
<simpara>Each partition table entry contains several important characteristics of the partition:</simpara>
<itemizedlist>
<listitem>
<simpara>The points on the disk where the partition starts and ends.</simpara>
</listitem>
<listitem>
<simpara>Whether the partition is <emphasis role="strong">active</emphasis>. Only one partition can be flagged as <emphasis role="strong">active</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>The partition’s type.</simpara>
</listitem>
</itemizedlist>
<simpara>The starting and ending points define the partition’s size and location on the disk. The "active" flag is used by some operating systems boot loaders. In other words, the operating
system in the partition that is marked "active" is booted, in this case.</simpara>
<simpara>The type is a number that identifies the partition’s anticipated usage. Some operating systems use the partition type to denote a specific file system type, to flag
the partition as being associated with a particular operating system, to indicate that the partition contains a bootable operating system, or some combination of the three.</simpara>
<simpara>The following diagram shows an example of a drive with single partition:</simpara>
<figure>
<title>Disk with a single partition</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/dos-single-partition.svg"/>
</imageobject>
<textobject><phrase>dos single partition</phrase></textobject>
</mediaobject>
</figure>
<simpara>The single partition in this example is labeled as <literal>DOS</literal>. This label shows the partition type, with <literal>DOS</literal> being one of the most common ones.</simpara>
</section>
<section xml:id="extended-mbr-partitions_assembly_creating-a-partition-table-on-a-disk">
<title>Extended MBR partitions</title>
<simpara>In case four partitions are insufficient for your needs, you can use extended partitions to create up additional partitions. You can do this by setting the type of partition to
"Extended".</simpara>
<simpara>An extended partition is like a disk drive in its own right - it has its own partition table, which points to one or more partitions (now called logical partitions, as opposed to
the four primary partitions), contained entirely within the extended partition itself. The following diagram shows a disk drive with one primary partition and one extended
partition containing two logical partitions (along with some unpartitioned free space):</simpara>
<figure>
<title>Disk with both a primary and an extended MBR partition</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/extended-partitions.svg"/>
</imageobject>
<textobject><phrase>extended partitions</phrase></textobject>
</mediaobject>
</figure>
<simpara>As this figure implies, there is a difference between primary and logical partitions - there can be only up to four primary and extended partitions,
but there is no fixed limit to the number of logical partitions that can exist. However, due to the way in which partitions are accessed in Linux,
no more than 15 logical partitions can be defined on a single disk drive.</simpara>
</section>
<section xml:id="mbr-partition-types_assembly_creating-a-partition-table-on-a-disk">
<title>MBR partition types</title>
<simpara>The table below shows a list of some of the commonly used MBR partition types and hexadecimal numbers used to represent them.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MBR partition types</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">MBR partition type</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Value</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">MBR partition type</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Value</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Empty</simpara></entry>
<entry align="left" valign="top"><simpara>00</simpara></entry>
<entry align="left" valign="top"><simpara>Novell Netware 386</simpara></entry>
<entry align="left" valign="top"><simpara>65</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>DOS 12-bit FAT</simpara></entry>
<entry align="left" valign="top"><simpara>01</simpara></entry>
<entry align="left" valign="top"><simpara>PIC/IX</simpara></entry>
<entry align="left" valign="top"><simpara>75</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>XENIX root</simpara></entry>
<entry align="left" valign="top"><simpara>O2</simpara></entry>
<entry align="left" valign="top"><simpara>Old MINIX</simpara></entry>
<entry align="left" valign="top"><simpara>80</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>XENIX usr</simpara></entry>
<entry align="left" valign="top"><simpara>O3</simpara></entry>
<entry align="left" valign="top"><simpara>Linux/MINUX</simpara></entry>
<entry align="left" valign="top"><simpara>81</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>DOS 16-bit ⇐32M</simpara></entry>
<entry align="left" valign="top"><simpara>04</simpara></entry>
<entry align="left" valign="top"><simpara>Linux swap</simpara></entry>
<entry align="left" valign="top"><simpara>82</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Extended</simpara></entry>
<entry align="left" valign="top"><simpara>05</simpara></entry>
<entry align="left" valign="top"><simpara>Linux native</simpara></entry>
<entry align="left" valign="top"><simpara>83</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>DOS 16-bit &gt;=32</simpara></entry>
<entry align="left" valign="top"><simpara>06</simpara></entry>
<entry align="left" valign="top"><simpara>Linux extended</simpara></entry>
<entry align="left" valign="top"><simpara>85</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>OS/2 HPFS</simpara></entry>
<entry align="left" valign="top"><simpara>07</simpara></entry>
<entry align="left" valign="top"><simpara>Amoeba</simpara></entry>
<entry align="left" valign="top"><simpara>93</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>AIX</simpara></entry>
<entry align="left" valign="top"><simpara>08</simpara></entry>
<entry align="left" valign="top"><simpara>Amoeba BBT</simpara></entry>
<entry align="left" valign="top"><simpara>94</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>AIX bootable</simpara></entry>
<entry align="left" valign="top"><simpara>09</simpara></entry>
<entry align="left" valign="top"><simpara>BSD/386</simpara></entry>
<entry align="left" valign="top"><simpara>a5</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>OS/2 Boot Manager</simpara></entry>
<entry align="left" valign="top"><simpara>0a</simpara></entry>
<entry align="left" valign="top"><simpara>OpenBSD</simpara></entry>
<entry align="left" valign="top"><simpara>a6</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Win95 FAT32</simpara></entry>
<entry align="left" valign="top"><simpara>0b</simpara></entry>
<entry align="left" valign="top"><simpara>NEXTSTEP</simpara></entry>
<entry align="left" valign="top"><simpara>a7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Win95 FAT32 (LBA)</simpara></entry>
<entry align="left" valign="top"><simpara>0c</simpara></entry>
<entry align="left" valign="top"><simpara>BSDI fs</simpara></entry>
<entry align="left" valign="top"><simpara>b7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Win95 FAT16 (LBA)</simpara></entry>
<entry align="left" valign="top"><simpara>0e</simpara></entry>
<entry align="left" valign="top"><simpara>BSDI swap</simpara></entry>
<entry align="left" valign="top"><simpara>b8</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Win95 Extended (LBA)</simpara></entry>
<entry align="left" valign="top"><simpara>0f</simpara></entry>
<entry align="left" valign="top"><simpara>Syrinx</simpara></entry>
<entry align="left" valign="top"><simpara>c7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Venix 80286</simpara></entry>
<entry align="left" valign="top"><simpara>40</simpara></entry>
<entry align="left" valign="top"><simpara>CP/M</simpara></entry>
<entry align="left" valign="top"><simpara>db</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Novell</simpara></entry>
<entry align="left" valign="top"><simpara>51</simpara></entry>
<entry align="left" valign="top"><simpara>DOS access</simpara></entry>
<entry align="left" valign="top"><simpara>e1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>PRep Boot</simpara></entry>
<entry align="left" valign="top"><simpara>41</simpara></entry>
<entry align="left" valign="top"><simpara>DOS R/O</simpara></entry>
<entry align="left" valign="top"><simpara>e3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>GNU HURD</simpara></entry>
<entry align="left" valign="top"><simpara>63</simpara></entry>
<entry align="left" valign="top"><simpara>DOS secondary</simpara></entry>
<entry align="left" valign="top"><simpara>f2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Novell Netware 286</simpara></entry>
<entry align="left" valign="top"><simpara>64</simpara></entry>
<entry align="left" valign="top"><simpara>BBT</simpara></entry>
<entry align="left" valign="top"><simpara>ff</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="guid-partition-table_assembly_creating-a-partition-table-on-a-disk">
<title>GUID Partition Table</title>
<simpara>The GUID Partition Table (GPT) is a partitioning scheme based on using Globally Unique Identifier (GUID). GPT was developed to cope with limitations of the MBR partition table,
especially with the limited maximum addressable storage space of a disk. Unlike MBR, which is unable to address storage larger than 2 TiB (equivalent to approximately 2.2 TB),
GPT is used with hard disks larger than this; the maximum addressable disk size is 2.2 ZiB. In addition, GPT, by default, supports creating up to 128 primary partitions.
This number could be extended by allocating more space to the partition table.</simpara>
<note>
<simpara>A GPT has partition types based on GUIDs. Note that certain partitions require a specific GUID. For example, the system partition for EFI boot loaders require GUID
<literal>C12A7328-F81F-11D2-BA4B-00A0C93EC93B</literal>.</simpara>
</note>
<simpara>GPT disks use logical block addressing (LBA) and the partition layout is as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>To preserve backward compatibility with MBR disks, the first sector (LBA 0) of GPT is reserved for MBR data and it is called "protective MBR".</simpara>
</listitem>
<listitem>
<simpara>The primary GPT header begins on the second logical block (LBA 1) of the device. The header contains the disk GUID, the location of the primary partition table, the location
of the secondary GPT header, and CRC32 checksums of itself, and the primary partition table. It also specifies the number of partition entries on the table.</simpara>
</listitem>
<listitem>
<simpara>The primary GPT includes, by default 128 partition entries, each with an entry size of 128 bytes, its partition type GUID and unique partition GUID.</simpara>
</listitem>
<listitem>
<simpara>The secondary GPT is identical to the primary GPT. It is used mainly as a backup table for recovery in case the primary partition table is corrupted.</simpara>
</listitem>
<listitem>
<simpara>The secondary GPT header is located on the last logical sector of the disk and it can be used to recover GPT information in case the primary header is corrupted. It contains
the disk GUID, the location of the secondary partition table and the primary GPT header, CRC32 checksums of itself and the secondary partition table, and the number of possible
partition entries.</simpara>
</listitem>
</itemizedlist>
<figure>
<title>Disk with a GUID Partition Table</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/gpt-partition.svg"/>
</imageobject>
<textobject><phrase>gpt partition</phrase></textobject>
</mediaobject>
</figure>
<important>
<simpara>There must be a BIOS boot partition for the boot loader to be installed successfully onto a disk that contains a GPT (GUID Partition table). This includes disks initialized
 by <emphasis role="strong">Anaconda</emphasis>. If the disk already contains a BIOS boot partition, it can be reused.</simpara>
</important>
</section>
<section xml:id="proc_creating-a-partition-table-on-a-disk-with-parted_assembly_creating-a-partition-table-on-a-disk">
<title>Creating a partition table on a disk with parted</title>
<simpara>This procedure describes how to format a block device with a partition table using the <literal>parted</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the interactive <literal>parted</literal> shell:</simpara>
<screen># parted <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device where you want to create a partition table: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Determine if there already is a partition table on the device:</simpara>
<screen>(parted) print</screen>
<simpara>If the device already contains partitions, they will be deleted in the next steps.</simpara>
</listitem>
<listitem>
<simpara>Create the new partition table:</simpara>
<screen>(parted) mklabel <emphasis><phrase role="replaceable">table-type</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">table-type</phrase></emphasis> with with the intended partition table type:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>msdos</literal> for MBR</simpara>
</listitem>
<listitem>
<simpara><literal>gpt</literal> for GPT</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<example>
<title>Creating a GPT table</title>
<simpara>For example, to create a GPT table on the disk, use:</simpara>
<screen>(parted) mklabel gpt</screen>
</example>
<simpara>The changes start taking place as soon as you enter this command, so review it before executing it.</simpara>
</listitem>
<listitem>
<simpara>View the partition table to confirm that the partition table exists:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Exit the <literal>parted</literal> shell:</simpara>
<screen>(parted) quit</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>parted(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>Create partitions on the device. See <xref linkend="assembly_creating-a-partition_assembly_getting-started-with-partitions"/> for details.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assembly_creating-a-partition_assembly_getting-started-with-partitions">
<title>Creating a partition</title>
<simpara>As a system administrator, you can create new partitions on a disk.</simpara>
<section xml:id="con_considerations-before-modifying-partitions-on-a-disk_assembly_creating-a-partition">
<title>Considerations before modifying partitions on a disk</title>
<simpara>This section lists key points to consider before creating, removing, or resizing partitions.</simpara>
<note>
<simpara>This section does not cover the DASD partition table, which is specific to the IBM Z architecture. For information on DASD, see:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/configuring-a-linux-instance-on-ibm-z_installing-rhel">Configuring a Linux instance on IBM Z</link></simpara>
</listitem>
<listitem>
<simpara>The <link xlink:href="https://www.ibm.com/support/knowledgecenter/linuxonibm/com.ibm.linux.z.lgdd/lgdd_c_dasd_know.html">What you should know about DASD</link> article at the IBM Knowledge Center</simpara>
</listitem>
</itemizedlist>
</note>
<bridgehead xml:id="the_maximum_number_of_partitions_2" renderas="sect4" remap="_the_maximum_number_of_partitions_2">The maximum number of partitions</bridgehead>
<simpara>The number of partitions on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, you can have either:</simpara>
<itemizedlist>
<listitem>
<simpara>Up to four primary partitions, or</simpara>
</listitem>
<listitem>
<simpara>Up to three primary partitions, one extended partition, and multiple logical partitions within the extended.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum number of partitions is 128. While the GPT specification allows for more partitions by growing the area reserved for the partition table, common practice used by the <literal>parted</literal> utility is to limit it to enough area for 128 partitions.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Red Hat recommends that, unless you have a reason for doing otherwise, you should <emphasis>at least</emphasis> create the following partitions: <literal>swap</literal>, <literal>/boot/</literal>, and <literal>/</literal> (root).</simpara>
</note>
<bridgehead xml:id="the_maximum_size_of_a_partition_2" renderas="sect4" remap="_the_maximum_size_of_a_partition_2">The maximum size of a partition</bridgehead>
<simpara>The size of a partition on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, the maximum size is 2TiB.</simpara>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum size is 8ZiB.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to create a partition larger than 2TiB, the disk must be formatted with GPT.</simpara>
<bridgehead xml:id="size_alignment_2" renderas="sect4" remap="_size_alignment_2">Size alignment</bridgehead>
<simpara>The <literal>parted</literal> utility enables you to specify partition size using multiple different suffixes:</simpara>
<variablelist>
<varlistentry>
<term>MiB, GiB, or TiB</term>
<listitem>
<simpara>Size expressed in powers of 2.</simpara>
<itemizedlist>
<listitem>
<simpara>The starting point of the partition is aligned to the exact sector specified by size.</simpara>
</listitem>
<listitem>
<simpara>The ending point is aligned to the specified size minus 1 sector.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>MB, GB, or TB</term>
<listitem>
<simpara>Size expressed in powers of 10.</simpara>
<simpara>The starting and ending point is aligned within one half of the specified unit: for example, ±500KB when using the MB suffix.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="con_partition-types_assembly_creating-a-partition">
<title>Partition types</title>
<simpara>This section describes different attributes that specify the type of a partition.</simpara>
<bridgehead xml:id="partition_types_or_flags" renderas="sect4" remap="_partition_types_or_flags">Partition types or flags</bridgehead>
<simpara>The partition type, or flag, is used by a running system only rarely. However, the partition type matters to on-the-fly generators, such as <literal>systemd-gpt-auto-generator</literal>, which use the partition type to, for example, automatically identify and mount devices.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>parted</literal> utility provides some control of partition types by mapping the partition type to <emphasis>flags</emphasis>. The parted utility can handle only certain partition types: for example LVM, swap, or RAID.</simpara>
</listitem>
<listitem>
<simpara>The <literal>fdisk</literal> utility supports the full range of partition types by specifying hexadecimal codes.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="partition_file_system_type" renderas="sect4" remap="_partition_file_system_type">Partition file system type</bridgehead>
<simpara>The <literal>parted</literal> utility optionally accepts a file system type argument when creating a partition. The value is used to:</simpara>
<itemizedlist>
<listitem>
<simpara>Set the partition flags on MBR, or</simpara>
</listitem>
<listitem>
<simpara>Set the partition UUID type on GPT. For example, the <literal>swap</literal>, <literal>fat</literal>, or <literal>hfs</literal> file system types set different GUIDs. The default value is the Linux Data GUID.</simpara>
</listitem>
</itemizedlist>
<simpara>The argument does not modify the file system on the partition in any way. It only differentiates between the supported flags or GUIDs.</simpara>
<simpara>The following file system types are supported:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>xfs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ext2</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ext3</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ext4</literal></simpara>
</listitem>
<listitem>
<simpara><literal>fat16</literal></simpara>
</listitem>
<listitem>
<simpara><literal>fat32</literal></simpara>
</listitem>
<listitem>
<simpara><literal>hfs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>hfs+</literal></simpara>
</listitem>
<listitem>
<simpara><literal>linux-swap</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ntfs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>reiserfs</literal></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="partition-naming-scheme_assembly_creating-a-partition">
<title>Partition naming scheme</title>
<simpara>Red Hat Enterprise Linux uses a file-based naming scheme, with file names in the form of <literal>/dev/<emphasis>xxyN</emphasis></literal>.</simpara>
<simpara>Device and partition names consist of the following structure:</simpara>
<variablelist>
<varlistentry>
<term><literal><emphasis role="strong"><emphasis>/dev/</emphasis></emphasis></literal></term>
<listitem>
<simpara>This is the name of the directory in which all device files are located. Because partitions are placed on hard disks, and hard disks are devices, the files representing all possible partitions
are located in <literal>/dev</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal><emphasis role="strong"><emphasis>xx</emphasis></emphasis></literal></term>
<listitem>
<simpara>The first two letters of the partitions name indicate the type of device on which is the partition located, usually <literal>sd</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal><emphasis role="strong"><emphasis>y</emphasis></emphasis></literal></term>
<listitem>
<simpara>This letter indicates which device the partition is on. For example, <literal>/dev/sda</literal> for the first hard disk, <literal>/dev/sdb</literal> for the second, and so on. In systems with more than 26 drives,
you can use more letters. For example, <literal>/dev/sdaa1</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal><emphasis role="strong"><emphasis>N</emphasis></emphasis></literal></term>
<listitem>
<simpara>The final letter indicates the number that represents the partition. The first four (primary or extended) partitions are numbered <literal>1</literal> through <literal>4</literal>. Logical partitions start at <literal>5</literal>.
For example, <literal>/dev/sda3</literal> is the third primary or extended partition on the first hard disk, and <literal>/dev/sdb6</literal> is the second logical partition on the second hard disk.
Drive partition numbering applies only to MBR partition tables. Note that <emphasis role="strong"><emphasis>N</emphasis></emphasis> does not always mean partition.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Even if Red Hat Enterprise Linux can identify and refer to <emphasis>all</emphasis> types of disk partitions, it might not be able to read the file system and therefore access stored data on every
partition type. However, in many cases, it is possible to successfully access data on a partition dedicated to another operating system.</simpara>
</note>
</section>
<section xml:id="mount-points-and-disk-partitions_assembly_creating-a-partition">
<title>Mount points and disk partitions</title>
<simpara>In Red Hat Enterprise Linux, each partition is used to form part of the storage necessary to support a single set of files and directories. This is done using the process known as
<emphasis>mounting</emphasis>, which associates a partition with a directory. Mounting a partition makes its storage available starting at the specified directory, known as a <emphasis>mount point</emphasis>.</simpara>
<simpara>For example, if partition <literal>/dev/sda5</literal> is mounted on <literal>/usr/</literal>, that would mean that all files and directories under <literal>/usr/</literal> physically reside on <literal>/dev/sda5</literal>. So the file
<literal>/usr/share/doc/FAQ/txt/Linux-FAQ</literal> would be stored on <literal>/dev/sda5</literal>, while the file <literal>/etc/gdm/custom.conf</literal> would not.</simpara>
<simpara>Continuing the example, it is also possible that one or more directories below <literal>/usr/</literal> would be mount points for other partitions. For instance, a partition <literal>/dev/sda7</literal>
could be mounted on <literal>/usr/local</literal>, meaning that <literal>/usr/local/man/whatis</literal> would then reside on <literal>/dev/sda7</literal> rather than <literal>/dev/sda5</literal>.</simpara>
</section>
<section xml:id="proc_creating-a-partition-with-parted_assembly_creating-a-partition">
<title>Creating a partition with parted</title>
<simpara>This procedure describes how to create a new partition on a block device using the <literal>parted</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>There is a partition table on the disk. For details on how to format the disk, see <xref linkend="assembly_creating-a-partition-table-on-a-disk_assembly_getting-started-with-partitions"/>.</simpara>
</listitem>
<listitem>
<simpara>If the partition you want to create is larger than 2TiB, the disk must be formatted with the GUID Partition Table (GPT).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the interactive <literal>parted</literal> shell:</simpara>
<screen># parted <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device where you want to create a partition: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>View the current partition table to determine if there is enough free space:</simpara>
<screen>(parted) print</screen>
<itemizedlist>
<listitem>
<simpara>If there is not enough free space, you can resize an existing partition. For more information, see <xref linkend="assembly_resizing-a-partition_assembly_getting-started-with-partitions"/>.</simpara>
</listitem>
<listitem>
<simpara>From the partition table, determine:</simpara>
<itemizedlist>
<listitem>
<simpara>The start and end points of the new partition</simpara>
</listitem>
<listitem>
<simpara>On MBR, what partition type it should be.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the new partition:</simpara>
<screen>(parted) mkpart <emphasis><phrase role="replaceable">part-type</phrase></emphasis> <emphasis><phrase role="replaceable">name</phrase></emphasis> <emphasis><phrase role="replaceable">fs-type</phrase></emphasis> <emphasis><phrase role="replaceable">start</phrase></emphasis> <emphasis><phrase role="replaceable">end</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">part-type</phrase></emphasis> with with <literal>primary</literal>, <literal>logical</literal>, or <literal>extended</literal> based on what you decided from the partition table. This applies only to the MBR partition table.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">name</phrase></emphasis> with an arbitrary partition name. This is required for GPT partition tables.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">fs-type</phrase></emphasis> with any one of <literal>xfs</literal>, <literal>ext2</literal>, <literal>ext3</literal>, <literal>ext4</literal>, <literal>fat16</literal>, <literal>fat32</literal>, <literal>hfs</literal>, <literal>hfs+</literal>, <literal>linux-swap</literal>, <literal>ntfs</literal>, or <literal>reiserfs</literal>. The <emphasis><phrase role="replaceable">fs-type</phrase></emphasis> parameter is optional. Note that <literal>parted</literal> does not create the file system on the partition.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">start</phrase></emphasis> and <emphasis><phrase role="replaceable">end</phrase></emphasis> with the sizes that determine the starting and ending points of the partition, counting from the beginning of the disk. You can use size suffixes, such as <literal>512MiB</literal>, <literal>20GiB</literal>, or <literal>1.5TiB</literal>. The default size megabytes.</simpara>
</listitem>
</itemizedlist>
<example>
<title>Creating a small primary partition</title>
<simpara>For example, to create a primary partition from 1024MiB until 2048MiB on an MBR table, use:</simpara>
<screen>(parted) mkpart primary 1024MiB 2048MiB</screen>
</example>
<simpara>The changes start taking place as soon as you enter this command, so review it before executing it.</simpara>
</listitem>
<listitem>
<simpara>View the partition table to confirm that the created partition is in the partition table with the correct partition type, file system type, and size:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Exit the <literal>parted</literal> shell:</simpara>
<screen>(parted) quit</screen>
</listitem>
<listitem>
<simpara>Use the following command to wait for the system to register the new device node:</simpara>
<screen># udevadm settle</screen>
</listitem>
<listitem>
<simpara>Verify that the kernel recognizes the new partition:</simpara>
<screen># cat /proc/partitions</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>parted(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_setting-a-partition-type-with-fdisk_assembly_creating-a-partition">
<title>Setting a partition type with fdisk</title>
<simpara>This procedure describes how to set a partition type, or flag, using the <literal>fdisk</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>There is a partition on the disk.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the interactive <literal>fdisk</literal> shell:</simpara>
<screen># fdisk <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device where you want to set a partition type: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>View the current partition table to determine the minor partition number:</simpara>
<screen>Command (m for help): <emphasis role="strong"><phrase role="command">print</phrase></emphasis></screen>
<simpara>You can see the current partition type in the <literal>Type</literal> column and its corresponding type ID in the <literal>Id</literal> column.</simpara>
</listitem>
<listitem>
<simpara>Enter the partition type command and select a partition using its minor number:</simpara>
<screen>Command (m for help): <emphasis role="strong"><phrase role="command">type</phrase></emphasis>
Partition number (<emphasis><phrase role="replaceable">1,2,3</phrase></emphasis> default <emphasis><phrase role="replaceable">3</phrase></emphasis>): <emphasis><phrase role="replaceable"><emphasis role="strong">2</emphasis></phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Optionally, list the available hexadecimal codes:</simpara>
<screen>Hex code (type L to list all codes): <emphasis role="strong"><phrase role="command">L</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Set the partition type:</simpara>
<screen>Hex code (type L to list all codes): <emphasis><phrase role="replaceable"><emphasis role="strong">8e</emphasis></phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Write your changes and exit the <literal>fdisk</literal> shell:</simpara>
<screen>Command (m for help): <emphasis role="strong"><phrase role="command">write</phrase></emphasis>
The partition table has been altered.
Syncing disks.</screen>
</listitem>
<listitem>
<simpara>Verify your changes:</simpara>
<screen># fdisk --list <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="assembly_removing-a-partition_assembly_getting-started-with-partitions">
<title>Removing a partition</title>
<simpara>As a system administrator, you can remove a disk partition that is no longer used to free up disk space.</simpara>
<warning>
<simpara>Removing a partition deletes all data stored on the partition.</simpara>
</warning>
<section xml:id="con_considerations-before-modifying-partitions-on-a-disk_assembly_removing-a-partition">
<title>Considerations before modifying partitions on a disk</title>
<simpara>This section lists key points to consider before creating, removing, or resizing partitions.</simpara>
<note>
<simpara>This section does not cover the DASD partition table, which is specific to the IBM Z architecture. For information on DASD, see:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/configuring-a-linux-instance-on-ibm-z_installing-rhel">Configuring a Linux instance on IBM Z</link></simpara>
</listitem>
<listitem>
<simpara>The <link xlink:href="https://www.ibm.com/support/knowledgecenter/linuxonibm/com.ibm.linux.z.lgdd/lgdd_c_dasd_know.html">What you should know about DASD</link> article at the IBM Knowledge Center</simpara>
</listitem>
</itemizedlist>
</note>
<bridgehead xml:id="the_maximum_number_of_partitions_3" renderas="sect4" remap="_the_maximum_number_of_partitions_3">The maximum number of partitions</bridgehead>
<simpara>The number of partitions on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, you can have either:</simpara>
<itemizedlist>
<listitem>
<simpara>Up to four primary partitions, or</simpara>
</listitem>
<listitem>
<simpara>Up to three primary partitions, one extended partition, and multiple logical partitions within the extended.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum number of partitions is 128. While the GPT specification allows for more partitions by growing the area reserved for the partition table, common practice used by the <literal>parted</literal> utility is to limit it to enough area for 128 partitions.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Red Hat recommends that, unless you have a reason for doing otherwise, you should <emphasis>at least</emphasis> create the following partitions: <literal>swap</literal>, <literal>/boot/</literal>, and <literal>/</literal> (root).</simpara>
</note>
<bridgehead xml:id="the_maximum_size_of_a_partition_3" renderas="sect4" remap="_the_maximum_size_of_a_partition_3">The maximum size of a partition</bridgehead>
<simpara>The size of a partition on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, the maximum size is 2TiB.</simpara>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum size is 8ZiB.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to create a partition larger than 2TiB, the disk must be formatted with GPT.</simpara>
<bridgehead xml:id="size_alignment_3" renderas="sect4" remap="_size_alignment_3">Size alignment</bridgehead>
<simpara>The <literal>parted</literal> utility enables you to specify partition size using multiple different suffixes:</simpara>
<variablelist>
<varlistentry>
<term>MiB, GiB, or TiB</term>
<listitem>
<simpara>Size expressed in powers of 2.</simpara>
<itemizedlist>
<listitem>
<simpara>The starting point of the partition is aligned to the exact sector specified by size.</simpara>
</listitem>
<listitem>
<simpara>The ending point is aligned to the specified size minus 1 sector.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>MB, GB, or TB</term>
<listitem>
<simpara>Size expressed in powers of 10.</simpara>
<simpara>The starting and ending point is aligned within one half of the specified unit: for example, ±500KB when using the MB suffix.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="proc_removing-a-partition-with-parted_assembly_removing-a-partition">
<title>Removing a partition with parted</title>
<simpara>This procedure describes how to remove a disk partition using the <literal>parted</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the interactive <literal>parted</literal> shell:</simpara>
<screen># parted <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device where you want to remove a partition: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>View the current partition table to determine the minor number of the partition to remove:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Remove the partition:</simpara>
<screen>(parted) rm <emphasis><phrase role="replaceable">minor-number</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">minor-number</phrase></emphasis> with the minor number of the partition you want to remove: for example, <literal>3</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>The changes start taking place as soon as you enter this command, so review it before executing it.</simpara>
</listitem>
<listitem>
<simpara>Confirm that the partition is removed from the partition table:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Exit the <literal>parted</literal> shell:</simpara>
<screen>(parted) quit</screen>
</listitem>
<listitem>
<simpara>Verify that the kernel knows the partition is removed:</simpara>
<screen># cat /proc/partitions</screen>
</listitem>
<listitem>
<simpara>Remove the partition from the <literal role="filename">/etc/fstab</literal> file if it is present. Find the line that declares the removed partition, and remove it from the file.</simpara>
</listitem>
<listitem>
<simpara>Regenerate mount units so that your system registers the new <literal role="filename">/etc/fstab</literal> configuration:</simpara>
<screen># systemctl daemon-reload</screen>
</listitem>
<listitem>
<simpara>If you have deleted a swap partition or removed pieces of LVM, remove all references to the partition from the kernel command line in the <literal role="filename">/etc/default/grub</literal> file and regenerate GRUB configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>On a BIOS-based system:</simpara>
<screen># grub2-mkconfig --output=/etc/grub2.cfg</screen>
</listitem>
<listitem>
<simpara>On a UEFI-based system:</simpara>
<screen># grub2-mkconfig --output=/etc/grub2-efi.cfg</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To register the changes in the early boot system, rebuild the <literal>initramfs</literal> file system:</simpara>
<screen># dracut --force --verbose</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>parted(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assembly_resizing-a-partition_assembly_getting-started-with-partitions">
<title>Resizing a partition</title>
<simpara>As a system administrator, you can extend a partition to utilize unused disk space, or shrink a partition to use its capacity for different purposes.</simpara>
<section xml:id="con_considerations-before-modifying-partitions-on-a-disk_assembly_resizing-a-partition">
<title>Considerations before modifying partitions on a disk</title>
<simpara>This section lists key points to consider before creating, removing, or resizing partitions.</simpara>
<note>
<simpara>This section does not cover the DASD partition table, which is specific to the IBM Z architecture. For information on DASD, see:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/configuring-a-linux-instance-on-ibm-z_installing-rhel">Configuring a Linux instance on IBM Z</link></simpara>
</listitem>
<listitem>
<simpara>The <link xlink:href="https://www.ibm.com/support/knowledgecenter/linuxonibm/com.ibm.linux.z.lgdd/lgdd_c_dasd_know.html">What you should know about DASD</link> article at the IBM Knowledge Center</simpara>
</listitem>
</itemizedlist>
</note>
<bridgehead xml:id="the_maximum_number_of_partitions_4" renderas="sect4" remap="_the_maximum_number_of_partitions_4">The maximum number of partitions</bridgehead>
<simpara>The number of partitions on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, you can have either:</simpara>
<itemizedlist>
<listitem>
<simpara>Up to four primary partitions, or</simpara>
</listitem>
<listitem>
<simpara>Up to three primary partitions, one extended partition, and multiple logical partitions within the extended.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum number of partitions is 128. While the GPT specification allows for more partitions by growing the area reserved for the partition table, common practice used by the <literal>parted</literal> utility is to limit it to enough area for 128 partitions.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Red Hat recommends that, unless you have a reason for doing otherwise, you should <emphasis>at least</emphasis> create the following partitions: <literal>swap</literal>, <literal>/boot/</literal>, and <literal>/</literal> (root).</simpara>
</note>
<bridgehead xml:id="the_maximum_size_of_a_partition_4" renderas="sect4" remap="_the_maximum_size_of_a_partition_4">The maximum size of a partition</bridgehead>
<simpara>The size of a partition on a device is limited by the type of the partition table:</simpara>
<itemizedlist>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">Master Boot Record (MBR)</emphasis> partition table, the maximum size is 2TiB.</simpara>
</listitem>
<listitem>
<simpara>On a device formatted with the <emphasis role="strong">GUID Partition Table (GPT)</emphasis>, the maximum size is 8ZiB.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to create a partition larger than 2TiB, the disk must be formatted with GPT.</simpara>
<bridgehead xml:id="size_alignment_4" renderas="sect4" remap="_size_alignment_4">Size alignment</bridgehead>
<simpara>The <literal>parted</literal> utility enables you to specify partition size using multiple different suffixes:</simpara>
<variablelist>
<varlistentry>
<term>MiB, GiB, or TiB</term>
<listitem>
<simpara>Size expressed in powers of 2.</simpara>
<itemizedlist>
<listitem>
<simpara>The starting point of the partition is aligned to the exact sector specified by size.</simpara>
</listitem>
<listitem>
<simpara>The ending point is aligned to the specified size minus 1 sector.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>MB, GB, or TB</term>
<listitem>
<simpara>Size expressed in powers of 10.</simpara>
<simpara>The starting and ending point is aligned within one half of the specified unit: for example, ±500KB when using the MB suffix.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="proc_resizing-a-partition-with-parted_assembly_resizing-a-partition">
<title>Resizing a partition with parted</title>
<simpara>This procedure resizes a disk partition using the <literal>parted</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you want to shrink a partition, back up the data that are stored on it.</simpara>
<warning>
<simpara>Shrinking a partition might result in data loss on the partition.</simpara>
</warning>
</listitem>
<listitem>
<simpara>If you want to resize a partition to be larger than 2TiB, the disk must be formatted with the GUID Partition Table (GPT). For details on how to format the disk, see <xref linkend="assembly_creating-a-partition-table-on-a-disk_assembly_getting-started-with-partitions"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If you want to shrink the partition, shrink the file system on it first so that it is not larger than the resized partition. Note that XFS does not support shrinking.</simpara>
</listitem>
<listitem>
<simpara>Start the interactive <literal>parted</literal> shell:</simpara>
<screen># parted <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the device where you want to resize a partition: for example, <literal role="filename">/dev/sda</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>View the current partition table:</simpara>
<screen>(parted) print</screen>
<simpara>From the partition table, determine:</simpara>
<itemizedlist>
<listitem>
<simpara>The minor number of the partition</simpara>
</listitem>
<listitem>
<simpara>The location of the existing partition and its new ending point after resizing</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Resize the partition:</simpara>
<screen>(parted) resizepart <emphasis><phrase role="replaceable">minor-number</phrase></emphasis> <emphasis><phrase role="replaceable">new-end</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">minor-number</phrase></emphasis> with the minor number of the partition that you are resizing: for example, <literal>3</literal>.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">new-end</phrase></emphasis> with the size that determines the new ending point of the resized partition, counting from the beginning of the disk. You can use size suffixes, such as <literal>512MiB</literal>, <literal>20GiB</literal>, or <literal>1.5TiB</literal>. The default size megabytes.</simpara>
</listitem>
</itemizedlist>
<example>
<title>Extending a partition</title>
<simpara>For example, to extend a partition located at the beginning of the disk to be 2GiB in size, use:</simpara>
<screen>(parted) resizepart 1 2GiB</screen>
</example>
<simpara>The changes start taking place as soon as you enter this command, so review it before executing it.</simpara>
</listitem>
<listitem>
<simpara>View the partition table to confirm that the resized partition is in the partition table with the correct size:</simpara>
<screen>(parted) print</screen>
</listitem>
<listitem>
<simpara>Exit the <literal>parted</literal> shell:</simpara>
<screen>(parted) quit</screen>
</listitem>
<listitem>
<simpara>Verify that the kernel recognizes the new partition:</simpara>
<screen># cat /proc/partitions</screen>
</listitem>
<listitem>
<simpara>If you extended the partition, extend the file system on it as well. See (reference) for details.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>parted(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="strategies-for-repartitioning-a-disk_assembly_getting-started-with-partitions">
<title>Strategies for repartitioning a disk</title>
<simpara>There are several different ways to repartition a disk. This section discusses the following possible approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>Unpartitioned free space is available</simpara>
</listitem>
<listitem>
<simpara>An unused partition is available</simpara>
</listitem>
<listitem>
<simpara>Free space in an actively used partition is available</simpara>
</listitem>
</itemizedlist>
<simpara>Note that this section discusses the previously mentioned concepts only theoretically and it does not include any procedural steps on how to perform disk repartitioning step-by-step.</simpara>
<note>
<simpara>The following illustrations are simplified in the interest of clarity and do not reflect the exact partition layout that you encounter when actually installing
Red Hat Enterprise Linux.</simpara>
</note>
<section xml:id="using-unpartitioned-free-space_strategies-for-repartitioning-a-disk">
<title>Using unpartitioned free space</title>
<simpara>In this situation, the partitions that are already defined do not span the entire hard disk, leaving unallocated space that is not part of any defined partition.
The following diagram shows what this might look like:</simpara>
<figure>
<title>Disk with unpartitioned free space</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/unpart-space.svg"/>
</imageobject>
<textobject><phrase>unpart space</phrase></textobject>
</mediaobject>
</figure>
<simpara>In the previous example, the first diagram represents a disk with one primary partition and an undefined partition with unallocated space,
and the second diagram represents a disk with two defined partitions with allocated space.</simpara>
<simpara>An unused hard disk also falls into this category. The only difference is that <emphasis>all</emphasis> the space is not part of any defined partition.</simpara>
<simpara>In any case, you can create the necessary partitions from the unused space. This scenario is mostly likely for a new disk. Most preinstalled operating
systems are configured to take up all available space on a disk drive.</simpara>
</section>
<section xml:id="using-space-from-an-unused-partition_strategies-for-repartitioning-a-disk">
<title>Using space from an unused partition</title>
<simpara>In this case, you can have one or more partitions that you no longer use. The following diagram illustrated such a situation.</simpara>
<figure>
<title>Disk with an unused partition</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/unused-partition.svg"/>
</imageobject>
<textobject><phrase>unused partition</phrase></textobject>
</mediaobject>
</figure>
<simpara>In the previous example, the first diagram represents a disk with an unused partition, and the second diagram represents reallocating an unused partition for Linux.</simpara>
<simpara>In this situation, you can use the space allocated to the unused partition. You must delete the partition and then create the appropriate Linux partition(s) in its place.
You can delete the unused partition and manually create new partitions during the installation process.</simpara>
</section>
<section xml:id="using-free-space-from-an-active-partition_strategies-for-repartitioning-a-disk">
<title>Using free space from an active partition</title>
<simpara>This is the most common situation. It is also the hardest to handle, because even if you have enough free space, it is presently allocated to a partition
that is already in use. If you purchased a computer with preinstalled software, the hard disk most likely has one massive partition holding the operating system and data.</simpara>
<simpara>Aside from adding a new hard drive to your system, you can choose from destructive and non-destructive repartitioning.</simpara>
<section xml:id="destructive-repartitioning_using-free-space-from-an-active-partition">
<title>Destructive repartitioning</title>
<simpara>This deletes the partition and creates several smaller ones instead. You must make a complete backup because any data in the original partition is destroyed.
Create two backups, use verification (if available in your backup software), and try to read data from the backup <emphasis>before</emphasis> deleting the partition.</simpara>
<warning>
<simpara>If an operating system was installed on that partition, it must be reinstalled if you want to use that system as well. Be aware that some computers sold with pre-installed operating
systems might not include the installation media to reinstall the original operating system. You should check whether this applies to your system <emphasis>before</emphasis> you destroy your original
partition and its operating system installation.</simpara>
</warning>
<simpara>After creating a smaller partition for your existing operating system, you can reinstall software, restore your data, and start your Red Hat Enterprise Linux installation.</simpara>
<figure>
<title>Destructive repartitioning action on disk</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/dstrct-reprt.svg"/>
</imageobject>
<textobject><phrase>dstrct reprt</phrase></textobject>
</mediaobject>
</figure>
<warning>
<simpara>Any data previously present in the original partition is lost.</simpara>
</warning>
</section>
<section xml:id="non-destructive-repartitioning_using-free-space-from-an-active-partition">
<title>Non-destructive repartitioning</title>
<simpara>With non-destructive repartitioning you execute a program that makes a big partition smaller without losing any of the files stored in that partition. This method is usually
reliable, but can be very time-consuming on large drives.</simpara>
<simpara>The non-destructive repartitioning process is straightforward and consist of three steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Compress and backup existing data</simpara>
</listitem>
<listitem>
<simpara>Resize the existing partition</simpara>
</listitem>
<listitem>
<simpara>Create new partition(s)</simpara>
</listitem>
</orderedlist>
<simpara>Each step is described further in more detail.</simpara>
<section xml:id="compressing-existing-data_non-destructive-repartitioning">
<title>Compressing existing data</title>
<simpara>The first step is to compress the data in your existing partition. The reason for doing this is to rearrange the data to maximize the available free space at the "end" of the partition.</simpara>
<figure>
<title>Compression on disk</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/compression.svg"/>
</imageobject>
<textobject><phrase>compression</phrase></textobject>
</mediaobject>
</figure>
<simpara>In the previous example, the first diagram represents disk before compression, and the second diagram after compression.</simpara>
<simpara>This step is crucial. Without it, the location of the data could prevent the partition from being resized to the desired extent. Note that some data cannot be moved. In this case,
it severely restricts the size of your new partitions, and you might be forced to destructively repartition your disk.</simpara>
</section>
<section xml:id="resizing-the-existing-partition_non-destructive-repartitioning">
<title>Resizing the existing partition</title>
<simpara>The following figure shows the actual resizing process. While the actual result of the resizing operation varies, depending on the software used, in most cases the newly freed
space is used to create an unformatted partition of the same type as the original partition.</simpara>
<figure>
<title>Partition resizing on disk</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/part-resize.svg"/>
</imageobject>
<textobject><phrase>part resize</phrase></textobject>
</mediaobject>
</figure>
<simpara>In the previous example, the first diagram represents partition before resizing, and the second diagram after resizing.</simpara>
<simpara>It is important to understand what the resizing software you use does with the newly freed space,so that you can take the appropriate steps. In the case illustrated here,
it would be best to delete the new DOS partition and create the appropriate Linux partition or partitions.</simpara>
</section>
<section xml:id="creating-new-partitions_non-destructive-repartitioning">
<title>Creating new partitions</title>
<simpara>As mentioned in the example <link linkend="resizing-the-existing-partition_non-destructive-repartitioning">Resizing the existing partition</link>, it might or might not be necessary to create new partitions. However, unless your resizing software supports systems with Linux installed, it is likely that you must delete
the partition that was created during the resizing process.</simpara>
<figure>
<title>Disk with final partition configuration</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/nondestruct-fin.svg"/>
</imageobject>
<textobject><phrase>nondestruct fin</phrase></textobject>
</mediaobject>
</figure>
<simpara>In the previous example, the first diagram represents disk before configuration, and the second diagram after configuration.</simpara>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="assembly_getting-started-with-xfs-managing-file-systems">
<title>Getting started with XFS</title>
<simpara>This is an overview of how to create and maintain XFS file systems.</simpara>
<section xml:id="the-xfs-file-system_getting-started-with-xfs">
<title>The XFS file system</title>
<simpara>XFS is a highly scalable, high-performance, robust, and mature 64-bit journaling file system that supports very large files and file systems on a single host. It is the default file system in Red Hat Enterprise Linux 8. XFS was originally developed in the early 1990s by SGI and has a long history of running on extremely large servers and storage arrays.</simpara>
<simpara>The features of XFS include:</simpara>
<variablelist>
<varlistentry>
<term>Reliability</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Metadata journaling, which ensures file system integrity after a system crash by keeping a record of file system operations that can be replayed when the system is restarted and the file system remounted</simpara>
</listitem>
<listitem>
<simpara>Extensive run-time metadata consistency checking</simpara>
</listitem>
<listitem>
<simpara>Scalable and fast repair utilities</simpara>
</listitem>
<listitem>
<simpara>Quota journaling. This avoids the need for lengthy quota consistency checks after a crash.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Scalability and performance</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Supported file system size up to 1024 TiB</simpara>
</listitem>
<listitem>
<simpara>Ability to support a large number of concurrent operations</simpara>
</listitem>
<listitem>
<simpara>B-tree indexing for scalability of free space management</simpara>
</listitem>
<listitem>
<simpara>Sophisticated metadata read-ahead algorithms</simpara>
</listitem>
<listitem>
<simpara>Optimizations for streaming video workloads</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Allocation schemes</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Extent-based allocation</simpara>
</listitem>
<listitem>
<simpara>Stripe-aware allocation policies</simpara>
</listitem>
<listitem>
<simpara>Delayed allocation</simpara>
</listitem>
<listitem>
<simpara>Space pre-allocation</simpara>
</listitem>
<listitem>
<simpara>Dynamically allocated inodes</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Other features</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Reflink-based file copies (new in Red Hat Enterprise Linux 8)</simpara>
</listitem>
<listitem>
<simpara>Tightly integrated backup and restore utilities</simpara>
</listitem>
<listitem>
<simpara>Online defragmentation</simpara>
</listitem>
<listitem>
<simpara>Online file system growing</simpara>
</listitem>
<listitem>
<simpara>Comprehensive diagnostics capabilities</simpara>
</listitem>
<listitem>
<simpara>Extended attributes (<literal>xattr</literal>). This allows the system to associate several additional name/value pairs per file.</simpara>
</listitem>
<listitem>
<simpara>Project or directory quotas. This allows quota restrictions over a directory tree.</simpara>
</listitem>
<listitem>
<simpara>Subsecond timestamps</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<bridgehead xml:id="performance_characteristics_3" renderas="sect3" remap="_performance_characteristics_3">Performance characteristics</bridgehead>
<simpara>XFS has a high performance on large systems with enterprise workloads. A large system is one with a relatively high number of CPUs, multiple HBAs, and connections to external disk arrays. XFS also performs well on smaller systems that have a multi-threaded, parallel I/O workload.</simpara>
<simpara>XFS has a relatively low performance for single threaded, metadata-intensive workloads: for example, a workload that creates or deletes large numbers of small files in a single thread.</simpara>
</section>
<section xml:id="assembly_creating-an-xfs-file-system-getting-started-with-xfs">
<title>Creating an XFS file system</title>
<simpara>As a system administrator, you can create an XFS file system on a block device to enable it to store files and directories.</simpara>
<section xml:id="proc_creating-an-xfs-file-system-with-mkfs-xfs-creating-an-xfs-file-system">
<title>Creating an XFS file system with mkfs.xfs</title>
<simpara>This procedure describes how to create an XFS file system on a block device.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create the file system:</simpara>
<itemizedlist>
<listitem>
<simpara>If the device is a regular partition, an LVM volume, an MD volume, a disk, or a similar device, use the following command:</simpara>
<screen># mkfs.xfs <emphasis>block-device</emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">block-device</phrase></emphasis> with the path to the block device. For example, <literal role="filename">/dev/sdb1</literal>, <literal role="filename">/dev/disk/by-uuid/05e99ec8-def1-4a5e-8a9d-5945339ceb2a</literal>, or <literal role="filename">/dev/my-volgroup/my-lv</literal>.</simpara>
</listitem>
<listitem>
<simpara>In general, the default options are optimal for common use.</simpara>
</listitem>
<listitem>
<simpara>When using <literal>mkfs.xfs</literal> on a block device containing an existing file system, add the <literal role="option">-f</literal> option to overwrite that file system.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To create the file system on a hardware RAID device, check if the system correctly detects the stripe geometry of the device:</simpara>
<itemizedlist>
<listitem>
<simpara>If the stripe geometry information is correct, no additional options are needed. Create the file system:</simpara>
<screen># mkfs.xfs <emphasis>block-device</emphasis></screen>
</listitem>
<listitem>
<simpara>If the information is incorrect, specify stripe geometry manually with the <literal role="option">su</literal> and <literal role="option">sw</literal> parameters of the <literal role="option">-d</literal> option. The <literal role="option">su</literal> parameter specifies the RAID chunk size, and the <literal role="option">sw</literal> parameter specifies the number of data disks in the RAID device.</simpara>
<simpara>For example:</simpara>
<screen># mkfs.xfs -d su=<emphasis>64k</emphasis>,sw=<emphasis>4</emphasis> <emphasis>/dev/sda3</emphasis></screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Use the following command to wait for the system to register the new device node:</simpara>
<screen># udevadm settle</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mkfs.xfs(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-an-xfs-file-system-using-rhel-system-roles_creating-an-xfs-file-system">
<title>Creating an XFS file system on a block device using RHEL System Roles</title>
<simpara>This section describes how to create an XFS file system on a block device on multiple target machines using the <literal>storage</literal> role.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Ansible playbook that uses the <literal>storage</literal> role exists.</simpara>
<simpara>For information on how to apply such a playbook, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#applying-a-role_con_intro-to-rhel-system-roles">Applying a role</link>.</simpara>
</listitem>
</itemizedlist>
<section xml:id="an-example-ansible-playbook-to-create-an-xfs-file-system_creating-an-xfs-file-system-using-rhel-system-roles">
<title>Example Ansible playbook to create an XFS file system on a block device</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create an XFS file system on a block device using the default parameters.</simpara>
<warning>
<simpara>The <literal>storage</literal> role can create a file system only on an unpartitioned, whole disk or a logical volume (LV). It cannot create the file system on a partition.</simpara>
</warning>
<example>
<title>A playbook that creates XFS on /dev/sdb</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: xfs
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The volume name (<literal><emphasis>barefs</emphasis></literal> in the example) is currently arbitrary. The <literal>storage</literal> role identifies the volume by the disk device listed under the <literal>disks:</literal> attribute.</simpara>
</listitem>
<listitem>
<simpara>You can omit the <literal>fs_type: xfs</literal> line because XFS is the default file system in RHEL 8.</simpara>
</listitem>
<listitem>
<simpara>To create the file system on an LV, provide the LVM setup under the <literal>disks:</literal> attribute, including the enclosing volume group. For details, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configuring-lvm-volumes-configuring-and-managing-logical-volumes#an-example-playbook-to-manage-logical-volumes_managing-lvm-logical-volumes-using-rhel-system-roles">Example Ansible playbook to manage logical volumes</link>.</simpara>
<simpara>Do not provide the path to the LV device.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional_resources" remap="_additional_resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>For more information about the <literal>storage</literal> role, see <xref linkend="storage-role-intro_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="assembly_backing-up-an-xfs-file-system-getting-started-with-xfs">
<title>Backing up an XFS file system</title>
<simpara>As a system administrator, you can use the <literal>xfsdump</literal> to back up an XFS file system into a file or on a tape. This provides a simple backup mechanism.</simpara>
<section xml:id="con_features-of-xfs-backup-backing-up-an-xfs-file-system">
<title>Features of XFS backup</title>
<simpara>This section describes key concepts and features of backing up an XFS file system with the <literal>xfsdump</literal> utility.</simpara>
<simpara>You can use the <literal>xfsdump</literal> utility to:</simpara>
<itemizedlist>
<listitem>
<simpara>Perform backups to regular file images.</simpara>
<simpara>Only one backup can be written to a regular file.</simpara>
</listitem>
<listitem>
<simpara>Perform backups to tape drives.</simpara>
<simpara>The <literal>xfsdump</literal> utility also enables you to write multiple backups to the same tape. A backup can span multiple tapes.</simpara>
<simpara>To back up multiple file systems to a single tape device, simply write the backup to a tape that already contains an XFS backup. This appends the new backup to the previous one. By default, <literal>xfsdump</literal> never overwrites existing backups.</simpara>
</listitem>
<listitem>
<simpara>Create incremental backups.</simpara>
<simpara>The <literal>xfsdump</literal> utility uses dump levels to determine a base backup to which other backups are relative. Numbers from 0 to 9 refer to increasing dump levels. An incremental backup only backs up files that have changed since the last dump of a lower level:</simpara>
<itemizedlist>
<listitem>
<simpara>To perform a full backup, perform a level 0 dump on the file system.</simpara>
</listitem>
<listitem>
<simpara>A level 1 dump is the first incremental backup after a full backup. The next incremental backup would be level 2, which only backs up files that have changed since the last level 1 dump; and so on, to a maximum of level 9.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Exclude files from a backup using size, subtree, or inode flags to filter them.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfsdump(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_backing-up-an-xfs-file-system-with-xfsdump-backing-up-an-xfs-file-system">
<title>Backing up an XFS file system with xfsdump</title>
<simpara>This procedure describes how to back up the content of an XFS file system into a file or a tape.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An XFS file system that you can back up.</simpara>
</listitem>
<listitem>
<simpara>Another file system or a tape drive where you can store the backup.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Use the following command to back up an XFS file system:</simpara>
<screen># xfsdump -l <emphasis>level</emphasis> [-L <emphasis>label</emphasis>] \
          -f <emphasis>backup-destination</emphasis> <emphasis>path-to-xfs-filesystem</emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis>level</emphasis> with the dump level of your backup. Use <literal>0</literal> to perform a full backup or <literal>1</literal> to <literal>9</literal> to perform consequent incremental backups.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis>backup-destination</emphasis> with the path where you want to store your backup. The destination can be a regular file, a tape drive, or a remote tape device. For example, <literal role="filename">/backup-files/Data.xfsdump</literal> for a file or <literal role="filename">/dev/st0</literal> for a tape drive.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis>path-to-xfs-filesystem</emphasis> with the mount point of the XFS file system you want to back up. For example, <literal role="filename">/mnt/data/</literal>. The file system must be mounted.</simpara>
</listitem>
<listitem>
<simpara>When backing up multiple file systems and saving them on a single tape device, add a session label to each backup using the <literal>-L <emphasis>label</emphasis></literal> option so that it is easier to identify them when restoring. Replace <emphasis>label</emphasis> with any name for your backup: for example, <literal>backup_data</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<example>
<title>Backing up multiple XFS file systems</title>
<itemizedlist>
<listitem>
<simpara>To back up the content of XFS file systems mounted on the <literal role="filename">/boot/</literal> and <literal role="filename">/data/</literal> directories and save them as files in the <literal role="filename">/backup-files/</literal> directory:</simpara>
<screen># xfsdump -l 0 -f <emphasis>/backup-files/boot.xfsdump</emphasis> <emphasis>/boot</emphasis>
# xfsdump -l 0 -f <emphasis>/backup-files/data.xfsdump</emphasis> <emphasis>/data</emphasis></screen>
</listitem>
<listitem>
<simpara>To back up multiple file systems on a single tape device, add a session label to each backup using the <literal>-L <emphasis>label</emphasis></literal> option:</simpara>
<screen># xfsdump -l 0 -L <emphasis>"backup_boot"</emphasis> -f <emphasis>/dev/st0</emphasis> <emphasis>/boot</emphasis>
# xfsdump -l 0 -L <emphasis>"backup_data"</emphasis> -f <emphasis>/dev/st0</emphasis> <emphasis>/data</emphasis></screen>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfsdump(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources-backing-up-an-xfs-file-system">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>The <literal>xfsdump(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assembly_recovering-an-xfs-file-system-from-backup-getting-started-with-xfs">
<title>Restoring an XFS file system from backup</title>
<simpara>As a system administrator, you can use the <literal>xfsrestore</literal> utility to restore XFS backup created with the <literal>xfsdump</literal> utility and stored in a file or on a tape.</simpara>
<section xml:id="con_features-of-restoring-xfs-from-backup-restoring-an-xfs-file-system-from-backup">
<title>Features of restoring XFS from backup</title>
<simpara>This section describes key concepts and features of restoring an XFS file system from backup with the <literal>xfsrestore</literal> utility.</simpara>
<simpara>The <literal>xfsrestore</literal> utility restores file systems from backups produced by <literal>xfsdump</literal>. The <literal>xfsrestore</literal> utility has two modes:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">simple</emphasis> mode enables users to restore an entire file system from a level 0 dump. This is the default mode.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">cumulative</emphasis> mode enables file system restoration from an incremental backup: that is, level 1 to level 9.</simpara>
</listitem>
</itemizedlist>
<simpara>A unique <emphasis>session ID</emphasis> or <emphasis>session label</emphasis> identifies each backup. Restoring a backup from a tape containing multiple backups requires its corresponding session ID or label.</simpara>
<simpara>To extract, add, or delete specific files from a backup, enter the <literal>xfsrestore</literal> interactive mode. The interactive mode provides a set of commands to manipulate the backup files.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfsrestore(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_restoring-an-xfs-file-system-from-backup-with-xfsrestore-restoring-an-xfs-file-system-from-backup">
<title>Restoring an XFS file system from backup with xfsrestore</title>
<simpara>This procedure describes how to restore the content of an XFS file system from a file or tape backup.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A file or tape backup of XFS file systems, as described in <xref linkend="assembly_backing-up-an-xfs-file-system-getting-started-with-xfs"/>.</simpara>
</listitem>
<listitem>
<simpara>A storage device where you can restore the backup.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>The command to restore the backup varies depending on whether you are restoring from a full backup or an incremental one, or are restoring multiple backups from a single tape device:</simpara>
<screen># xfsrestore [-r] [-S session-id] [-L session-label] [-i]
             -f backup-location restoration-path</screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">backup-location</phrase></emphasis> with the location of the backup. This can be a regular file, a tape drive, or a remote tape device. For example, <literal role="filename">/backup-files/Data.xfsdump</literal> for a file or <literal role="filename">/dev/st0</literal> for a tape drive.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">restoration-path</phrase></emphasis> with the path to the directory where you want to restore the file system. For example, <literal role="filename">/mnt/data/</literal>.</simpara>
</listitem>
<listitem>
<simpara>To restore a file system from an incremental (level 1 to level 9) backup, add the <literal role="option">-r</literal> option.</simpara>
</listitem>
<listitem>
<simpara>To restore a backup from a tape device that contains multiple backups, specify the backup using the <literal role="option">-S</literal> or <literal role="option">-L</literal> options.</simpara>
<simpara>The <literal role="option">-S</literal> option lets you choose a backup by its session ID, while the <literal role="option">-L</literal> option lets you choose by the session label. To obtain the session ID and session labels, use the <literal role="command">xfsrestore -I</literal> command.</simpara>
<simpara>Replace <emphasis><phrase role="replaceable">session-id</phrase></emphasis> with the session ID of the backup. For example, <literal>b74a3586-e52e-4a4a-8775-c3334fa8ea2c</literal>. Replace <emphasis><phrase role="replaceable">session-label</phrase></emphasis> with the session label of the backup. For example, <literal>my_backup_session_label</literal>.</simpara>
</listitem>
<listitem>
<simpara>To use <literal>xfsrestore</literal> interactively, use the <literal role="option">-i</literal> option.</simpara>
<simpara>The interactive dialog begins after <literal>xfsrestore</literal> finishes reading the specified device. Available commands in the interactive <literal>xfsrestore</literal> shell include <literal>cd</literal>, <literal>ls</literal>, <literal>add</literal>, <literal>delete</literal>, and <literal>extract</literal>; for a complete list of commands, use the <literal>help</literal> command.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<example>
<title>Restoring Multiple XFS File Systems</title>
<itemizedlist>
<listitem>
<simpara>To restore the XFS backup files and save their content into directories under <literal role="filename">/mnt/</literal>:</simpara>
<screen># xfsrestore -f <emphasis>/backup-files/boot.xfsdump</emphasis> <emphasis>/mnt/boot/</emphasis>
# xfsrestore -f <emphasis>/backup-files/data.xfsdump</emphasis> <emphasis>/mnt/data/</emphasis></screen>
</listitem>
<listitem>
<simpara>To restore from a tape device containing multiple backups, specify each backup by its session label or session ID:</simpara>
<screen># xfsrestore -L <emphasis>"backup_boot"</emphasis> -f <emphasis>/dev/st0</emphasis> <emphasis>/mnt/boot/</emphasis>
# xfsrestore -S <emphasis>"45e9af35-efd2-4244-87bc-4762e476cbab"</emphasis> \
             -f <emphasis>/dev/st0</emphasis> <emphasis>/mnt/data/</emphasis></screen>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfsrestore(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="con_informational-messages-when-restoring-an-xfs-backup-from-a-tape-restoring-an-xfs-file-system-from-backup">
<title>Informational messages when restoring an XFS backup from a tape</title>
<simpara>When restoring a backup from a tape with backups from multiple file systems, the <literal>xfsrestore</literal> utility might issue messages. The messages inform you whether a match of the requested backup has been found when <literal>xfsrestore</literal> examines each backup on the tape in sequential order. For example:</simpara>
<screen>xfsrestore: preparing drive
xfsrestore: examining media file 0
xfsrestore: inventory session uuid (8590224e-3c93-469c-a311-fc8f23029b2a) does not match the media header's session uuid (7eda9f86-f1e9-4dfd-b1d4-c50467912408)
xfsrestore: examining media file 1
xfsrestore: inventory session uuid (8590224e-3c93-469c-a311-fc8f23029b2a) does not match the media header's session uuid (7eda9f86-f1e9-4dfd-b1d4-c50467912408)
[...]</screen>
<simpara>The informational messages keep appearing until the matching backup is found.</simpara>
</section>
<section xml:id="additional-resources-restoring-an-xfs-file-system-from-backup">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>The <literal>xfsrestore(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assembly_increasing-the-size-of-an-xfs-file-system_getting-started-with-xfs">
<title>Increasing the size of an XFS file system</title>
<simpara>As a system administrator, you can increase the size of an XFS file system to utilize larger storage capacity.</simpara>
<important>
<simpara>It is not currently possible to decrease the size of XFS file systems.</simpara>
</important>
<section xml:id="proc_increasing-the-size-of-an-xfs-file-system-with-xfs_growfs_assembly_increasing-the-size-of-an-xfs-file-system">
<title>Increasing the size of an XFS file system with xfs_growfs</title>
<simpara>This procedure describes how to grow an XFS file system using the <literal>xfs_growfs</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Ensure that the underlying block device is of an appropriate size to hold the resized file system later. Use the appropriate resizing methods for the affected block device.</simpara>
</listitem>
<listitem>
<simpara>Mount the XFS file system.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>While the XFS file system is mounted, use the <literal>xfs_growfs</literal> utility to increase its size:</simpara>
<screen># xfs_growfs <emphasis><phrase role="replaceable">file-system</phrase></emphasis> -D <emphasis><phrase role="replaceable">new-size</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">file-system</phrase></emphasis> with the mount point of the XFS file system.</simpara>
</listitem>
<listitem>
<simpara>With the <literal role="option">-D</literal> option, replace <emphasis><phrase role="replaceable">new-size</phrase></emphasis> with the desired new size of the file system specified in the number of file system blocks.</simpara>
<simpara>To find out the block size in kB of a given XFS file system, use the <literal>xfs_info</literal> utility:</simpara>
<screen># xfs_info <emphasis><phrase role="replaceable">block-device</phrase></emphasis>

...
data     =              bsize=4096
...</screen>
</listitem>
<listitem>
<simpara>Without the <literal role="option">-D</literal> option, <literal>xfs_growfs</literal> grows the file system to the maximum size supported by the underlying device.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfs_growfs(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="comparison-of-tools-used-with-ext4-and-xfs_getting-started-with-xfs">
<title>Comparison of tools used with ext4 and XFS</title>
<simpara>This section compares which tools to use to accomplish common tasks on the ext4 and XFS file systems.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Task</entry>
<entry align="left" valign="top">ext4</entry>
<entry align="left" valign="top">XFS</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Create a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>mkfs.ext4</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>mkfs.xfs</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File system check</simpara></entry>
<entry align="left" valign="top"><simpara><literal>e2fsck</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_repair</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Resize a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>resize2fs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_growfs</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Save an image of a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>e2image</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_metadump</literal> and <literal>xfs_mdrestore</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Label or tune a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>tune2fs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_admin</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Back up a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>dump</literal> and <literal>restore</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfsdump</literal> and <literal>xfsrestore</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Quota management</simpara></entry>
<entry align="left" valign="top"><simpara><literal>quota</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_quota</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File mapping</simpara></entry>
<entry align="left" valign="top"><simpara><literal>filefrag</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_bmap</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</chapter>
<chapter xml:id="configuring-xfs-error-behavior_managing-file-systems">
<title>Configuring XFS error behavior</title>
<simpara>You can configure how an XFS file system behaves when it encounters different I/O errors.</simpara>
<section xml:id="configurable-error-handling-in-xfs_configuring-xfs-error-behavior">
<title>Configurable error handling in XFS</title>
<simpara>The XFS file system responds in one of the following ways when an error occurs during an I/O operation:</simpara>
<itemizedlist>
<listitem>
<simpara>XFS repeatedly retries the I/O operation until the operation succeeds or XFS reaches a set limit.</simpara>
<simpara>The limit is based either on a maximum number of retries or a maximum time for retries.</simpara>
</listitem>
<listitem>
<simpara>XFS considers the error permanent and stops the operation on the file system.</simpara>
</listitem>
</itemizedlist>
<simpara>You can configure how XFS reacts to the following error conditions:</simpara>
<variablelist>
<varlistentry>
<term><literal>EIO</literal></term>
<listitem>
<simpara>Error when reading or writing</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ENOSPC</literal></term>
<listitem>
<simpara>No space left on the device</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ENODEV</literal></term>
<listitem>
<simpara>Device cannot be found</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>You can set the maximum number of retries and the maximum time in seconds until XFS considers an error permanent. XFS stops retrying the operation when it reaches either of the limits.</simpara>
<simpara>You can also configure XFS so that when unmounting a file system, XFS immediately cancels the retries regardless of any other configuration. This configuration enables the unmount operation to succeed despite persistent errors.</simpara>
<formalpara>
<title>Default behavior</title>
<para>The default behavior for each XFS error condition depends on the error context. Some XFS errors such as <literal>ENODEV</literal> are considered to be fatal and unrecoverable, regardless of the retry count. Their default retry limit is 0.</para>
</formalpara>
</section>
<section xml:id="configuration-files-for-specific-and-undefined-xfs-error-conditions_configuring-xfs-error-behavior">
<title>Configuration files for specific and undefined XFS error conditions</title>
<simpara>The following directories store configuration files that control XFS error behavior for different error conditions:</simpara>
<variablelist>
<varlistentry>
<term><literal>/sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/EIO/</literal></term>
<listitem>
<simpara>For the <literal>EIO</literal> error condition</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/ENODEV/</literal></term>
<listitem>
<simpara>For the <literal>ENODEV</literal> error condition</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/ENOSPC/</literal></term>
<listitem>
<simpara>For the <literal>ENOSPC</literal> error condition</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/sys/fs/xfs/<emphasis>device</emphasis>/error/default/</literal></term>
<listitem>
<simpara>Common configuration for all other, undefined error conditions</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Each directory contains the following configuration files for configuring retry limits:</simpara>
<variablelist>
<varlistentry>
<term><literal>max_retries</literal></term>
<listitem>
<simpara>Controls the maximum number of times that XFS retries the operation.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>retry_timeout_seconds</literal></term>
<listitem>
<simpara>Specifies the time limit in seconds after which XFS stops retrying the operation.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="setting-specific-and-undefined-xfs-error-conditions_configuring-xfs-error-behavior">
<title>Setting XFS behavior for specific conditions</title>
<simpara>This procedure configures how XFS reacts to specific error conditions.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the maximum number of retries, the retry time limit, or both:</simpara>
<itemizedlist>
<listitem>
<simpara>To set the maximum number of retries, write the desired number to the <literal>max_retries</literal> file:</simpara>
<screen># echo <emphasis>value</emphasis> &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/<emphasis>condition</emphasis>/max_retries</screen>
</listitem>
<listitem>
<simpara>To set the time limit, write the desired number of seconds to the <literal>retry_timeout_seconds</literal> file:</simpara>
<screen># echo <emphasis>value</emphasis> &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/<emphasis>condition</emphasis>/retry_timeout_second</screen>
</listitem>
</itemizedlist>
<simpara><emphasis>value</emphasis> is a number between -1 and the maximum possible value of the C signed integer type. This is 2147483647 on 64-bit Linux.</simpara>
<simpara>In both limits, the value <literal>-1</literal> is used for continuous retries and <literal>0</literal> to stop immediately.</simpara>
<simpara><emphasis>device</emphasis> is the name of the device, as found in the <literal>/dev/</literal> directory; for example, <literal>sda</literal>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-undefined-xfs-error-conditions_configuring-xfs-error-behavior">
<title>Setting XFS behavior for undefined conditions</title>
<simpara>This procedure configures how XFS reacts to all undefined error conditions, which share a common configuration.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the maximum number of retries, the retry time limit, or both:</simpara>
<itemizedlist>
<listitem>
<simpara>To set the maximum number of retries, write the desired number to the <literal>max_retries</literal> file:</simpara>
<screen># echo <emphasis>value</emphasis> &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/default/max_retries</screen>
</listitem>
<listitem>
<simpara>To set the time limit, write the desired number of seconds to the <literal>retry_timeout_seconds</literal> file:</simpara>
<screen># echo <emphasis>value</emphasis> &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/metadata/default/retry_timeout_seconds</screen>
</listitem>
</itemizedlist>
<simpara><emphasis>value</emphasis> is a number between -1 and the maximum possible value of the C signed integer type. This is 2147483647 on 64-bit Linux.</simpara>
<simpara>In both limits, the value <literal>-1</literal> is used for continuous retries and <literal>0</literal> to stop immediately.</simpara>
<simpara><emphasis>device</emphasis> is the name of the device, as found in the <literal>/dev/</literal> directory; for example, <literal>sda</literal>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-the-unmount-behavior_configuring-xfs-error-behavior">
<title>Setting the XFS unmount behavior</title>
<simpara>This procedure configures how XFS reacts to error conditions when unmounting the file system.</simpara>
<simpara>If you set the <literal>fail_at_unmount</literal> option in the file system, it overrides all other error configurations during unmount, and immediately unmounts the file system without retrying the I/O operation. This allows the unmount operation to succeed even in case of persistent errors.</simpara>
<warning>
<simpara>You cannot change the <literal>fail_at_unmount</literal> value after the unmount process starts, because the unmount process removes the configuration files from the <literal>sysfs</literal> interface for the respective file system. You must configure the unmount behavior before the file system starts unmounting.</simpara>
</warning>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enable or disable the <literal>fail_at_unmount</literal> option:</simpara>
<itemizedlist>
<listitem>
<simpara>To cancel retrying all operations when the file system unmounts, enable the option:</simpara>
<screen># echo 1 &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/fail_at_unmount</screen>
</listitem>
<listitem>
<simpara>To respect the <literal>max_retries</literal> and <literal>retry_timeout_seconds</literal> retry limits when the file system unmounts, disable the option:</simpara>
<screen># echo 0 &gt; /sys/fs/xfs/<emphasis>device</emphasis>/error/fail_at_unmount</screen>
</listitem>
</itemizedlist>
<simpara><emphasis>device</emphasis> is the name of the device, as found in the <literal>/dev/</literal> directory; for example, <literal>sda</literal>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="checking-and-repairing-a-file-system_managing-file-systems">
<title>Checking and repairing a file system</title>
<simpara>RHEL provides file system administration utilities which are capable of checking and repairing file systems. These tools are often referred to as <literal>fsck</literal> tools, where <literal>fsck</literal> is a shortened version of <emphasis>file system check</emphasis>. In most cases, these utilities are run automatically during system boot, if needed, but can also be manually invoked if required.</simpara>
<important>
<simpara>File system checkers guarantee only metadata consistency across the file system. They have no awareness of the actual data contained within the file system and are not data recovery tools.</simpara>
</important>
<section xml:id="file-system-checking-and-repair_checking-and-repairing-a-file-system">
<title>Scenarios that require a file system check</title>
<simpara>The relevant <literal>fsck</literal> tools can be used to check your system if any of the following occurs:</simpara>
<itemizedlist>
<listitem>
<simpara>System fails to boot</simpara>
</listitem>
<listitem>
<simpara>Files on a specific disk become corrupt</simpara>
</listitem>
<listitem>
<simpara>The file system shuts down or changes to read-only due to inconsistencies</simpara>
</listitem>
<listitem>
<simpara>A file on the file system is inaccessible</simpara>
</listitem>
</itemizedlist>
<simpara>File system inconsistencies can occur for various reasons, including but not limited to hardware errors, storage administration errors, and software bugs.</simpara>
<important>
<simpara>File system check tools cannot repair hardware problems. A file system must be fully readable and writable if repair is to operate successfully. If a file system was corrupted due to a hardware error, the file system must first be moved to a good disk, for example with the <literal>dd(8)</literal> utility.</simpara>
</important>
<simpara>For journaling file systems, all that is normally required at boot time is to replay the journal if required and this is usually a very short operation.</simpara>
<simpara>However, if a file system inconsistency or corruption occurs, even for journaling file systems, then the file system checker must be used to repair the file system.</simpara>
<important>
<simpara>It is possible to disable file system check at boot by setting the sixth field in <literal>/etc/fstab</literal> to <literal>0</literal>.  However, Red Hat does not recommend doing so unless you are having issues with <literal>fsck</literal> at boot time, for example with extremely large or remote file systems.</simpara>
</important>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>fstab(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>fsck(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>dd(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="potential-side-effects-of-running-fsck_checking-and-repairing-a-file-system">
<title>Potential side effects of running fsck</title>
<simpara>Generally, running the file system check and repair tool can be expected to automatically repair at least some of the inconsistencies it finds. In some cases, the following issues can arise:</simpara>
<itemizedlist>
<listitem>
<simpara>Severely damaged inodes or directories may be discarded if they cannot be repaired.</simpara>
</listitem>
<listitem>
<simpara>Significant changes to the file system may occur.</simpara>
</listitem>
</itemizedlist>
<simpara>To ensure that unexpected or undesirable changes are not permanently made, ensure you follow any precautionary steps outlined in the procedure.</simpara>
</section>
<section xml:id="error-handling-mechanisms-in-xfs_checking-and-repairing-a-file-system">
<title>Error-handling mechanisms in XFS</title>
<simpara>This section describes how XFS handles various kinds of errors in the file system.</simpara>
<bridgehead xml:id="unclean_unmounts" renderas="sect3" remap="_unclean_unmounts">Unclean unmounts</bridgehead>
<simpara>Journalling maintains a transactional record of metadata changes that happen on the file system.</simpara>
<simpara>In the event of a system crash, power failure, or other unclean unmount, XFS uses the journal (also called log) to recover the file system. The kernel performs journal recovery when mounting the XFS file system.</simpara>
<bridgehead xml:id="corruption" renderas="sect3" remap="_corruption">Corruption</bridgehead>
<simpara>In this context, <emphasis>corruption</emphasis> means errors on the file system caused by, for example:</simpara>
<itemizedlist>
<listitem>
<simpara>Hardware faults</simpara>
</listitem>
<listitem>
<simpara>Bugs in storage firmware, device drivers, the software stack, or the file system itself</simpara>
</listitem>
<listitem>
<simpara>Problems that cause parts of the file system to be overwritten by something outside of the file system</simpara>
</listitem>
</itemizedlist>
<simpara>When XFS detects corruption in the file system or the file-system metadata, it may shut down the file system and report the incident in the system log. Note that if the corruption occurred on the file system hosting the <literal role="filename">/var</literal> directory, these logs will not be available after a reboot.</simpara>
<example>
<title>System log entry reporting an XFS corruption</title>
<screen># dmesg --notime | tail -15

XFS (loop0): Mounting V5 Filesystem
XFS (loop0): Metadata CRC error detected at xfs_agi_read_verify+0xcb/0xf0 [xfs], xfs_agi block 0x2
XFS (loop0): Unmount and run xfs_repair
XFS (loop0): First 128 bytes of corrupted metadata buffer:
00000000027b3b56: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000000005f9abc7a: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000000005b0aef35: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
00000000da9d2ded: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000000001e265b07: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000000006a40df69: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000000000b272907: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
00000000e484aac5: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
XFS (loop0): metadata I/O error in "xfs_trans_read_buf_map" at daddr 0x2 len 1 error 74
XFS (loop0): xfs_imap_lookup: xfs_ialloc_read_agi() returned error -117, agno 0
XFS (loop0): Failed to read root inode 0x80, error 11</screen>
</example>
<simpara>User-space utilities usually report the <emphasis>Input/output error</emphasis> message when trying to access a corrupted XFS file system. Mounting an XFS file system with a corrupted log results in a failed mount and the following error message:</simpara>
<screen>mount: <emphasis><phrase role="replaceable">/mount-point</phrase></emphasis>: mount(2) system call failed: Structure needs cleaning.</screen>
<simpara>You must manually use the <literal>xfs_repair</literal> utility to repair the corruption.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfs_repair(8)</literal> man page provides a detailed list of XFS corruption checks.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="checking-an-xfs-file-system-with-xfs-repair_checking-and-repairing-a-file-system">
<title>Checking an XFS file system with <literal>xfs_repair</literal></title>
<simpara>This procedure performs a read-only check of an XFS file system using the <literal>xfs_repair</literal> utility. You must manually use the <literal>xfs_repair</literal> utility to repair any corruption. Unlike other file system repair utilities, <literal>xfs_repair</literal> does not run at boot time, even when an XFS file system was not cleanly unmounted. In the event of an unclean unmount, XFS simply replays the log at mount time, ensuring a consistent file system; <literal>xfs_repair</literal> cannot repair an XFS file system with a dirty log without remounting it first.</simpara>
<note>
<simpara>Although an <literal>fsck.xfs</literal> binary is present in the <literal>xfsprogs</literal> package, this is present only to satisfy <literal>initscripts</literal> that look for an <literal>fsck.file</literal> system binary at boot time. <literal>fsck.xfs</literal> immediately exits with an exit code of 0.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Replay the log by mounting and unmounting the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis>
# umount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
<note>
<simpara>If the mount fails with a structure needs cleaning error, the log is corrupted and cannot be replayed. The dry run should discover and report more on-disk corruption as a result.</simpara>
</note>
</listitem>
<listitem>
<simpara>Use the <literal>xfs_repair</literal> utility to perform a dry run to check the file system. Any errors are printed and an indication of the actions that would be taken, without modifying the file system.</simpara>
<screen># xfs_repair -n <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mount the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfs_repair(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>xfs_metadump(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="proc_repairing-an-xfs-file-system-with-xfs_repair_checking-and-repairing-a-file-system">
<title>Repairing an XFS file system with xfs_repair</title>
<simpara>This procedure repairs a corrupted XFS file system using the <literal>xfs_repair</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a metadata image prior to repair for diagnostic or testing purposes using the <literal>xfs_metadump</literal> utility. A pre-repair file system metadata image can be useful for support investigations if the corruption is due to a software bug. Patterns of corruption present in the pre-repair image can aid in root-cause analysis.</simpara>
<itemizedlist>
<listitem>
<simpara>Use the <literal>xfs_metadump</literal> debugging tool to copy the metadata from an XFS file system to a file. The resulting <literal>metadump</literal> file can be compressed using standard compression utilities to reduce the file size if large <literal>metadump</literal> files need to be sent to support.</simpara>
<screen># xfs_metadump <emphasis><phrase role="replaceable">block-device</phrase></emphasis> <emphasis><phrase role="replaceable">metadump-file</phrase></emphasis></screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Replay the log by remounting the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis>
# umount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Use the <literal>xfs_repair</literal> utility to repair the unmounted file system:</simpara>
<itemizedlist>
<listitem>
<simpara>If the mount succeeded, no additional options are required:</simpara>
<screen># xfs_repair <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>If the mount failed with the <emphasis>Structure needs cleaning</emphasis> error, the log is corrupted and cannot be replayed. Use the <literal role="option">-L</literal> option (<emphasis>force log zeroing</emphasis>) to clear the log:</simpara>
<warning>
<simpara>This command causes all metadata updates in progress at the time of the crash to be lost, which might cause significant file system damage and data loss. This should be used only as a last resort if the log cannot be replayed.</simpara>
</warning>
<screen># xfs_repair -L <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Mount the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>xfs_repair(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="error-handling-mechanisms-in-ext2-ext3-and-ext4_checking-and-repairing-a-file-system">
<title>Error handling mechanisms in ext2, ext3, and ext4</title>
<simpara>The ext2, ext3, and ext4 file systems use the <literal>e2fsck</literal> utility to perform file system checks and repairs. The file names <literal>fsck.ext2</literal>, <literal>fsck.ext3</literal>, and <literal>fsck.ext4</literal> are hardlinks to the <literal>e2fsck</literal> utility. These binaries are run automatically at boot time and their behavior differs based on the file system being checked and the state of the file system.</simpara>
<simpara>A full file system check and repair is invoked for ext2, which is not a metadata journaling file system, and for ext4 file systems without a journal.</simpara>
<simpara>For ext3 and ext4 file systems with metadata journaling, the journal is replayed in userspace and the utility exits. This is the default action because journal replay ensures a consistent file system after a crash.</simpara>
<simpara>If these file systems encounter metadata inconsistencies while mounted, they record this fact in the file system superblock. If <literal>e2fsck</literal> finds that a file system is marked with such an error, <literal>e2fsck</literal> performs a full check after replaying the journal (if present).</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>fsck(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>e2fsck(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="checking-an-ext2-ext3-or-ext4-file-system-with-e2fsck_checking-and-repairing-a-file-system">
<title>Checking an ext2, ext3, or ext4 file system with e2fsck</title>
<simpara>This procedure checks an ext2, ext3, or ext4 file system using the <literal>e2fsck</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Replay the log by remounting the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis>
# umount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Perform a dry run to check the file system.</simpara>
<screen># e2fsck -n <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<note>
<simpara>Any errors are printed and an indication of the actions that would be taken, without modifying the file system. Later phases of consistency checking may print extra errors as it discovers inconsistencies which would have been fixed in early phases if it were running in repair mode.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>e2image(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>e2fsck(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="repairing-an-ext2-ext3-or-ext4-file-system-with-e2fsck_checking-and-repairing-a-file-system">
<title>Repairing an ext2, ext3, or ext4 file system with e2fsck</title>
<simpara>This procedure repairs a corrupted ext2, ext3, or ext4 file system using the <literal>e2fsck</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Save a file system image for support investigations. A pre-repair file system metadata image can be useful for support investigations if the corruption is due to a software bug. Patterns of corruption present in the pre-repair image can aid in root-cause analysis.</simpara>
<note>
<simpara>Severely damaged file systems may cause problems with metadata image creation.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>If you are creating the image for testing purposes, use the <literal>-r</literal> option to create a sparse file of the same size as the file system itself. <literal>e2fsck</literal> can then operate directly on the resulting file.</simpara>
<screen># e2image -r <emphasis><phrase role="replaceable">block-device</phrase></emphasis> <emphasis><phrase role="replaceable">image-file</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>If you are creating the image to be archived or provided for diagnostic, use the <literal>-Q</literal> option, which creates a more compact file format suitable for transfer.</simpara>
<screen># e2image -Q <emphasis><phrase role="replaceable">block-device</phrase></emphasis> <emphasis><phrase role="replaceable">image-file</phrase></emphasis></screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Replay the log by remounting the file system:</simpara>
<screen># mount <emphasis><phrase role="replaceable">file-system</phrase></emphasis>
# umount <emphasis><phrase role="replaceable">file-system</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Automatically repair the file system. If user intervention is required, <literal>e2fsck</literal> indicates the unfixed problem in its output and reflects this status in the exit code.</simpara>
<screen># e2fsck -p <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>e2image(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>e2fsck(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="assembly_mounting-file-systems_managing-file-systems">
<title>Mounting file systems</title>
<simpara>As a system administrator, you can mount file systems on your system to access data on them.</simpara>
<section xml:id="the-linux-mount-mechanism_assembly_mounting-file-systems">
<title>The Linux mount mechanism</title>
<simpara>This section explains basic concepts of mounting file systems on Linux.</simpara>
<simpara>On Linux, UNIX, and similar operating systems, file systems on different partitions and removable devices (CDs, DVDs, or USB flash drives for example) can be attached to a certain point (the mount point) in the directory tree, and then detached again. While a file system is mounted on a directory, the original content of the directory is not accessible.</simpara>
<simpara>Note that Linux does not prevent you from mounting a file system to a directory with a file system already attached to it.</simpara>
<simpara>When mounting, you can identify the device by:</simpara>
<itemizedlist>
<listitem>
<simpara>a universally unique identifier (UUID): for example, <literal>UUID=34795a28-ca6d-4fd8-a347-73671d0c19cb</literal></simpara>
</listitem>
<listitem>
<simpara>a volume label: for example, <literal>LABEL=home</literal></simpara>
</listitem>
<listitem>
<simpara>a full path to a non-persistent block device: for example, <literal role="filename">/dev/sda3</literal></simpara>
</listitem>
</itemizedlist>
<simpara>When you mount a file system using the <literal>mount</literal> command without all required information, that is without the device name, the target directory, or the file system type, the <literal>mount</literal> utility reads the content of the <literal role="filename">/etc/fstab</literal> file to check if the given file system is listed there. The <literal role="filename">/etc/fstab</literal> file contains a list of device names and the directories in which the selected file systems are set to be mounted as well as the file system type and mount options. Therefore, when mounting a file system that is specified in <literal role="filename">/etc/fstab</literal>, the following command syntax is sufficient:</simpara>
<itemizedlist>
<listitem>
<simpara>Mounting by the mount point:</simpara>
<screen># mount <emphasis><phrase role="replaceable">directory</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mounting by the block device:</simpara>
<screen># mount <emphasis><phrase role="replaceable">device</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>For information on how to list persistent naming attributes such as the UUID, see <xref linkend="proc_listing-persistent-naming-attributes_assembly_overview-of-persistent-naming-attributes"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="listing-currently-mounted-file-systems_assembly_mounting-file-systems">
<title>Listing currently mounted file systems</title>
<simpara>This procedure describes how to list all currently mounted file systems on the command line.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To list all mounted file systems, use the <literal>findmnt</literal> utility:</simpara>
<screen>$ findmnt</screen>
</listitem>
<listitem>
<simpara>To limit the listed file systems only to a certain file system type, add the <literal role="option">--types</literal> option:</simpara>
<screen>$ findmnt --types <emphasis><phrase role="replaceable">fs-type</phrase></emphasis></screen>
<simpara>For example:</simpara>
<example>
<title>Listing only XFS file systems</title>
<screen>$ findmnt --types xfs

TARGET  SOURCE                                                FSTYPE OPTIONS
/       /dev/mapper/luks-5564ed00-6aac-4406-bfb4-c59bf5de48b5 xfs    rw,relatime
├─/boot /dev/sda1                                             xfs    rw,relatime
└─/home /dev/mapper/luks-9d185660-7537-414d-b727-d92ea036051e xfs    rw,relatime</screen>
</example>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>findmnt(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mounting-a-file-system-with-mount_assembly_mounting-file-systems">
<title>Mounting a file system with mount</title>
<simpara>This procedure describes how to mount a file system using the <literal>mount</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Make sure that no file system is already mounted on your chosen mount point:</simpara>
<screen>$ findmnt <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To attach a certain file system, use the <literal>mount</literal> utility:</simpara>
<screen># mount <emphasis><phrase role="replaceable">device</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
<example>
<title>Mounting an XFS file system</title>
<simpara>For example, to mount a local XFS file system identified by UUID:</simpara>
<screen># mount UUID=ea74bbec-536d-490c-b8d9-5b40bbd7545b /mnt/data</screen>
</example>
</listitem>
<listitem>
<simpara>If <literal>mount</literal> cannot recognize the file system type automatically, specify it using the <literal role="option">--types</literal> option:</simpara>
<screen># mount --types <emphasis><phrase role="replaceable">type</phrase></emphasis> <emphasis><phrase role="replaceable">device</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
<example>
<title>Mounting an NFS file system</title>
<simpara>For example, to mount a remote NFS file system:</simpara>
<screen># mount --types nfs4 host:/remote-export /mnt/nfs</screen>
</example>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="moving-a-mount-point_assembly_mounting-file-systems">
<title>Moving a mount point</title>
<simpara>This procedure describes how to change the mount point of a mounted file system to a different directory.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To change the directory in which a file system is mounted:</simpara>
<screen># mount --move <emphasis><phrase role="replaceable">old-directory</phrase></emphasis> <emphasis><phrase role="replaceable">new-directory</phrase></emphasis></screen>
<example>
<title>Moving a home file system</title>
<simpara>For example, to move the file system mounted in the <literal role="filename">/mnt/userdirs/</literal> directory to the <literal role="filename">/home/</literal> mount point:</simpara>
<screen># mount --move /mnt/userdirs /home</screen>
</example>
</listitem>
<listitem>
<simpara>Verify that the file system has been moved as expected:</simpara>
<screen>$ findmnt
$ ls <emphasis><phrase role="replaceable">old-directory</phrase></emphasis>
$ ls <emphasis><phrase role="replaceable">new-directory</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="unmounting-a-file-system-with-umount_assembly_mounting-file-systems">
<title>Unmounting a file system with umount</title>
<simpara>This procedure describes how to unmount a file system using the <literal>umount</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Try unmounting the file system using either of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>By mount point:</simpara>
<screen># umount <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>By device:</simpara>
<screen># umount <emphasis><phrase role="replaceable">device</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<simpara>If the command fails with an error similar to the following, it means that the file system is in use because of a process is using resources on it:</simpara>
<screen>umount: <emphasis><phrase role="replaceable">/run/media/user/FlashDrive</phrase></emphasis>: target is busy.</screen>
</listitem>
<listitem>
<simpara>If the file system is in use, use the <literal>fuser</literal> utility to determine which processes are accessing it. For example:</simpara>
<screen>$ fuser --mount <emphasis><phrase role="replaceable">/run/media/user/FlashDrive</phrase></emphasis>

<emphasis><phrase role="replaceable">/run/media/user/FlashDrive</phrase></emphasis>: <emphasis><phrase role="replaceable">18351</phrase></emphasis></screen>
<simpara>Afterwards, terminate the processes using the file system and try unmounting it again.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="common-mount-options_assembly_mounting-file-systems">
<title>Common mount options</title>
<simpara>This section lists some commonly used options of the <literal>mount</literal> utility.</simpara>
<simpara>You can use these options in the following syntax:</simpara>
<screen># mount --options <emphasis><phrase role="replaceable">option1,option2,option3</phrase></emphasis> <emphasis><phrase role="replaceable">device</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
<table frame="all" rowsep="1" colsep="1">
<title>Common mount options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>async</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables asynchronous input and output operations on the file system.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>auto</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables the file system to be mounted automatically using the <literal role="command">mount -a</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>defaults</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides an alias for the <literal>async,auto,dev,exec,nouser,rw,suid</literal> options.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>exec</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allows the execution of binary files on the particular file system.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>loop</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Mounts an image as a loop device.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>noauto</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Default behavior disables the automatic mount of the file system using the <literal role="command">mount -a</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>noexec</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disallows the execution of binary files on the particular file system.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>nouser</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disallows an ordinary user (that is, other than root) to mount and unmount the file system.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>remount</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Remounts the file system in case it is already mounted.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ro</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Mounts the file system for reading only.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>rw</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Mounts the file system for both reading and writing.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>user</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allows an ordinary user (that is, other than root) to mount and unmount the file system.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="sharing-a-mount-on-multiple-mount-points_assembly_mounting-file-systems">
<title>Sharing a mount on multiple mount points</title>
<simpara>As a system administrator, you can duplicate mount points to make the file systems accessible from multiple directories.</simpara>
<section xml:id="types-of-shared-mounts_sharing-a-mount-on-multiple-mount-points">
<title>Types of shared mounts</title>
<simpara>There are multiple types of shared mounts that you can use. The difference between them is what happens when you mount another file system under one of the shared mount points. The shared mounts are implemented using the <emphasis>shared subtrees</emphasis> functionality.</simpara>
<simpara>The types are:</simpara>
<variablelist>
<varlistentry>
<term>Private mount</term>
<listitem>
<simpara>This type does not receive or forward any propagation events.</simpara>
<simpara>When you mount another file system under either the duplicate or the original mount point, it is not reflected in the other.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Shared mount</term>
<listitem>
<simpara>This type creates an exact replica of a given mount point.</simpara>
<simpara>When a mount point is marked as a shared mount, any mount within the original mount point is reflected in it, and vice versa.</simpara>
<simpara>This is the default mount type of the root file system.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Slave mount</term>
<listitem>
<simpara>This type creates a limited duplicate of a given mount point.</simpara>
<simpara>When a mount point is marked as a slave mount, any mount within the original mount point is reflected in it, but no mount within a slave mount is reflected in its original.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Unbindable mount</term>
<listitem>
<simpara>This type prevents the given mount point from being duplicated whatsoever.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="creating-a-private-mount-point-duplicate_sharing-a-mount-on-multiple-mount-points">
<title>Creating a private mount point duplicate</title>
<simpara>This procedure duplicates a mount point as a private mount. File systems that you later mount under the duplicate or the original mount point are not reflected in the other.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a virtual file system (VFS) node from the original mount point:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mark the original mount point as private:</simpara>
<screen># mount --make-private <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
<simpara>Alternatively, to change the mount type for the selected mount point and all mount points under it, use the <literal role="option">--make-rprivate</literal> option instead of <literal role="option">--make-private</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create the duplicate:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">duplicate-dir</phrase></emphasis></screen>
</listitem>
</orderedlist>
<example>
<title>Duplicating /media into /mnt as a private mount point</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a VFS node from the <literal role="filename">/media</literal> directory:</simpara>
<screen># mount --bind /media /media</screen>
</listitem>
<listitem>
<simpara>Mark the <literal role="filename">/media</literal> directory as private:</simpara>
<screen># mount --make-private /media</screen>
</listitem>
<listitem>
<simpara>Create its duplicate in <literal role="filename">/mnt</literal>:</simpara>
<screen># mount --bind /media /mnt</screen>
</listitem>
<listitem>
<simpara>It is now possible to verify that <literal role="filename">/media</literal> and <literal role="filename">/mnt</literal> share content but none of the mounts within <literal role="filename">/media</literal> appear in <literal role="filename">/mnt</literal>. For example, if the CD-ROM drive contains non-empty media and the <literal role="filename">/media/cdrom/</literal> directory exists, use:</simpara>
<screen># mount /dev/cdrom /media/cdrom
# ls /media/cdrom
EFI  GPL  isolinux  LiveOS
# ls /mnt/cdrom
#</screen>
</listitem>
<listitem>
<simpara>It is also possible to verify that file systems mounted in the <literal role="filename">/mnt</literal> directory are not reflected in <literal role="filename">/media</literal>. For instance, if a non-empty USB flash drive that uses the <literal role="filename">/dev/sdc1</literal> device is plugged in and the <literal role="filename">/mnt/flashdisk/</literal> directory is present, use:</simpara>
<screen># mount /dev/sdc1 /mnt/flashdisk
# ls /media/flashdisk
# ls /mnt/flashdisk
en-US  publican.cfg</screen>
</listitem>
</orderedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-a-shared-mount-point-duplicate_sharing-a-mount-on-multiple-mount-points">
<title>Creating a shared mount point duplicate</title>
<simpara>This procedure duplicates a mount point as a shared mount. File systems that you later mount under the original directory or the duplicate are always reflected in the other.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a virtual file system (VFS) node from the original mount point:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mark the original mount point as shared:</simpara>
<screen># mount --make-shared <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
<simpara>Alternatively, to change the mount type for the selected mount point and all mount points under it, use the <literal role="option">--make-rshared</literal> option instead of <literal role="option">--make-shared</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create the duplicate:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">duplicate-dir</phrase></emphasis></screen>
</listitem>
</orderedlist>
<example>
<title>Duplicating /media into /mnt as a shared mount point</title>
<simpara>To make the <literal role="filename">/media</literal> and <literal role="filename">/mnt</literal> directories share the same content:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a VFS node from the <literal role="filename">/media</literal> directory:</simpara>
<screen># mount --bind /media /media</screen>
</listitem>
<listitem>
<simpara>Mark the <literal role="filename">/media</literal> directory as shared:</simpara>
<screen># mount --make-shared /media</screen>
</listitem>
<listitem>
<simpara>Create its duplicate in <literal role="filename">/mnt</literal>:</simpara>
<screen># mount --bind /media /mnt</screen>
</listitem>
<listitem>
<simpara>It is now possible to verify that a mount within <literal role="filename">/media</literal> also appears in <literal role="filename">/mnt</literal>. For example, if the CD-ROM drive contains non-empty media and the <literal role="filename">/media/cdrom/</literal> directory exists, use:</simpara>
<screen># mount /dev/cdrom /media/cdrom
# ls /media/cdrom
EFI  GPL  isolinux  LiveOS
# ls /mnt/cdrom
EFI  GPL  isolinux  LiveOS</screen>
</listitem>
<listitem>
<simpara>Similarly, it is possible to verify that any file system mounted in the <literal role="filename">/mnt</literal> directory is reflected in <literal role="filename">/media</literal>. For instance, if a non-empty USB flash drive that uses the <literal role="filename">/dev/sdc1</literal> device is plugged in and the <literal role="filename">/mnt/flashdisk/</literal> directory is present, use:</simpara>
<screen># mount /dev/sdc1 /mnt/flashdisk
# ls /media/flashdisk
en-US  publican.cfg
# ls /mnt/flashdisk
en-US  publican.cfg</screen>
</listitem>
</orderedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-a-slave-mount-point-duplicate_sharing-a-mount-on-multiple-mount-points">
<title>Creating a slave mount point duplicate</title>
<simpara>This procedure duplicates a mount point as a slave mount. File systems that you later mount under the original mount point are reflected in the duplicate but not the other way around.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a virtual file system (VFS) node from the original mount point:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mark the original mount point as shared:</simpara>
<screen># mount --make-shared <emphasis><phrase role="replaceable">original-dir</phrase></emphasis></screen>
<simpara>Alternatively, to change the mount type for the selected mount point and all mount points under it, use the <literal role="option">--make-rshared</literal> option instead of <literal role="option">--make-shared</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create the duplicate and mark it as slave:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">original-dir</phrase></emphasis> <emphasis><phrase role="replaceable">duplicate-dir</phrase></emphasis>
# mount --make-slave <emphasis><phrase role="replaceable">duplicate-dir</phrase></emphasis></screen>
</listitem>
</orderedlist>
<example>
<title>Duplicating /media into /mnt as a slave mount point</title>
<simpara>This example shows how to get the content of the <literal role="filename">/media</literal> directory to appear in <literal role="filename">/mnt</literal> as well, but without any mounts in the <literal role="filename">/mnt</literal> directory to be reflected in <literal role="filename">/media</literal>.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a VFS node from the <literal role="filename">/media</literal> directory:</simpara>
<screen># mount --bind /media /media</screen>
</listitem>
<listitem>
<simpara>Mark the <literal role="filename">/media</literal> directory as shared:</simpara>
<screen># mount --make-shared /media</screen>
</listitem>
<listitem>
<simpara>Create its duplicate in <literal role="filename">/mnt</literal> and mark it as slave:</simpara>
<screen># mount --bind /media /mnt
# mount --make-slave /mnt</screen>
</listitem>
<listitem>
<simpara>Verify that a mount within <literal role="filename">/media</literal> also appears in <literal role="filename">/mnt</literal>. For example, if the CD-ROM drive contains non-empty media and the <literal role="filename">/media/cdrom/</literal> directory exists, use:</simpara>
<screen># mount /dev/cdrom /media/cdrom
# ls /media/cdrom
EFI  GPL  isolinux  LiveOS
# ls /mnt/cdrom
EFI  GPL  isolinux  LiveOS</screen>
</listitem>
<listitem>
<simpara>Also verify that file systems mounted in the <literal role="filename">/mnt</literal> directory are not reflected in <literal role="filename">/media</literal>. For instance, if a non-empty USB flash drive that uses the <literal role="filename">/dev/sdc1</literal> device is plugged in and the <literal role="filename">/mnt/flashdisk/</literal> directory is present, use:</simpara>
<screen># mount /dev/sdc1 /mnt/flashdisk
# ls /media/flashdisk
# ls /mnt/flashdisk
en-US  publican.cfg</screen>
</listitem>
</orderedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="preventing-a-mount-point-from-being-duplicated_sharing-a-mount-on-multiple-mount-points">
<title>Preventing a mount point from being duplicated</title>
<simpara>This procedure marks a mount point as unbindable so that it is not possible to duplicate it in another mount point.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change the type of a mount point to an unbindable mount, use:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">mount-point</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis>
# mount --make-unbindable <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
<simpara>Alternatively, to change the mount type for the selected mount point and all mount points under it, use the <literal role="option">--make-runbindable</literal> option instead of <literal role="option">--make-unbindable</literal>.</simpara>
<simpara>Any subsequent attempt to make a duplicate of this mount fails with the following error:</simpara>
<screen># mount --bind <emphasis><phrase role="replaceable">mount-point</phrase></emphasis> <emphasis><phrase role="replaceable">duplicate-dir</phrase></emphasis>

mount: wrong fs type, bad option, bad superblock on <emphasis><phrase role="replaceable">mount-point</phrase></emphasis>,
missing codepage or helper program, or other error
In some cases useful info is found in syslog - try
dmesg | tail  or so</screen>
</listitem>
</itemizedlist>
<example>
<title>Preventing /media from being duplicated</title>
<itemizedlist>
<listitem>
<simpara>To prevent the <literal role="filename">/media</literal> directory from being shared, use:</simpara>
<screen># mount --bind /media /media
# mount --make-unbindable /media</screen>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-sharing-a-mount-on-multiple-mount-points">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Shared subtrees</emphasis> article on Linux Weekly News: <link xlink:href="https://lwn.net/Articles/159077/">https://lwn.net/Articles/159077/</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assembly_persistently-mounting-file-systems_assembly_mounting-file-systems">
<title>Persistently mounting file systems</title>
<simpara>As a system administrator, you can persistently mount file systems to configure non-removable storage.</simpara>
<section xml:id="con_the-etc-fstab-file_assembly_persistently-mounting-file-systems">
<title>The /etc/fstab file</title>
<simpara>This section describes the <literal role="filename">/etc/fstab</literal> configuration file, which controls persistent mount points of file systems. Using <literal role="filename">/etc/fstab</literal> is the recommended way to persistently mount file systems.</simpara>
<simpara>Each line in the <literal role="filename">/etc/fstab</literal> file defines a mount point of a file system. It includes six fields separated by white space:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The block device identified by a persistent attribute or a path it the <literal>/dev</literal> directory.</simpara>
</listitem>
<listitem>
<simpara>The directory where the device will be mounted.</simpara>
</listitem>
<listitem>
<simpara>The file system on the device.</simpara>
</listitem>
<listitem>
<simpara>Mount options for the file system. The option <literal>defaults</literal> means that the partition is mounted at boot time with default options. This section also recognizes <literal>systemd</literal> mount unit options in the <literal>x-systemd.<emphasis><phrase role="replaceable">option</phrase></emphasis></literal> format.</simpara>
</listitem>
<listitem>
<simpara>Backup option for the <literal>dump</literal> utility.</simpara>
</listitem>
<listitem>
<simpara>Check order for the <literal>fsck</literal> utility.</simpara>
</listitem>
</orderedlist>
<example>
<title>The <literal role="filename">/boot</literal> file system in <literal role="filename">/etc/fstab</literal></title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="6">
<colspec colname="col_1" colwidth="29*"/>
<colspec colname="col_2" colwidth="14*"/>
<colspec colname="col_3" colwidth="14*"/>
<colspec colname="col_4" colwidth="14*"/>
<colspec colname="col_5" colwidth="14*"/>
<colspec colname="col_6" colwidth="14*"/>
<thead>
<row>
<entry align="left" valign="top">Block device</entry>
<entry align="left" valign="top">Mount point</entry>
<entry align="left" valign="top">File system</entry>
<entry align="left" valign="top">Options</entry>
<entry align="left" valign="top">Backup</entry>
<entry align="left" valign="top">Check</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>UUID=ea74bbec-536d-490c-b8d9-5b40bbd7545b</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>/boot</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>defaults</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>The <literal>systemd</literal> service automatically generates mount units from entries in <literal role="filename">/etc/fstab</literal>.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>fstab(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis>fstab</emphasis> section of the <literal>systemd.mount(5)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="adding-a-file-system-to-etc-fstab_assembly_persistently-mounting-file-systems">
<title>Adding a file system to /etc/fstab</title>
<simpara>This procedure describes how to configure persistent mount point for a file system in the <literal role="filename">/etc/fstab</literal> configuration file.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Find out the UUID attribute of the file system:</simpara>
<screen>$ lsblk --fs <emphasis><phrase role="replaceable">storage-device</phrase></emphasis></screen>
<simpara>For example:</simpara>
<example>
<title>Viewing the UUID of a partition</title>
<screen>$ lsblk --fs <emphasis><phrase role="replaceable">/dev/sda1</phrase></emphasis>

NAME FSTYPE LABEL <emphasis role="strong">UUID</emphasis>                                 MOUNTPOINT
sda1 xfs    Boot  <emphasis role="strong">ea74bbec-536d-490c-b8d9-5b40bbd7545b</emphasis> /boot</screen>
</example>
</listitem>
<listitem>
<simpara>If the mount point directory does not exist, create it:</simpara>
<screen># mkdir --parents <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>As root, edit the <literal role="filename">/etc/fstab</literal> file and add a line for the file system, identified by the UUID.</simpara>
<simpara>For example:</simpara>
<example>
<title>The /boot mount point in /etc/fstab</title>
<screen>UUID=ea74bbec-536d-490c-b8d9-5b40bbd7545b /boot xfs defaults 0 0</screen>
</example>
</listitem>
<listitem>
<simpara>Regenerate mount units so that your system registers the new configuration:</simpara>
<screen># systemctl daemon-reload</screen>
</listitem>
<listitem>
<simpara>Try mounting the file system to verify that the configuration works:</simpara>
<screen># mount <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>Other persistent attributes that you can use to identify the file system: <xref linkend="con_device-names-managed-by-the-udev-mechanism-in-dev-disk-_assembly_overview-of-persistent-naming-attributes"/></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="persistently-mounting-a-file-system-using-rhel-system-roles_assembly_persistently-mounting-file-systems">
<title>Persistently mounting a file system using RHEL System Roles</title>
<simpara>This section describes how to persistently mount a file system using the <literal>storage</literal> role.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Ansible playbook that uses the <literal>storage</literal> role exists.</simpara>
<simpara>For information on how to apply such a playbook, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#applying-a-role_con_intro-to-rhel-system-roles">Applying a role</link>.</simpara>
</listitem>
</itemizedlist>
<section xml:id="an-example-ansible-playbook-to-persistently-mount-a-file-system_persistently-mounting-a-file-system-using-rhel-system-roles">
<title>Example Ansible playbook to persistently mount a file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to immediately and persistently mount an XFS file system.</simpara>
<example>
<title>A playbook that mounts a file system on /dev/sdb to /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: <emphasis><phrase role="replaceable">xfs</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>This playbook adds the file system to the <literal role="filename">/etc/fstab</literal> file, and mounts the file system immediately.</simpara>
</listitem>
<listitem>
<simpara>If the file system on the <literal>/dev/sdb</literal> device or the mount point directory do not exist, the playbook creates them.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional_resources_2" remap="_additional_resources_2">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>For more information about the <literal>storage</literal> role, see <xref linkend="storage-role-intro_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="assembly_mounting-file-systems-on-demand_assembly_mounting-file-systems">
<title>Mounting file systems on demand</title>
<simpara>As a system administrator, you can configure file systems, such as NFS, to mount automatically on demand.</simpara>
<section xml:id="the-autofs-service_assembly_mounting-file-systems-on-demand">
<title>The autofs service</title>
<simpara>This section explains the benefits and basic concepts of the <literal>autofs</literal> service, used to mount file systems on demand.</simpara>
<simpara>One drawback of permanent mounting using the <literal role="filename">/etc/fstab</literal> configuration is that, regardless of how infrequently a user accesses the mounted file system, the system must dedicate resources to keep the mounted file system in place. This might affect system performance when, for example, the system is maintaining NFS mounts to many systems at one time.</simpara>
<simpara>An alternative to <literal role="filename">/etc/fstab</literal> is to use the kernel-based <literal>autofs</literal> service. It consists of the following components:</simpara>
<itemizedlist>
<listitem>
<simpara>A kernel module that implements a file system, and</simpara>
</listitem>
<listitem>
<simpara>A user-space service that performs all of the other functions.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>autofs</literal> service can mount and unmount file systems automatically (on-demand), therefore saving system resources. It can be used to mount file systems such as NFS, AFS, SMBFS, CIFS, and local file systems.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>autofs(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="the-autofs-configuration-files_assembly_mounting-file-systems-on-demand">
<title>The autofs configuration files</title>
<simpara>This section describes the usage and syntax of configuration files used by the <literal>autofs</literal> service.</simpara>
<bridgehead xml:id="the_master_map_file" renderas="sect4" remap="_the_master_map_file">The master map file</bridgehead>
<simpara>The <literal>autofs</literal> service uses <literal role="filename">/etc/auto.master</literal> (master map) as its default primary configuration file. This can be changed to use another supported network source and name using the <literal>autofs</literal> configuration in the <literal role="filename">/etc/autofs.conf</literal> configuration file in conjunction with the Name Service Switch (NSS) mechanism.</simpara>
<simpara>All on-demand mount points must be configured in the master map. Mount point, host name, exported directory, and options can all be specified in a set of files (or other supported network sources) rather than configuring them manually for each host.</simpara>
<simpara>The master map file lists mount points controlled by <literal>autofs</literal>, and their corresponding configuration files or network sources known as automount maps. The format of the master map is as follows:</simpara>
<screen><emphasis><phrase role="replaceable">mount-point</phrase></emphasis>  <emphasis><phrase role="replaceable">map-name</phrase></emphasis>  <emphasis><phrase role="replaceable">options</phrase></emphasis></screen>
<simpara>The variables used in this format are:</simpara>
<variablelist>
<varlistentry>
<term><emphasis><phrase role="replaceable">mount-point</phrase></emphasis></term>
<listitem>
<simpara>The <literal>autofs</literal> mount point; for example, <literal role="filename">/mnt/data/</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">map-file</phrase></emphasis></term>
<listitem>
<simpara>The map source file, which contains a list of mount points and the file system location from which those mount points should be mounted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">options</phrase></emphasis></term>
<listitem>
<simpara>If supplied, these apply to all entries in the given map, if they do not themselves have options specified.</simpara>
</listitem>
</varlistentry>
</variablelist>
<example>
<title>The /etc/auto.master file</title>
<simpara>The following is a sample line from <literal role="filename">/etc/auto.master</literal> file:</simpara>
<screen>/mnt/data  /etc/auto.data</screen>
</example>
<bridgehead xml:id="map_files" renderas="sect4" remap="_map_files">Map files</bridgehead>
<simpara>Map files configure the properties of individual on-demand mount points.</simpara>
<simpara>The automounter creates the directories if they do not exist. If the directories exist before the automounter was started, the automounter will not remove them when it exits. If a timeout is specified, the directory is automatically unmounted if the directory is not accessed for the timeout period.</simpara>
<simpara>The general format of maps is similar to the master map. However, the options field appears between the mount point and the location instead of at the end of the entry as in the master map:</simpara>
<screen><emphasis><phrase role="replaceable">mount-point</phrase></emphasis>  <emphasis><phrase role="replaceable">options</phrase></emphasis>  <emphasis><phrase role="replaceable">location</phrase></emphasis></screen>
<simpara>The variables used in this format are:</simpara>
<variablelist>
<varlistentry>
<term><emphasis><phrase role="replaceable">mount-point</phrase></emphasis></term>
<listitem>
<simpara>This refers to the <literal>autofs</literal> mount point. This can be a single directory name for an indirect mount or the full path of the mount point for direct mounts. Each direct and indirect map entry key (<emphasis><phrase role="replaceable">mount-point</phrase></emphasis>) can be followed by a space separated list of offset directories (subdirectory names each beginning with <literal>/</literal>) making them what is known as a multi-mount entry.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">options</phrase></emphasis></term>
<listitem>
<simpara>When supplied, these are the mount options for the map entries that do not specify their own options. This field is optional.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><emphasis><phrase role="replaceable">location</phrase></emphasis></term>
<listitem>
<simpara>This refers to the file system location such as a local file system path (preceded with the Sun map format escape character <literal>:</literal> for map names beginning with <literal>/</literal>), an NFS file system or other valid file system location.</simpara>
</listitem>
</varlistentry>
</variablelist>
<example>
<title>A map file</title>
<simpara>The following is a sample from a map file; for example, <literal role="filename">/etc/auto.misc</literal>:</simpara>
<screen>payroll  -fstype=nfs4  personnel:/dev/disk/by-uuid/52b94495-e106-4f29-b868-fe6f6c2789b1
sales    -fstype=xfs   :/dev/disk/by-uuid/5564ed00-6aac-4406-bfb4-c59bf5de48b5</screen>
<simpara>The first column in the map file indicates the <literal>autofs</literal> mount point: <literal>sales</literal> and <literal>payroll</literal> from the server called <literal>personnel</literal>. The second column indicates the options for the <literal>autofs</literal> mount. The third column indicates the source of the mount.</simpara>
<simpara>Following the given configuration, the <literal>autofs</literal> mount points will be <literal>/home/payroll</literal> and <literal>/home/sales</literal>. The <literal role="option">-fstype=</literal> option is often omitted and is generally not needed for correct operation.</simpara>
<simpara>Using the given configuration, if a process requires access to an <literal>autofs</literal> unmounted directory such as <literal role="filename">/home/payroll/2006/July.sxc</literal>, the <literal>autofs</literal> service automatically mounts the directory.</simpara>
</example>
<bridgehead xml:id="the_amd_map_format" renderas="sect4" remap="_the_amd_map_format">The amd map format</bridgehead>
<simpara>The <literal>autofs</literal> service recognizes map configuration in the <literal>amd</literal> format as well. This is useful if you want to reuse existing automounter configuration written for the <literal>am-utils</literal> service, which has been removed from Red Hat Enterprise Linux.</simpara>
<simpara>However, Red Hat recommends using the simpler <literal>autofs</literal> format described in the previous sections.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>autofs(5)</literal>, <literal>autofs.conf(5)</literal>, and <literal>auto.master(5)</literal> man pages.</simpara>
</listitem>
<listitem>
<simpara>For details on the <literal>amd</literal> map format, see the <literal role="filename">/usr/share/doc/autofs/README.amd-maps</literal> file, which is provided by the <literal role="package">autofs</literal> package.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-autofs-mount-points_assembly_mounting-file-systems-on-demand">
<title>Configuring autofs mount points</title>
<simpara>This procedure describes how to configure on-demand mount points using the <literal>autofs</literal> service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal role="package">autofs</literal> package:</simpara>
<screen># yum install autofs</screen>
</listitem>
<listitem>
<simpara>Start and enable the <literal>autofs</literal> service:</simpara>
<screen># systemctl enable --now autofs</screen>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a map file for the on-demand mount point, located at <literal role="filename">/etc/auto.<emphasis><phrase role="replaceable">identifier</phrase></emphasis></literal>. Replace <emphasis><phrase role="replaceable">identifier</phrase></emphasis> with a name that identifies the mount point.</simpara>
</listitem>
<listitem>
<simpara>In the map file, fill in the mount point, options, and location fields as described in <xref linkend="the-autofs-configuration-files_assembly_mounting-file-systems-on-demand"/>.</simpara>
</listitem>
<listitem>
<simpara>Register the map file in the master map file, as described in <xref linkend="the-autofs-configuration-files_assembly_mounting-file-systems-on-demand"/>.</simpara>
</listitem>
<listitem>
<simpara>Try accessing content in the on-demand directory:</simpara>
<screen>$ ls <emphasis><phrase role="replaceable">automounted-directory</phrase></emphasis></screen>
</listitem>
</orderedlist>
</section>
<section xml:id="automounting-user-home-directories-with-autofs-service_assembly_mounting-file-systems-on-demand">
<title>Automounting NFS server user home directories with autofs service</title>
<simpara>This procedure describes how to configure the autofs service to mount user home directories automatically.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <emphasis role="strong"><phrase role="package">autofs</phrase></emphasis> package is installed.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong"><phrase role="service">autofs</phrase></emphasis> service is enabled and running.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Specify the mount point and location of the map file by editing the <literal role="filename">/etc/auto.master</literal> file on a server on which you need to mount user home directories. To do so, add the following line into the <literal role="filename">/etc/auto.master</literal> file:</simpara>
<screen>/home /etc/auto.home</screen>
</listitem>
<listitem>
<simpara>Create a map file with the name of <literal role="filename">/etc/auto.home</literal> on a server on which you need to mount user home directories, and edit the file with the following parameters:</simpara>
<screen>* -fstype=nfs,rw,sync <emphasis>host.example.com</emphasis>:/home/&amp;i</screen>
<simpara>You can skip <literal><emphasis>fstype</emphasis></literal> parameter, as it is <literal><emphasis>nfs</emphasis></literal> by default. For more information, see <literal>autofs(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>Reload the <literal role="service">autofs</literal> service:</simpara>
<screen># systemctl reload autofs</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="overriding-or-augmenting-autofs-site-configuration-files_assembly_mounting-file-systems-on-demand">
<title>Overriding or augmenting autofs site configuration files</title>
<simpara>It is sometimes useful to override site defaults for a specific mount point on a client system.</simpara>
<example>
<title>Initial conditions</title>
<simpara>For example, consider the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>Automounter maps are stored in NIS and the <literal role="filename">/etc/nsswitch.conf</literal> file has the following directive:</simpara>
<screen>automount:    files nis</screen>
</listitem>
<listitem>
<simpara>The <literal>auto.master</literal> file contains:</simpara>
<screen>+auto.master</screen>
</listitem>
<listitem>
<simpara>The NIS <literal>auto.master</literal> map file contains:</simpara>
<screen>/home auto.home</screen>
</listitem>
<listitem>
<simpara>The NIS <literal>auto.home</literal> map contains:</simpara>
<screen>beth    fileserver.example.com:/export/home/beth
joe     fileserver.example.com:/export/home/joe
*       fileserver.example.com:/export/home/&amp;</screen>
</listitem>
<listitem>
<simpara>The file map <literal role="filename">/etc/auto.home</literal> does not exist.</simpara>
</listitem>
</itemizedlist>
</example>
<example>
<title>Mounting home directories from a different server</title>
<simpara>Given the preceding conditions, let’s assume that the client system needs to override the NIS map <literal>auto.home</literal> and mount home directories from a different server.</simpara>
<itemizedlist>
<listitem>
<simpara>In this case, the client needs to use the following <literal role="filename">/etc/auto.master</literal> map:</simpara>
<screen>/home ­/etc/auto.home
+auto.master</screen>
</listitem>
<listitem>
<simpara>The <literal role="filename">/etc/auto.home</literal> map contains the entry:</simpara>
<screen>*    host.example.com:/export/home/&amp;</screen>
</listitem>
</itemizedlist>
<simpara>Because the automounter only processes the first occurrence of a mount point, the <literal role="filename">/home</literal> directory contains the content of <literal role="filename">/etc/auto.home</literal> instead of the NIS <literal>auto.home</literal> map.</simpara>
</example>
<example>
<title>Augmenting auto.home with only selected entries</title>
<simpara>Alternatively, to augment the site-wide <literal>auto.home</literal> map with just a few entries:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create an <literal role="filename">/etc/auto.home</literal> file map, and in it put the new entries. At the end, include the NIS <literal>auto.home</literal> map. Then the <literal role="filename">/etc/auto.home</literal> file map looks similar to:</simpara>
<screen>mydir someserver:/export/mydir
+auto.home</screen>
</listitem>
<listitem>
<simpara>With these NIS <literal>auto.home</literal> map conditions, listing the content of the <literal role="filename">/home</literal> directory outputs:</simpara>
<screen>$ ls /home

beth joe mydir</screen>
</listitem>
</orderedlist>
<simpara>This last example works as expected because <literal>autofs</literal> does not include the contents of a file map of the same name as the one it is reading. As such, <literal>autofs</literal> moves on to the next map source in the <literal>nsswitch</literal> configuration.</simpara>
</example>
</section>
<section xml:id="using-ldap-to-store-automounter-maps_assembly_mounting-file-systems-on-demand">
<title>Using LDAP to store automounter maps</title>
<simpara>This procedure configures <literal>autofs</literal> to store automounter maps in LDAP configuration rather than in <literal>autofs</literal> map files.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>LDAP client libraries must be installed on all systems configured to retrieve automounter maps from LDAP. On Red Hat Enterprise Linux, the <literal role="package">openldap</literal> package should be installed automatically as a dependency of the <literal role="package">autofs</literal> package.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To configure LDAP access, modify the <literal role="filename">/etc/openldap/ldap.conf</literal> file. Ensure that the <literal>BASE</literal>, <literal>URI</literal>, and <literal>schema</literal> options are set appropriately for your site.</simpara>
</listitem>
<listitem>
<simpara>The most recently established schema for storing automount maps in LDAP is described by the <literal>rfc2307bis</literal> draft. To use this schema, set it in the <literal role="filename">/etc/autofs.conf</literal> configuration file by removing the comment characters from the schema definition. For example:</simpara>
<example>
<title>Setting autofs configuration</title>
<screen>DEFAULT_MAP_OBJECT_CLASS="automountMap"
DEFAULT_ENTRY_OBJECT_CLASS="automount"
DEFAULT_MAP_ATTRIBUTE="automountMapName"
DEFAULT_ENTRY_ATTRIBUTE="automountKey"
DEFAULT_VALUE_ATTRIBUTE="automountInformation"</screen>
</example>
</listitem>
<listitem>
<simpara>Ensure that all other schema entries are commented in the configuration. The <literal>automountKey</literal> attribute replaces the <literal>cn</literal> attribute in the <literal>rfc2307bis</literal> schema. Following is an example of an LDAP Data Interchange Format (LDIF) configuration:</simpara>
<example>
<title>LDF Configuration</title>
<screen># extended LDIF
#
# LDAPv3
# base &lt;&gt; with scope subtree
# filter: (&amp;(objectclass=automountMap)(automountMapName=auto.master))
# requesting: ALL
#

# auto.master, example.com
dn: automountMapName=auto.master,dc=example,dc=com
objectClass: top
objectClass: automountMap
automountMapName: auto.master

# extended LDIF
#
# LDAPv3
# base &lt;automountMapName=auto.master,dc=example,dc=com&gt; with scope subtree
# filter: (objectclass=automount)
# requesting: ALL
#

# /home, auto.master, example.com
dn: automountMapName=auto.master,dc=example,dc=com
objectClass: automount
cn: /home

automountKey: /home
automountInformation: auto.home

# extended LDIF
#
# LDAPv3
# base &lt;&gt; with scope subtree
# filter: (&amp;(objectclass=automountMap)(automountMapName=auto.home))
# requesting: ALL
#

# auto.home, example.com
dn: automountMapName=auto.home,dc=example,dc=com
objectClass: automountMap
automountMapName: auto.home

# extended LDIF
#
# LDAPv3
# base &lt;automountMapName=auto.home,dc=example,dc=com&gt; with scope subtree
# filter: (objectclass=automount)
# requesting: ALL
#

# foo, auto.home, example.com
dn: automountKey=foo,automountMapName=auto.home,dc=example,dc=com
objectClass: automount
automountKey: foo
automountInformation: filer.example.com:/export/foo

# /, auto.home, example.com
dn: automountKey=/,automountMapName=auto.home,dc=example,dc=com
objectClass: automount
automountKey: /
automountInformation: filer.example.com:/export/&amp;</screen>
</example>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>rfc2307bis</literal> draft: <link xlink:href="https://tools.ietf.org/html/draft-howard-rfc2307bis">https://tools.ietf.org/html/draft-howard-rfc2307bis</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="setting-read-only-permissions-for-the-root-file-system_assembly_mounting-file-systems">
<title>Setting read-only permissions for the root file system</title>
<simpara>Sometimes, you need to mount the root file system (<literal role="filename">/</literal>) with read-only permissions. Example use cases include enhancing security or ensuring data integrity after an unexpected system power-off.</simpara>
<section xml:id="files-and-directories-that-always-retain-write-permissions_setting-read-only-permissions-for-the-root-file-system">
<title>Files and directories that always retain write permissions</title>
<simpara>For the system to function properly, some files and directories need to retain write permissions. When the root file system is mounted in read-only mode, these files are mounted in RAM using the <literal>tmpfs</literal> temporary file system.</simpara>
<simpara>The default set of such files and directories is read from the <literal role="filename">/etc/rwtab</literal> file, which contains:</simpara>
<screen>dirs	/var/cache/man
dirs	/var/gdm
<emphasis>&lt;content truncated&gt;</emphasis>

empty	/tmp
empty	/var/cache/foomatic
<emphasis>&lt;content truncated&gt;</emphasis>

files	/etc/adjtime
files	/etc/ntp.conf
<emphasis>&lt;content truncated&gt;</emphasis></screen>
<simpara>Entries in the <literal role="filename">/etc/rwtab</literal> file follow this format:</simpara>
<screen><emphasis><phrase role="replaceable">copy-method</phrase></emphasis>    <emphasis><phrase role="replaceable">path</phrase></emphasis></screen>
<simpara>In this syntax:</simpara>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">copy-method</phrase></emphasis> with one of the keywords specifying how the file or directory is copied to tmpfs.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">path</phrase></emphasis> with the path to the file or directory.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal role="filename">/etc/rwtab</literal> file recognizes the following ways in which a file or directory can be copied to <literal>tmpfs</literal>:</simpara>
<variablelist>
<varlistentry>
<term><literal>empty</literal></term>
<listitem>
<simpara>An empty path is copied to <literal>tmpfs</literal>. For example:</simpara>
<screen>empty /tmp</screen>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>dirs</literal></term>
<listitem>
<simpara>A directory tree is copied to <literal>tmpfs</literal>, empty. For example:</simpara>
<screen>dirs /var/run</screen>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>files</literal></term>
<listitem>
<simpara>A file or a directory tree is copied to <literal>tmpfs</literal> intact. For example:</simpara>
<screen>files /etc/resolv.conf</screen>
</listitem>
</varlistentry>
</variablelist>
<simpara>The same format applies when adding custom paths to <literal role="filename">/etc/rwtab.d/</literal>.</simpara>
</section>
<section xml:id="configuring-the-root-file-system-to-mount-with-read-only-permissions-on-boot_setting-read-only-permissions-for-the-root-file-system">
<title>Configuring the root file system to mount with read-only permissions on boot</title>
<simpara>With this procedure, the root file system is mounted read-only on all following boots.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <literal role="filename">/etc/sysconfig/readonly-root</literal> file, set the <literal role="option">READONLY</literal> option to <literal>yes</literal>:</simpara>
<screen># Set to 'yes' to mount the file systems as read-only.
READONLY=yes</screen>
</listitem>
<listitem>
<simpara>Add the <literal role="option">ro</literal> option in the root entry (<literal role="filename">/</literal>) in the <literal role="filename">/etc/fstab</literal> file:</simpara>
<screen>/dev/mapper/luks-c376919e...  /  xfs  x-systemd.device-timeout=0,<emphasis role="strong">ro</emphasis>  1  1</screen>
</listitem>
<listitem>
<simpara>Add the <literal role="option">ro</literal> option to the <literal>GRUB_CMDLINE_LINUX</literal> directive in the <literal role="filename">/etc/default/grub</literal> file and ensure that the directive does not contain <literal>rw</literal>:</simpara>
<screen>GRUB_CMDLINE_LINUX="rhgb quiet... <emphasis role="strong">ro</emphasis>"</screen>
</listitem>
<listitem>
<simpara>Recreate the GRUB2 configuration file:</simpara>
<screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
</listitem>
<listitem>
<simpara>If you need to add files and directories to be mounted with write permissions in the <literal>tmpfs</literal> file system, create a text file in the <literal role="filename">/etc/rwtab.d/</literal> directory and put the configuration there.</simpara>
<simpara>For example, to mount the <literal role="filename">/etc/example/file</literal> file with write permissions, add this line to the <literal role="filename">/etc/rwtab.d/example</literal> file:</simpara>
<screen>files /etc/example/file</screen>
<important>
<simpara>Changes made to files and directories in <literal>tmpfs</literal> do not persist across boots.</simpara>
</important>
</listitem>
<listitem>
<simpara>Reboot the system to apply the changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Troubleshooting</title>
<listitem>
<simpara>If you mount the root file system with read-only permissions by mistake, you can remount it with read-and-write permissions again using the following command:</simpara>
<screen># mount -o remount,rw /</screen>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="limiting-storage-space-usage-with-quotas_managing-file-systems">
<title>Limiting storage space usage with quotas</title>
<simpara>You can restrict the amount of disk space available to users or groups by implementing disk quotas. You can also define a warning level at which system administrators are informed before a user consumes too much disk space or a partition becomes full.</simpara>
<section xml:id="disk-quotas_limiting-storage-space-usage-with-quotas">
<title>Disk quotas</title>
<simpara>In most computing environments, disk space is not infinite. The quota subsystem provides a mechanism to  control  usage  of  disk  space.</simpara>
<simpara>You can configure disk quotas for individual users as well as user groups on the local file systems. This makes it possible to manage the space allocated for user-specific files (such as email) separately from the space allocated to the projects that a user works on. The quota subsystem warns users when they exceed their allotted limit, but allows some extra space for current work (hard limit/soft limit).</simpara>
<simpara>If quotas are implemented, you need to check if the quotas are exceeded and make sure the quotas are accurate. If users repeatedly exceed their quotas or consistently reach their soft limits, a system administrator can either help the user determine how to use less disk space or increase the user’s disk quota.</simpara>
<simpara>You can set quotas to control:</simpara>
<itemizedlist>
<listitem>
<simpara>The number of consumed disk blocks.</simpara>
</listitem>
<listitem>
<simpara>The number of inodes, which are data structures that contain information about files in UNIX file systems. Because inodes store file-related information, this allows control over the number of files that can be created.</simpara>
</listitem>
</itemizedlist>
<section xml:id="the-xfs_quota-tool_disk-quotas">
<title>The <literal>xfs_quota</literal> tool</title>
<simpara>You can use the <literal>xfs_quota</literal> tool to manage quotas on XFS file systems. In addition, you can use XFS file systems with limit enforcement turned off as an effective disk usage accounting system.</simpara>
<simpara>The  XFS  quota  system  differs  from other file systems in a number of ways.  Most importantly, XFS considers quota information as file system metadata and uses journaling to provide a higher level guarantee of consistency.</simpara>
<bridgehead xml:id="additional_resources_3" renderas="sect4" remap="_additional_resources_3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>The <literal>xfs_quota(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="managing-file-system-quotas-in-xfs_limiting-storage-space-usage-with-quotas">
<title>Managing XFS disk quotas</title>
<simpara>You can use the <literal>xfs_quota</literal> tool to manage quotas in XFS and to configure limits for project-controlled directories.</simpara>
<simpara>Generic quota configuration tools (<literal>quota</literal>, <literal>repquota</literal>, and <literal>edquota</literal> for example) may also be used to manipulate XFS quotas. However, these tools cannot be used with XFS project quotas.</simpara>
<important>
<simpara>Red Hat recommends the use of <literal>xfs_quota</literal> over all other available tools.</simpara>
</important>
<section xml:id="file-system-quota-management-in-xfs_file-system-quota-management-in-xfs">
<title>File system quota management in XFS</title>
<simpara>The XFS quota subsystem manages limits on disk space (blocks) and file (inode) usage. XFS quotas control or report on usage of these items on a user, group, or directory or project level. Group and project quotas are only mutually exclusive on older non-default XFS disk formats.</simpara>
<simpara>When managing on a per-directory or per-project basis, XFS manages the disk usage of directory hierarchies associated with a specific project.</simpara>
</section>
<section xml:id="enabling-disk-quotas-for-xfs_file-system-quota-management-in-xfs">
<title>Enabling disk quotas for XFS</title>
<simpara>This procedure enables disk quotas for users, groups, and projects on an XFS file system. Once quotas are enabled, the <literal>xfs_quota</literal> tool can be used to set limits and report on disk usage.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable quotas for users:</simpara>
<screen># mount -o uquota /dev/xvdb1 /xfs</screen>
<simpara>Replace <literal>uquota</literal> with <literal>uqnoenforce</literal> to allow usage reporting without enforcing any limits.</simpara>
</listitem>
<listitem>
<simpara>Enable quotas for groups:</simpara>
<screen># mount -o gquota /dev/xvdb1 /xfs</screen>
<simpara>Replace <literal>gquota</literal> with <literal>gqnoenforce</literal> to allow usage reporting without enforcing any limits.</simpara>
</listitem>
<listitem>
<simpara>Enable quotas for projects:</simpara>
<screen># mount -o pquota /dev/xvdb1 /xfs</screen>
<simpara>Replace <literal>pquota</literal> with <literal>pqnoenforce</literal> to allow usage reporting without enforcing any limits.</simpara>
</listitem>
<listitem>
<simpara>Alternatively, include the quota mount options in the <literal>/etc/fstab</literal> file. The following example shows entries in the <literal>/etc/fstab</literal> file to enable quotas for users, groups, and projects, respectively, on an XFS file system. These examples also mount the file system with read/write permissions:</simpara>
<screen># vim /etc/fstab
/dev/xvdb1    /xfs    xfs    rw,quota       0  0
/dev/xvdb1    /xfs    xfs    rw,gquota      0  0
/dev/xvdb1    /xfs    xfs    rw,prjquota    0  0</screen>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>xfs_quota(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="running-the-xfs_quota-tool_file-system-quota-management-in-xfs">
<title>Reporting XFS usage</title>
<simpara>You can use the <literal>xfs_quota</literal> tool to set limits and report on disk usage. By default, <literal>xfs_quota</literal> is run interactively, and in basic mode. Basic mode subcommands simply report usage, and are available to all users.</simpara>
<bridgehead xml:id="prerequisites" renderas="sect4" remap="_prerequisites">Prerequisites</bridgehead>
<itemizedlist>
<listitem>
<simpara>Quotas have been enabled for the XFS file system. See <link linkend="enabling-disk-quotas-for-xfs_file-system-quota-management-in-xfs">Enabling disk quotas for XFS</link>.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="procedure" renderas="sect4" remap="_procedure">Procedure</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Start the <literal>xfs_quota</literal> shell:</simpara>
<screen># xfs_quota</screen>
</listitem>
<listitem>
<simpara>Show usage and limits for the given user:</simpara>
<screen># xfs_quota&gt; quota <emphasis>username</emphasis></screen>
</listitem>
<listitem>
<simpara>Show free and used counts for blocks and inodes:</simpara>
<screen># xfs_quota&gt; df</screen>
</listitem>
<listitem>
<simpara>Run the help command to display the basic commands available with <literal>xfs_quota</literal>.</simpara>
<screen># xfs_quota&gt; help</screen>
</listitem>
<listitem>
<simpara>Specify <literal>q</literal> to exit <literal>xfs_quota</literal>.</simpara>
<screen># xfs_quota&gt; q</screen>
</listitem>
</orderedlist>
<bridgehead xml:id="additional_resources_4" renderas="sect4" remap="_additional_resources_4">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>The <literal>xfs_quota(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="running-the-xfs_quota-tool-in-expert-mode_file-system-quota-management-in-xfs">
<title>Modifying XFS quota limits</title>
<simpara>Start the <literal>xfs_quota</literal> tool with the <literal>-x</literal> option to enable expert mode and run the administrator commands, which allow modifications to the quota system. The subcommands of this mode allow actual configuration of limits, and are available only to users with elevated privileges.</simpara>
<bridgehead xml:id="prerequisites_2" renderas="sect4" remap="_prerequisites_2">Prerequisites</bridgehead>
<itemizedlist>
<listitem>
<simpara>Quotas have been enabled for the XFS file system. See <link linkend="enabling-disk-quotas-for-xfs_file-system-quota-management-in-xfs">Enabling disk quotas for XFS</link>.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="procedure_2" renderas="sect4" remap="_procedure_2">Procedure</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Start the <literal>xfs_quota</literal> shell with the <literal>-x</literal> option to enable expert mode:</simpara>
<screen># xfs_quota -x</screen>
</listitem>
<listitem>
<simpara>Report quota information for a specific file system:</simpara>
<screen># xfs_quota&gt; report /<emphasis>path</emphasis></screen>
<simpara>For example, to display a sample quota report for <literal>/home</literal> (on <literal>/dev/blockdevice</literal>), use the command <literal>report -h /home</literal>. This displays output similar to the following:</simpara>
<screen>User quota on /home (/dev/blockdevice)
Blocks
User ID      Used   Soft   Hard Warn/Grace
---------- ---------------------------------
root            0      0      0  00 [------]
testuser   103.4G      0      0  00 [------]</screen>
</listitem>
<listitem>
<simpara>Modify quota limits:</simpara>
<screen># xfs_quota&gt; limit isoft=<emphasis>500m</emphasis> ihard=<emphasis>700m</emphasis> <emphasis>user</emphasis> /<emphasis>path</emphasis></screen>
<simpara>For example, to set a soft and hard inode count limit of 500 and 700 respectively for user <literal>john</literal>, whose home directory is <literal>/home/john</literal>, use the following command:</simpara>
<screen># xfs_quota -x -c 'limit isoft=500 ihard=700 john' /home/</screen>
<simpara>In this case, pass <literal>mount_point</literal> which is the mounted xfs file system.</simpara>
</listitem>
<listitem>
<simpara>Run the help command to display the expert commands available with <literal>xfs_quota -x</literal>:</simpara>
<screen># xfs_quota&gt; help</screen>
<bridgehead xml:id="additional_resources_5" renderas="sect4" remap="_additional_resources_5">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>The <literal>xfs_quota(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="setting-project-limits-for-xfs_file-system-quota-management-in-xfs">
<title>Setting project limits for XFS</title>
<simpara>This procedure configures limits for project-controlled directories.</simpara>
<bridgehead xml:id="procedure_3" renderas="sect4" remap="_procedure_3">Procedure</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add the project-controlled directories to <literal>/etc/projects</literal>. For example, the following adds the <literal>/var/log</literal> path with a unique ID of 11 to <literal>/etc/projects</literal>. Your project ID can be any numerical value mapped to your project.</simpara>
<screen># echo 11:/var/log &gt;&gt; /etc/projects</screen>
</listitem>
<listitem>
<simpara>Add project names to <literal>/etc/projid</literal> to map project IDs to project names. For example, the following associates a project called <literal>Logs</literal> with the project ID of 11 as defined in the previous step.</simpara>
<screen># echo Logs:11 &gt;&gt; /etc/projid</screen>
</listitem>
<listitem>
<simpara>Initialize the project directory. For example, the following initializes the project directory <literal>/var</literal>:</simpara>
<screen># xfs_quota -x -c 'project -s logfiles' /var</screen>
</listitem>
<listitem>
<simpara>Configure quotas for projects with initialized directories:</simpara>
<screen># xfs_quota -x -c 'limit -p bhard=lg logfiles' /var</screen>
<bridgehead xml:id="additional_resources_6" renderas="sect4" remap="_additional_resources_6">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>The <literal>xfs_quota(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>projid(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>projects(5)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="managing-ext3-and-ext4-disk-quotas_limiting-storage-space-usage-with-quotas">
<title>Managing ext3 and ext4 disk quotas</title>
<simpara>You have to enable disk quotas on your system before you can assign them.
You can assign disk quotas per user, per group or per project. However, if there is a soft limit set, you can exceed these quotas for a configurable period of time, known as the grace period.</simpara>
<section xml:id="installing-quota-rpm_configuring-disk-quotas">
<title>Installing the quota tool</title>
<simpara>You must install the <literal>quota</literal> RPM package to implement disk quotas.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Install the <literal>quota</literal> package:</simpara>
</listitem>
</itemizedlist>
<screen># yum install quota</screen>
</section>
<section xml:id="enabling-quota-feature-in-file-system-creation_configuring-disk-quotas">
<title>Enabling quota feature on file system creation</title>
<simpara>This procedure describes how to enable quotas on file system creation.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable quotas on file system creation:</simpara>
<screen># mkfs.ext4 -O quota /dev/sda</screen>
<note>
<simpara>Only user and group quotas are enabled and initialized by default.</simpara>
</note>
</listitem>
<listitem>
<simpara>Change the defaults on file system creation:</simpara>
<screen># mkfs.ext4 -O quota -E quotatype=usrquota:grpquota:prjquota /dev/sda</screen>
</listitem>
<listitem>
<simpara>Mount the file system:</simpara>
<screen># mount /dev/sda</screen>
</listitem>
</orderedlist>
<formalpara>
<title>Additional resources</title>
<para>See the <literal>man</literal> page for <literal>ext4</literal> for additional information.</para>
</formalpara>
</section>
<section xml:id="enabling-quota-feature-on-existing-file-system_configuring-disk-quotas">
<title>Enabling quota feature on existing file systems</title>
<simpara>This procedure describes how to enable the quota feature on existing file system using the <literal>tune2fs</literal> command.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Unmount the file system:</simpara>
<screen># umount /dev/sda</screen>
</listitem>
<listitem>
<simpara>Enable quotas on existing file system:</simpara>
<screen># tune2fs -O quota /dev/sda</screen>
<note>
<simpara>Only user and group quotas are initialized by default.</simpara>
</note>
</listitem>
<listitem>
<simpara>Change the defaults:</simpara>
<screen># tune2fs -Q usrquota,grpquota,prjquota /dev/sda</screen>
</listitem>
<listitem>
<simpara>Mount the file system:</simpara>
<screen># mount /dev/sda</screen>
</listitem>
</orderedlist>
<formalpara>
<title>Additional resources</title>
<para>See the <literal>man</literal> page for <literal>ext4</literal> for additional information.</para>
</formalpara>
</section>
<section xml:id="enabling-quota-enforcement_configuring-disk-quotas">
<title>Enabling quota enforcement</title>
<simpara>The quota accounting is enabled by default after mounting the file system without any additional options, but quota enforcement is not.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Quota feature is enabled and the default quotas are initialized.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enable quota enforcement by <literal>quotaon</literal> for the user quota:</simpara>
<screen># mount /dev/sda /mnt</screen>
<screen># quotaon /mnt</screen>
<note>
<simpara>The quota enforcement can be enabled at mount time using <literal>usrquota</literal>, <literal>grpquota</literal>, or <literal>prjquota</literal> mount options.</simpara>
<screen># mount -o usrquota,grpquota,prjquota /dev/sda /mnt</screen>
</note>
</listitem>
<listitem>
<simpara>Enable user, group, and project quotas for all file systems:</simpara>
<screen># quotaon -vaugP</screen>
<itemizedlist>
<listitem>
<simpara>If neither of the <literal>-u</literal>, <literal>-g</literal>, or <literal>-P</literal> options are specified, only the user quotas are enabled.</simpara>
</listitem>
<listitem>
<simpara>If only <literal>-g</literal> option is specified, only group quotas are enabled.</simpara>
</listitem>
<listitem>
<simpara>If only <literal>-P</literal> option is specified, only project quotas are enabled.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Enable quotas for a specific file system, such as <literal>/home</literal>:</simpara>
<screen># quotaon -vugP /home</screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>See the <literal>quotaon(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="assigning-quotas-per-user_configuring-disk-quotas">
<title>Assigning quotas per user</title>
<simpara>The disk quotas are assigned to users with the <literal>edquota</literal> command.</simpara>
<note>
<simpara>The text editor defined by the
<literal>EDITOR</literal>
environment variable is used by <literal>edquota</literal>. To change the editor, set the <literal>EDITOR</literal>
environment variable in your <literal>~/.bash_profile</literal> file to the full path of the editor of your choice.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>User must exist prior to setting the user quota.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Assign the quota for a user:</simpara>
<screen># edquota <emphasis>username</emphasis></screen>
<simpara>Replace <emphasis>username</emphasis> with the user to which you want to assign the quotas.</simpara>
<simpara>For example, if you enable a quota for the <literal>/dev/sda</literal> partition and execute the command <literal>edquota testuser</literal>, the following is displayed in the default editor configured on the system:</simpara>
<literallayout class="monospaced">Disk quotas for user testuser (uid 501):
Filesystem   blocks   soft   hard   inodes   soft   hard
/dev/sda      44043      0      0    37418      0      0</literallayout>
</listitem>
<listitem>
<simpara>Change the desired limits.</simpara>
<simpara>If any of the values are set to 0, limit is not set. Change them in the text editor.</simpara>
<simpara>For example, the following shows the soft and hard block limits for the testuser have been set to 50000 and 55000 respectively.</simpara>
<screen>Disk quotas for user testuser (uid 501):
Filesystem   blocks   soft   hard   inodes   soft   hard
/dev/sda      44043  50000  55000    37418      0      0</screen>
<itemizedlist>
<listitem>
<simpara>The first column is the name of the file system that has a quota enabled for it.</simpara>
</listitem>
<listitem>
<simpara>The second column shows how many blocks the user is currently using.</simpara>
</listitem>
<listitem>
<simpara>The next two columns are used to set soft and hard block limits for the user on the file system.</simpara>
</listitem>
<listitem>
<simpara>The <literal>inodes</literal> column shows how many inodes the user is currently using.</simpara>
</listitem>
<listitem>
<simpara>The last two columns are used to set the soft and hard inode limits for the user on the file system.</simpara>
<itemizedlist>
<listitem>
<simpara>The hard block limit is the absolute maximum amount of disk space that a user or group can use. Once this limit is reached, no further disk space can be used.</simpara>
</listitem>
<listitem>
<simpara>The soft block limit defines the maximum amount of disk space that can be used. However, unlike the hard limit, the soft limit can be exceeded for a certain amount of time. That time is known as the <emphasis>grace period</emphasis>. The grace period can
be expressed in seconds, minutes, hours, days, weeks, or months.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification steps</title>
<listitem>
<simpara>Verify that the quota for the user has been set:</simpara>
<literallayout class="monospaced"># quota -v testuser
Disk quotas for user testuser:
Filesystem  blocks  quota  limit  grace  files  quota  limit  grace
/dev/sda      1000*  1000   1000             0      0      0</literallayout>
</listitem>
</itemizedlist>
</section>
<section xml:id="assigning-quotas-per-group_configuring-disk-quotas">
<title>Assigning quotas per group</title>
<simpara>You can assign quotas on a per-group basis.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Group must exist prior to setting the group quota.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set a group quota:</simpara>
<screen># edquota -g <emphasis>groupname</emphasis></screen>
<simpara>For example, to set a group quota for the <literal>devel</literal> group:</simpara>
<screen># edquota -g devel</screen>
<simpara>This command displays the existing quota for the group in the text editor:</simpara>
<screen>Disk quotas for group devel (gid 505):
Filesystem   blocks  soft  hard  inodes  soft  hard
/dev/sda     440400     0     0   37418     0     0</screen>
</listitem>
<listitem>
<simpara>Modify the limits and save the file.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification steps</title>
<listitem>
<simpara>Verify that the group quota is set:</simpara>
<screen># quota -vg <emphasis>groupname</emphasis></screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="assigning-quotas-per-project_configuring-disk-quotas">
<title>Assigning quotas per project</title>
<simpara>This procedure assigns quotas per project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Project quota is enabled on your file system.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the project-controlled directories to <literal>/etc/projects</literal>. For example, the following adds the <literal>/var/log</literal> path with a unique ID of 11 to <literal>/etc/projects</literal>.
Your project ID can be any numerical value mapped to your project.</simpara>
<screen># echo 11:/var/log &gt;&gt; /etc/projects</screen>
</listitem>
<listitem>
<simpara>Add project names to <literal>/etc/projid</literal> to map project IDs to project names. For example, the following associates a project called <literal>Logs</literal> with the project ID of 11
as defined in the previous step.</simpara>
<screen># echo Logs:11 &gt;&gt; /etc/projid</screen>
</listitem>
<listitem>
<simpara>Set the desired limits:</simpara>
<screen># edquota -P 11</screen>
<note>
<simpara>You can choose the project either by its project ID (<literal>11</literal> in this case), or by its name (<literal>Logs</literal> in this case).</simpara>
</note>
</listitem>
<listitem>
<simpara>Using <literal>quotaon</literal>, enable quota enforcement:</simpara>
<simpara>See <link linkend="enabling-quota-enforcement_configuring-disk-quotas">Enabling quota enforcement</link>.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification steps</title>
<listitem>
<simpara>Verify that the project quota is set:</simpara>
<screen># quota -vP 11</screen>
<note>
<simpara>You can verify either by the project ID, or by the project name.</simpara>
</note>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>edquota(8)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>projid(5)</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>projects(5)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-the-grace-period-for-soft-limits_configuring-disk-quotas">
<title>Setting the grace period for soft limits</title>
<simpara>If a given quota has soft limits, you can edit the grace period, which is the amount of time for which a soft limit can be exceeded.
You can set the grace period for users, groups, or projects.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the grace period:</simpara>
<literallayout class="monospaced"># edquota -t</literallayout>
</listitem>
</itemizedlist>
<important>
<simpara>While other <literal>edquota</literal> commands operate on quotas for a particular user, group, or project, the <literal>-t</literal> option operates on every file system with quotas enabled.</simpara>
</important>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>edquota(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="turning-file-system-quotas-off_configuring-disk-quotas">
<title>Turning file system quotas off</title>
<simpara>Use <literal>quotaoff</literal> to turn disk quota enforcement off on the specified file systems. Quota accounting stays enabled after executing this command.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To turn all user and group quotas off:</simpara>
<screen># quotaoff -vaugP</screen>
<itemizedlist>
<listitem>
<simpara>If neither of the <literal>-u</literal>, <literal>-g</literal>, or <literal>-P</literal> options are specified, only the user quotas are disabled.</simpara>
</listitem>
<listitem>
<simpara>If only <literal>-g</literal> option is specified, only group quotas are disabled.</simpara>
</listitem>
<listitem>
<simpara>If only <literal>-P</literal> option is specified, only project quotas are disabled.</simpara>
</listitem>
<listitem>
<simpara>The <literal>-v</literal> switch causes verbose status information to display as the command executes.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>See the <literal>quotaoff(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="reporting-on-disk-quotas_configuring-disk-quotas">
<title>Reporting on disk quotas</title>
<simpara>You can create a disk quota report using the <literal>repquota</literal> utility.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the <literal>repquota</literal> command:</simpara>
<screen># repquota</screen>
<simpara>For example, the command <literal>repquota /dev/sda</literal> produces this output:</simpara>
<literallayout class="monospaced">*** Report for user quotas on device /dev/sda
Block grace time: 7days; Inode grace time: 7days
			Block limits			File limits
User		used	soft	hard	grace	used	soft	hard	grace
----------------------------------------------------------------------
root      --      36       0       0              4     0     0
kristin   --     540       0       0            125     0     0
testuser  --  440400  500000  550000          37418     0     0</literallayout>
</listitem>
<listitem>
<simpara>View the disk usage report for all quota-enabled file systems:</simpara>
<literallayout class="monospaced"># repquota -augP</literallayout>
</listitem>
</orderedlist>
<simpara>The <literal>--</literal> symbol displayed after each user determines whether the block or inode limits have been exceeded. If either soft limit
is exceeded, a <literal>+</literal> character appears in place of the corresponding <literal>-</literal> character. The first <literal>-</literal> character represents the block limit, and the
second represents the inode limit.</simpara>
<simpara>The <literal>grace</literal> columns are normally blank. If a soft limit has been exceeded, the column contains a time specification equal to the amount of time remaining on the grace period. If the grace period has
expired, <literal>none</literal> appears in its place.</simpara>
<formalpara>
<title>Additional resources</title>
<para>The <literal>repquota(8)</literal> man page for more information.</para>
</formalpara>
</section>
</section>
</chapter>
<chapter xml:id="discarding-unused-blocks_managing-file-systems">
<title>Discarding unused blocks</title>
<simpara>You can perform or schedule discard operations on block devices that support them.</simpara>
<section xml:id="block-discard-operations_discarding-unused-blocks">
<title>Block discard operations</title>
<simpara>Block discard operations discard blocks that are no longer in use by a mounted file system. They are useful on:</simpara>
<itemizedlist>
<listitem>
<simpara>Solid-state drives (SSDs)</simpara>
</listitem>
<listitem>
<simpara>Thinly-provisioned storage</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="requirements" renderas="sect3" remap="_requirements">Requirements</bridgehead>
<simpara>The block device underlying the file system must support physical discard operations.</simpara>
<simpara>Physical discard operations are supported if the value in the <literal role="filename">/sys/block/<emphasis><phrase role="replaceable">device</phrase></emphasis>/queue/discard_max_bytes</literal> file is not zero.</simpara>
</section>
<section xml:id="types-of-block-discard-operations_discarding-unused-blocks">
<title>Types of block discard operations</title>
<simpara>You can run discard operations using different methods:</simpara>
<variablelist>
<varlistentry>
<term>Batch discard</term>
<listitem>
<simpara>Are run explicitly by the user. They discard all unused blocks in the selected file systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Online discard</term>
<listitem>
<simpara>Are specified at mount time. They run in real time without user intervention. Online discard operations discard only the blocks that are transitioning from used to free.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Periodic discard</term>
<listitem>
<simpara>Are batch operations that are run regularly by a <literal>systemd</literal> service.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>All types are supported by the XFS and ext4 file systems and by VDO.</simpara>
<bridgehead xml:id="recommendations_2" renderas="sect3" remap="_recommendations_2">Recommendations</bridgehead>
<simpara>Red Hat recommends that you use batch or periodic discard.</simpara>
<simpara>Use online discard only if:</simpara>
<itemizedlist>
<listitem>
<simpara>the system’s workload is such that batch discard is not feasible, or</simpara>
</listitem>
<listitem>
<simpara>online discard operations are necessary to maintain performance.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="performing-batch-block-discard_discarding-unused-blocks">
<title>Performing batch block discard</title>
<simpara>This procedure performs a batch block discard operation to discard unused blocks on a mounted file system.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The file system is mounted.</simpara>
</listitem>
<listitem>
<simpara>The block device underlying the file system supports physical discard operations.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>fstrim</literal> utility:</simpara>
<itemizedlist>
<listitem>
<simpara>To perform discard only on a selected file system, use:</simpara>
<screen># fstrim <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>To perform discard on all mounted file systems, use:</simpara>
<screen># fstrim --all</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>If you execute the <literal>fstrim</literal> command on:</simpara>
<itemizedlist>
<listitem>
<simpara>a device that does not support discard operations, or</simpara>
</listitem>
<listitem>
<simpara>a logical device (LVM or MD) composed of multiple devices, where any one of the device does not support discard operations,</simpara>
</listitem>
</itemizedlist>
<simpara>the following message displays:</simpara>
<screen># fstrim <emphasis><phrase role="replaceable">/mnt/non_discard</phrase></emphasis>

fstrim: <emphasis><phrase role="replaceable">/mnt/non_discard</phrase></emphasis>: the discard operation is not supported</screen>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>fstrim(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="enabling-online-block-discard_discarding-unused-blocks">
<title>Enabling online block discard</title>
<simpara>This procedure enables online block discard operations that automatically discard unused blocks on all supported file systems.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enable online discard at mount time:</simpara>
<itemizedlist>
<listitem>
<simpara>When mounting a file system manually, add the <literal role="option">-o discard</literal> mount option:</simpara>
<screen># mount -o discard <emphasis><phrase role="replaceable">device</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>When mounting a file system persistently, add the <literal role="option">discard</literal> option to the mount entry in the <literal role="filename">/etc/fstab</literal> file.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page</simpara>
</listitem>
<listitem>
<simpara>The <literal>fstab(5)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="enabling-online-block-discard-using-rhel-system-roles_discarding-unused-blocks">
<title>Enabling online block discard using RHEL System Roles</title>
<simpara>This section describes how to enable online block discard using the <literal>storage</literal> role.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Ansible playbook including the <literal>storage</literal> role exists.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on how to apply such a playbook, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#applying-a-role_con_intro-to-rhel-system-roles">Applying a role</link>.</simpara>
<section xml:id="an-example-ansible-playbook-to-enable-online-block-discard_enabling-online-block-discard-using-rhel-system-roles">
<title>Example Ansible playbook to enable online block discard</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to mount an XFS file system with online block discard enabled.</simpara>
<example>
<title>A playbook that enables online block discard on /mnt/data/</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: <emphasis><phrase role="replaceable">xfs</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
        mount_options: discard
  roles:
    - rhel-system-roles.storage</screen>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>This playbook also performs all the operations of the persistent mount example described in <xref linkend="an-example-ansible-playbook-to-persistently-mount-a-file-system_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional_resources_7" remap="_additional_resources_7">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>For more information about the <literal>storage</literal> role, see <xref linkend="storage-role-intro_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="enabling-periodic-block-discard_discarding-unused-blocks">
<title>Enabling periodic block discard</title>
<simpara>This procedure enables a <literal>systemd</literal> timer that regularly discards unused blocks on all supported file systems.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enable and start the <literal>systemd</literal> timer:</simpara>
<screen># systemctl enable --now fstrim.timer</screen>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="managing-layered-local-storage-with-stratis_managing-file-systems">
<title>Managing layered local storage with Stratis</title>
<simpara>You can easily set up and manage complex storage configurations integrated by the Stratis high-level system.</simpara>
<important>
<simpara>Stratis is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview">https://access.redhat.com/support/offerings/techpreview</link>.</simpara>
</important>
<section xml:id="setting-up-stratis-file-systems_managing-layered-local-storage-with-stratis">
<title>Setting up Stratis file systems</title>
<simpara>As a system administrator, you can enable and set up the Stratis volume-managing file system on your system to easily manage layered storage.</simpara>
<section xml:id="the-purpose-and-features-of-stratis_setting-up-stratis-file-systems">
<title>The purpose and features of Stratis</title>
<simpara>Stratis is a local storage-management solution for Linux. It is focused on simplicity and ease of use, and gives you access to advanced storage features.</simpara>
<simpara>Stratis makes the following activities easier:</simpara>
<itemizedlist>
<listitem>
<simpara>Initial configuration of storage</simpara>
</listitem>
<listitem>
<simpara>Making changes later</simpara>
</listitem>
<listitem>
<simpara>Using advanced storage features</simpara>
</listitem>
</itemizedlist>
<simpara>Stratis is a hybrid user-and-kernel local storage management system that supports advanced storage features. The central concept of Stratis is a storage <emphasis>pool</emphasis>. This pool is created from one or more local disks or partitions, and volumes are created from the pool.</simpara>
<simpara>The pool enables many useful features, such as:</simpara>
<itemizedlist>
<listitem>
<simpara>File system snapshots</simpara>
</listitem>
<listitem>
<simpara>Thin provisioning</simpara>
</listitem>
<listitem>
<simpara>Tiering</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-of-a-stratis-volume_setting-up-stratis-file-systems">
<title>Components of a Stratis volume</title>
<simpara>Externally, Stratis presents the following volume components in the command-line interface and the API:</simpara>
<variablelist>
<varlistentry>
<term><literal>blockdev</literal></term>
<listitem>
<simpara>Block devices, such as a disk or a disk partition.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>pool</literal></term>
<listitem>
<simpara>Composed of one or more block devices.</simpara>
<simpara>A pool has a fixed total size, equal to the size of the block devices.</simpara>
<simpara>The pool contains most Stratis layers, such as the non-volatile data cache using the <literal>dm-cache</literal> target.</simpara>
<simpara>Stratis creates a <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/</literal> directory for each pool. This directory contains links to devices that represent Stratis file systems in the pool.</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term><literal>filesystem</literal></term>
<listitem>
<simpara>Each pool can contain one or more file systems, which store files.</simpara>
<simpara>File systems are thinly provisioned and do not have a fixed total size. The actual size of a file system grows with the data stored on it. If the size of the data approaches the virtual size of the file system, Stratis grows the thin volume and the file system automatically.</simpara>
<simpara>The file systems are formatted with XFS.</simpara>
<important>
<simpara>Stratis tracks information about file systems created using Stratis that XFS is not aware of, and changes made using XFS do not automatically create updates in Stratis. Users must not reformat or reconfigure XFS file systems that are managed by Stratis.</simpara>
</important>
<simpara>Stratis creates links to file systems at the <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis></literal> path.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Stratis uses many Device Mapper devices, which show up in <literal>dmsetup</literal> listings and the <literal role="filename">/proc/partitions</literal> file. Similarly, the <literal>lsblk</literal> command output reflects the internal workings and layers of Stratis.</simpara>
</note>
</section>
<section xml:id="block-devices-usable-with-stratis_setting-up-stratis-file-systems">
<title>Block devices usable with Stratis</title>
<simpara>This section lists storage devices that you can use for Stratis.</simpara>
<bridgehead xml:id="supported_devices" renderas="sect4" remap="_supported_devices">Supported devices</bridgehead>
<simpara>Stratis pools have been tested to work on these types of block devices:</simpara>
<itemizedlist>
<listitem>
<simpara>LUKS</simpara>
</listitem>
<listitem>
<simpara>LVM logical volumes</simpara>
</listitem>
<listitem>
<simpara>MD RAID</simpara>
</listitem>
<listitem>
<simpara>DM Multipath</simpara>
</listitem>
<listitem>
<simpara>iSCSI</simpara>
</listitem>
<listitem>
<simpara>HDDs and SSDs</simpara>
</listitem>
<listitem>
<simpara>NVMe devices</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>In the current version, Stratis does not handle failures in hard drives or other hardware. If you create a Stratis pool over multiple hardware devices, you increase the risk of data loss because multiple devices must be operational to access the data.</simpara>
</warning>
<bridgehead xml:id="unsupported_devices" renderas="sect4" remap="_unsupported_devices">Unsupported devices</bridgehead>
<simpara>Because Stratis contains a thin-provisioning layer, Red Hat does not recommend placing a Stratis pool on block devices that are already thinly-provisioned.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For iSCSI and other block devices requiring network, see the <literal>systemd.mount(5)</literal> man page for information on the <literal role="option">_netdev</literal> mount option.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-stratis_setting-up-stratis-file-systems">
<title>Installing Stratis</title>
<simpara>This procedure installs all packages necessary to use Stratis.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install packages that provide the Stratis service and command-line utilities:</simpara>
<screen># yum install stratisd stratis-cli</screen>
</listitem>
<listitem>
<simpara>Make sure that the <literal>stratisd</literal> service is enabled:</simpara>
<screen># systemctl enable --now stratisd</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-a-stratis-pool_setting-up-stratis-file-systems">
<title>Creating a Stratis pool</title>
<simpara>This procedure describes how to create an encrypted or an unencrypted Stratis pool from one or more block devices.</simpara>
<simpara>The following notes apply to encrypted Stratis pools:</simpara>
<itemizedlist>
<listitem>
<simpara>Each block device is encrypted using the <literal>cryptsetup</literal> library and implements the <literal>LUKS2</literal> format.</simpara>
</listitem>
<listitem>
<simpara>Each Stratis pool can have a unique key or it can share the same key with other pools. These keys are stored in the kernel keyring.</simpara>
</listitem>
<listitem>
<simpara>All block devices that comprise a Stratis pool are either encrypted or unencrypted. It is not possible to have both encrypted and unencrypted block devices in the same Stratis pool.</simpara>
</listitem>
<listitem>
<simpara>Block devices added to the data tier of an encrypted Stratis pool are automatically encrypted.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis v2.2.1 is installed on your system. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>The block devices on which you are creating a Stratis pool are not in use and are not mounted.</simpara>
</listitem>
<listitem>
<simpara>The block devices on which you are creating a Stratis pool are at least 1 GiB in size each.</simpara>
</listitem>
<listitem>
<simpara>On the IBM Z architecture, the <literal>/dev/dasd*</literal> block devices must be partitioned. Use the partition in the Stratis pool.</simpara>
<simpara>For information on partitioning DASD devices, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/configuring-a-linux-instance-on-ibm-z_installing-rhel">Configuring a Linux instance on IBM Z</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If the selected block device contains file system, partition table, or RAID signatures, erase them using the following command:</simpara>
<screen># wipefs --all <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<simpara>where <literal role="replaceable"><emphasis>block-device</emphasis></literal> is the path to the block device; for example, <literal>/dev/sdb</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create the new Stratis pool on the selected block device(s):</simpara>
<note>
<simpara>Specify multiple block devices on a single line, separated by a space:</simpara>
<screen># stratis pool create <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">block-device-1</phrase></emphasis> <emphasis><phrase role="replaceable">block-device-2</phrase></emphasis></screen>
</note>
<itemizedlist>
<listitem>
<simpara>To create an unencrypted Stratis pool, use the following command and go to step 3:</simpara>
<screen># stratis pool create <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<simpara>where <literal role="replaceable"><emphasis>block-device</emphasis></literal> is the path to an empty or wiped block device.</simpara>
<note>
<simpara>You cannot encrypt an unencrypted Stratis pool after you create it.</simpara>
</note>
</listitem>
<listitem>
<simpara>To create an encrypted Stratis pool, complete the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If you have not created a key set already, run the following command and follow the prompts to create a key set to use for the encryption:</simpara>
<screen># stratis key set --capture-key <emphasis><phrase role="replaceable">key-description</phrase></emphasis></screen>
<simpara>where <literal role="replaceable"><emphasis>key-description</emphasis></literal> is the description or name of the key set.</simpara>
</listitem>
<listitem>
<simpara>Create the encrypted Stratis pool and specify the key description to use for the encryption. You can also specify the key path using the <literal>--keyfile-path</literal> parameter instead.</simpara>
<screen># stratis pool create --key-desc <emphasis><phrase role="replaceable">key-description</phrase></emphasis> <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">block-device</phrase></emphasis></screen>
<simpara>where</simpara>
<variablelist>
<varlistentry>
<term><literal><emphasis>key-description</emphasis></literal></term>
<listitem>
<simpara>Specifies the description or name of the key file to be used for the encryption.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal><emphasis>my-pool</emphasis></literal></term>
<listitem>
<simpara>Specifies the name of the new Stratis pool.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal><emphasis>block-device</emphasis></literal></term>
<listitem>
<simpara>Specifies the path to an empty or wiped block device.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Verify that the new Stratis pool was created:</simpara>
<screen># stratis pool list</screen>
</listitem>
</orderedlist>
<formalpara>
<title>Troubleshooting</title>
<para>After a system reboot, sometimes you might not see your encrypted Stratis pool or the block devices that comprise it. If you encounter this issue, you must unlock the Stratis pool to make it visible.</para>
</formalpara>
<simpara>To unlock the Stratis pool, complete the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Recreate the key set using the same key description that was used previously:</simpara>
<screen># stratis key set --capture-key <emphasis><phrase role="replaceable">key-description</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Unlock the Stratis pool and the block device(s):</simpara>
<screen># stratis pool unlock</screen>
</listitem>
<listitem>
<simpara>Verify that the Stratis pool is visible:</simpara>
<screen># stratis pool list</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>Create a Stratis file system on the pool. For more information, see <xref linkend="creating-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-a-stratis-file-system_setting-up-stratis-file-systems">
<title>Creating a Stratis file system</title>
<simpara>This procedure creates a Stratis file system on an existing Stratis pool.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis pool. See <xref linkend="creating-a-stratis-pool_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create a Stratis file system on a pool, use:</simpara>
<screen># stratis fs create <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
<itemizedlist>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> with the name of your existing Stratis pool.</simpara>
</listitem>
<listitem>
<simpara>Replace <emphasis><phrase role="replaceable">my-fs</phrase></emphasis> with an arbitrary name for the file system.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To verify, list file systems within the pool:</simpara>
<screen># stratis fs list <emphasis><phrase role="replaceable">my-pool</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>Mount the Stratis file system. See <xref linkend="mounting-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mounting-a-stratis-file-system_setting-up-stratis-file-systems">
<title>Mounting a Stratis file system</title>
<simpara>This procedure mounts an existing Stratis file system to access the content.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis file system. See <xref linkend="creating-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To mount the file system, use the entries that Stratis maintains in the <literal role="filename">/stratis/</literal> directory:</simpara>
<screen># mount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<simpara>The file system is now mounted on the <emphasis><phrase role="replaceable">mount-point</phrase></emphasis> directory and ready to use.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="persistently-mounting-a-stratis-file-system_setting-up-stratis-file-systems">
<title>Persistently mounting a Stratis file system</title>
<simpara>This procedure persistently mounts a Stratis file system so that it is available automatically after booting the system.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis file system. See <xref linkend="creating-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Determine the UUID attribute of the file system:</simpara>
<screen>$ lsblk --output=UUID /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
<simpara>For example:</simpara>
<example>
<title>Viewing the UUID of Stratis file system</title>
<screen>$ lsblk --output=UUID /stratis/my-pool/fs1

UUID
a1f0b64a-4ebb-4d4e-9543-b1d79f600283</screen>
</example>
</listitem>
<listitem>
<simpara>If the mount point directory does not exist, create it:</simpara>
<screen># mkdir --parents <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>As root, edit the <literal role="filename">/etc/fstab</literal> file and add a line for the file system, identified by the UUID. Use <literal>xfs</literal> as the file system type and add the <literal>x-systemd.requires=stratisd.service</literal> option.</simpara>
<simpara>For example:</simpara>
<example>
<title>The /fs1 mount point in /etc/fstab</title>
<screen>UUID=a1f0b64a-4ebb-4d4e-9543-b1d79f600283 /fs1 xfs defaults,x-systemd.requires=stratisd.service 0 0</screen>
</example>
</listitem>
<listitem>
<simpara>Regenerate mount units so that your system registers the new configuration:</simpara>
<screen># systemctl daemon-reload</screen>
</listitem>
<listitem>
<simpara>Try mounting the file system to verify that the configuration works:</simpara>
<screen># mount <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><xref linkend="assembly_persistently-mounting-file-systems_assembly_mounting-file-systems"/></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-setting-up-stratis-file-systems">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Stratis Storage</emphasis> website: <link xlink:href="https://stratis-storage.github.io/">https://stratis-storage.github.io/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="extending-a-stratis-volume-with-additional-block-devices_managing-layered-local-storage-with-stratis">
<title>Extending a Stratis volume with additional block devices</title>
<simpara>You can attach additional block devices to a Stratis pool to provide more storage capacity for Stratis file systems.</simpara>
<section xml:id="components-of-a-stratis-volume_extending-a-stratis-volume-with-additional-block-devices">
<title>Components of a Stratis volume</title>
<simpara>Externally, Stratis presents the following volume components in the command-line interface and the API:</simpara>
<variablelist>
<varlistentry>
<term><literal>blockdev</literal></term>
<listitem>
<simpara>Block devices, such as a disk or a disk partition.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>pool</literal></term>
<listitem>
<simpara>Composed of one or more block devices.</simpara>
<simpara>A pool has a fixed total size, equal to the size of the block devices.</simpara>
<simpara>The pool contains most Stratis layers, such as the non-volatile data cache using the <literal>dm-cache</literal> target.</simpara>
<simpara>Stratis creates a <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/</literal> directory for each pool. This directory contains links to devices that represent Stratis file systems in the pool.</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term><literal>filesystem</literal></term>
<listitem>
<simpara>Each pool can contain one or more file systems, which store files.</simpara>
<simpara>File systems are thinly provisioned and do not have a fixed total size. The actual size of a file system grows with the data stored on it. If the size of the data approaches the virtual size of the file system, Stratis grows the thin volume and the file system automatically.</simpara>
<simpara>The file systems are formatted with XFS.</simpara>
<important>
<simpara>Stratis tracks information about file systems created using Stratis that XFS is not aware of, and changes made using XFS do not automatically create updates in Stratis. Users must not reformat or reconfigure XFS file systems that are managed by Stratis.</simpara>
</important>
<simpara>Stratis creates links to file systems at the <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis></literal> path.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Stratis uses many Device Mapper devices, which show up in <literal>dmsetup</literal> listings and the <literal role="filename">/proc/partitions</literal> file. Similarly, the <literal>lsblk</literal> command output reflects the internal workings and layers of Stratis.</simpara>
</note>
</section>
<section xml:id="adding-block-devices-to-a-stratis-pool_extending-a-stratis-volume-with-additional-block-devices">
<title>Adding block devices to a Stratis pool</title>
<simpara>This procedure adds one or more block devices to a Stratis pool to be usable by Stratis file systems.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>The block devices that you are adding to the Stratis pool are not in use and not mounted.</simpara>
</listitem>
<listitem>
<simpara>The block devices that you are adding to the Stratis pool are at least 1 GiB in size each.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To add one or more block devices to the pool, use:</simpara>
<screen># stratis pool add-data <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">device-1</phrase></emphasis> <emphasis><phrase role="replaceable">device-2</phrase></emphasis> <emphasis><phrase role="replaceable">device-n</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-extending-a-stratis-volume-with-additional-block-devices">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Stratis Storage</emphasis> website: <link xlink:href="https://stratis-storage.github.io/">https://stratis-storage.github.io/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="monitoring-stratis-file-systems_managing-layered-local-storage-with-stratis">
<title>Monitoring Stratis file systems</title>
<simpara>As a Stratis user, you can view information about Stratis volumes on your system to monitor their state and free space.</simpara>
<section xml:id="stratis-sizes-reported-by-different-utilities_monitoring-stratis-file-systems">
<title>Stratis sizes reported by different utilities</title>
<simpara>This section explains the difference between Stratis sizes reported by standard utilities such as <literal>df</literal> and the <literal>stratis</literal> utility.</simpara>
<simpara>Standard Linux utilities such as <literal>df</literal> report the size of the XFS file system layer on Stratis, which is 1 TiB. This is not useful information, because the actual storage usage of Stratis is less due to thin provisioning, and also because Stratis automatically grows the file system when the XFS layer is close to full.</simpara>
<important>
<simpara>Regularly monitor the amount of data written to your Stratis file systems, which is reported as the <emphasis>Total Physical Used</emphasis> value. Make sure it does not exceed the <emphasis>Total Physical Size</emphasis> value.</simpara>
</important>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="displaying-information-about-stratis-volumes_monitoring-stratis-file-systems">
<title>Displaying information about Stratis volumes</title>
<simpara>This procedure lists statistics about your Stratis volumes, such as the total, used, and free size or file systems and block devices belonging to a pool.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To display information about all <emphasis role="strong">block devices</emphasis> used for Stratis on your system:</simpara>
<screen># stratis blockdev

Pool Name  Device Node    Physical Size   State  Tier
<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>    <emphasis><phrase role="replaceable">/dev/sdb</phrase></emphasis>            <emphasis><phrase role="replaceable">9.10 TiB</phrase></emphasis>  <emphasis><phrase role="replaceable">In-use</phrase></emphasis>  <emphasis><phrase role="replaceable">Data</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>To display information about all Stratis <emphasis role="strong">pools</emphasis> on your system:</simpara>
<screen># stratis pool

Name    Total Physical Size  Total Physical Used
<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>            <emphasis><phrase role="replaceable">9.10 TiB</phrase></emphasis>              <emphasis><phrase role="replaceable">598 MiB</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>To display information about all Stratis <emphasis role="strong">file systems</emphasis> on your system:</simpara>
<screen># stratis filesystem

Pool Name  Name  Used     Created            Device
<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>    <emphasis><phrase role="replaceable">my-fs</phrase></emphasis> <emphasis><phrase role="replaceable">546 MiB</phrase></emphasis>  <emphasis><phrase role="replaceable">Nov 08 2018 08:03</phrase></emphasis>  /stratis/<emphasis><phrase role="replaceable">my-pool/my-fs</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-monitoring-stratis-file-systems">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Stratis Storage</emphasis> website: <link xlink:href="https://stratis-storage.github.io/">https://stratis-storage.github.io/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-snapshots-on-stratis-file-systems_managing-layered-local-storage-with-stratis">
<title>Using snapshots on Stratis file systems</title>
<simpara>You can use snapshots on Stratis file systems to capture file system state at arbitrary times and restore it in the future.</simpara>
<section xml:id="characteristics-of-stratis-snapshots_using-snapshots-on-stratis-file-systems">
<title>Characteristics of Stratis snapshots</title>
<simpara>This section describes the properties and limitations of file system snapshots on Stratis.</simpara>
<simpara>In Stratis, a snapshot is a regular Stratis file system created as a copy of another Stratis file system. The snapshot initially contains the same file content as the original file system, but can change as the snapshot is modified. Whatever changes you make to the snapshot will not be reflected in the original file system.</simpara>
<simpara>The current snapshot implementation in Stratis is characterized by the following:</simpara>
<itemizedlist>
<listitem>
<simpara>A snapshot of a file system is another file system.</simpara>
</listitem>
<listitem>
<simpara>A snapshot and its origin are not linked in lifetime. A snapshotted file system can live longer than the file system it was created from.</simpara>
</listitem>
<listitem>
<simpara>A file system does not have to be mounted to create a snapshot from it.</simpara>
</listitem>
<listitem>
<simpara>Each snapshot uses around half a gigabyte of actual backing storage, which is needed for the XFS log.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-a-stratis-snapshot_using-snapshots-on-stratis-file-systems">
<title>Creating a Stratis snapshot</title>
<simpara>This procedure creates a Stratis file system as a snapshot of an existing Stratis file system.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis file system. See <xref linkend="creating-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To create a Stratis snapshot, use:</simpara>
<screen># stratis fs snapshot <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="accessing-the-content-of-a-stratis-snapshot_using-snapshots-on-stratis-file-systems">
<title>Accessing the content of a Stratis snapshot</title>
<simpara>This procedure mounts a snapshot of a Stratis file system to make it accessible for read and write operations.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis snapshot. See <xref linkend="creating-a-stratis-snapshot_using-snapshots-on-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To access the snapshot, mount it as a regular file system from the <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/</literal> directory:</simpara>
<screen># mount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><xref linkend="mounting-a-stratis-file-system_setting-up-stratis-file-systems"/></simpara>
</listitem>
<listitem>
<simpara>The <literal>mount(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="reverting-a-stratis-file-system-to-a-previous-snapshot_using-snapshots-on-stratis-file-systems">
<title>Reverting a Stratis file system to a previous snapshot</title>
<simpara>This procedure reverts the content of a Stratis file system to the state captured in a Stratis snapshot.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis snapshot. See <xref linkend="creating-a-stratis-snapshot_using-snapshots-on-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optionally, back up the current state of the file system to be able to access it later:</simpara>
<screen># stratis filesystem snapshot <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-backup</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Unmount and remove the original file system:</simpara>
<screen># umount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis>
# stratis filesystem destroy <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Create a copy of the snapshot under the name of the original file system:</simpara>
<screen># stratis filesystem snapshot <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Mount the snapshot, which is now accessible with the same name as the original file system:</simpara>
<screen># mount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis> <emphasis><phrase role="replaceable">mount-point</phrase></emphasis></screen>
</listitem>
</orderedlist>
<simpara>The content of the file system named <emphasis><phrase role="replaceable">my-fs</phrase></emphasis> is now identical to the snapshot <emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis>.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="removing-a-stratis-snapshot_using-snapshots-on-stratis-file-systems">
<title>Removing a Stratis snapshot</title>
<simpara>This procedure removes a Stratis snapshot from a pool. Data on the snapshot are lost.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis snapshot. See <xref linkend="creating-a-stratis-snapshot_using-snapshots-on-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Unmount the snapshot:</simpara>
<screen># umount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Destroy the snapshot:</simpara>
<screen># stratis filesystem destroy <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-snapshot</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-using-snapshots-on-stratis-file-systems">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Stratis Storage</emphasis> website: <link xlink:href="https://stratis-storage.github.io/">https://stratis-storage.github.io/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="removing-stratis-file-systems_managing-layered-local-storage-with-stratis">
<title>Removing Stratis file systems</title>
<simpara>You can remove an existing Stratis file system or a Stratis pool, destroying data on them.</simpara>
<section xml:id="components-of-a-stratis-volume_removing-stratis-file-systems">
<title>Components of a Stratis volume</title>
<simpara>Externally, Stratis presents the following volume components in the command-line interface and the API:</simpara>
<variablelist>
<varlistentry>
<term><literal>blockdev</literal></term>
<listitem>
<simpara>Block devices, such as a disk or a disk partition.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>pool</literal></term>
<listitem>
<simpara>Composed of one or more block devices.</simpara>
<simpara>A pool has a fixed total size, equal to the size of the block devices.</simpara>
<simpara>The pool contains most Stratis layers, such as the non-volatile data cache using the <literal>dm-cache</literal> target.</simpara>
<simpara>Stratis creates a <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/</literal> directory for each pool. This directory contains links to devices that represent Stratis file systems in the pool.</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term><literal>filesystem</literal></term>
<listitem>
<simpara>Each pool can contain one or more file systems, which store files.</simpara>
<simpara>File systems are thinly provisioned and do not have a fixed total size. The actual size of a file system grows with the data stored on it. If the size of the data approaches the virtual size of the file system, Stratis grows the thin volume and the file system automatically.</simpara>
<simpara>The file systems are formatted with XFS.</simpara>
<important>
<simpara>Stratis tracks information about file systems created using Stratis that XFS is not aware of, and changes made using XFS do not automatically create updates in Stratis. Users must not reformat or reconfigure XFS file systems that are managed by Stratis.</simpara>
</important>
<simpara>Stratis creates links to file systems at the <literal role="filename">/stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis></literal> path.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Stratis uses many Device Mapper devices, which show up in <literal>dmsetup</literal> listings and the <literal role="filename">/proc/partitions</literal> file. Similarly, the <literal>lsblk</literal> command output reflects the internal workings and layers of Stratis.</simpara>
</note>
</section>
<section xml:id="removing-a-stratis-file-system_removing-stratis-file-systems">
<title>Removing a Stratis file system</title>
<simpara>This procedure removes an existing Stratis file system. Data stored on it are lost.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis file system. See <xref linkend="creating-a-stratis-file-system_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Unmount the file system:</simpara>
<screen># umount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Destroy the file system:</simpara>
<screen># stratis filesystem destroy <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Verify that the file system no longer exists:</simpara>
<screen># stratis filesystem list <emphasis><phrase role="replaceable">my-pool</phrase></emphasis></screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="removing-a-stratis-pool_removing-stratis-file-systems">
<title>Removing a Stratis pool</title>
<simpara>This procedure removes an existing Stratis pool. Data stored on it are lost.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Stratis is installed. See <xref linkend="installing-stratis_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
<listitem>
<simpara>The <literal>stratisd</literal> service is running.</simpara>
</listitem>
<listitem>
<simpara>You have created a Stratis pool. See <xref linkend="creating-a-stratis-pool_setting-up-stratis-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List file systems on the pool:</simpara>
<screen># stratis filesystem list <emphasis><phrase role="replaceable">my-pool</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Unmount all file systems on the pool:</simpara>
<screen># umount /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs-1</phrase></emphasis> \
         /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs-2</phrase></emphasis> \
         /stratis/<emphasis><phrase role="replaceable">my-pool</phrase></emphasis>/<emphasis><phrase role="replaceable">my-fs-n</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Destroy the file systems:</simpara>
<screen># stratis filesystem destroy <emphasis><phrase role="replaceable">my-pool</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-1</phrase></emphasis> <emphasis><phrase role="replaceable">my-fs-2</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Destroy the pool:</simpara>
<screen># stratis pool destroy <emphasis><phrase role="replaceable">my-pool</phrase></emphasis></screen>
</listitem>
<listitem>
<simpara>Verify that the pool no longer exists:</simpara>
<screen># stratis pool list</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>stratis(8)</literal> man page</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="related-information-removing-stratis-file-systems">
<title>Related information</title>
<itemizedlist>
<listitem>
<simpara>The <emphasis>Stratis Storage</emphasis> website: <link xlink:href="https://stratis-storage.github.io/">https://stratis-storage.github.io/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="getting-started-with-an-ext3-file-system_managing-file-systems">
<title>Getting started with an ext3 file system</title>
<simpara>As a system administrator, you can create, mount, resize, backup, and restore an ext3 file system. The ext3 file system is essentially an enhanced version of the ext2 file system.</simpara>
<section xml:id="features-of-an-ext3-file-system_getting-started-with-an-ext3-file-system">
<title>Features of an ext3 file system</title>
<simpara>Following are the features of an ext3 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>Availability: After an unexpected power failure or system crash, file system check is not required due to the journaling provided. The default journal size takes about a second to recover, depending on the speed of the hardware</simpara>
<note>
<simpara>The only supported journaling mode in ext3 is <literal>data=ordered</literal> (default). For more information, see <link xlink:href="https://access.redhat.com/solutions/424073">Is the EXT journaling option "data=writeback" supported in RHEL?</link> Knowledgebase article.</simpara>
</note>
</listitem>
<listitem>
<simpara>Data Integrity: The ext3 file system prevents loss of data integrity during an unexpected power failure or system crash.</simpara>
</listitem>
<listitem>
<simpara>Speed: Despite writing some data more than once, ext3 has a higher throughput in most cases than ext2 because ext3’s journaling optimizes hard drive head motion.</simpara>
</listitem>
<listitem>
<simpara>Easy Transition: It is easy to migrate from ext2 to ext3 and gain the benefits of a robust journaling file system without reformatting.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>ext3</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-an-ext-file-system_getting-started-with-an-ext3-file-system">
<title>Creating an ext3 file system</title>
<simpara>As a system administrator, you can create an ext3 file system on a block device using <literal>mkfs.ext3</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A partition on your disk.  For information on creating MBR or GPT partitions, see <xref linkend="assembly_creating-a-partition-table-on-a-disk_assembly_getting-started-with-partitions"/>.</simpara>
<simpara>Alternatively, use an LVM or MD volume.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create an ext3 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>For a regular-partition device, an LVM volume, an MD volume, or a similar device, use the following command:</simpara>
<screen># mkfs.ext3 /dev/<emphasis>block_device</emphasis></screen>
<simpara>Replace /dev/<emphasis>block_device</emphasis> with the path to a block device.</simpara>
<simpara>For example, <literal>/dev/sdb1</literal>, <literal>/dev/disk/by-uuid/05e99ec8-def1-4a5e-8a9d-5945339ceb2a</literal>, or <literal>/dev/my-volgroup/my-lv</literal>.
In general, the default options are optimal for most usage scenarios.</simpara>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<simpara>To specify a UUID when creating a file system:</simpara>
<screen># mkfs.ext3 -U <emphasis>UUID</emphasis> /dev/<emphasis>block_device</emphasis></screen>
<simpara>Replace <emphasis>UUID</emphasis> with the UUID you want to set: for example, <literal>7cd65de3-e0be-41d9-b66d-96d749c02da7</literal>.</simpara>
<simpara>Replace /dev/<emphasis>block_device</emphasis> with the path to an ext3 file system to have the UUID added to it: for example, <literal>/dev/sda8</literal>.</simpara>
</listitem>
<listitem>
<simpara>To specify a label when creating a file system:</simpara>
<screen># mkfs.ext3 -L <emphasis>label-name</emphasis> /dev/<emphasis>block_device</emphasis></screen>
</listitem>
</itemizedlist>
</note>
</listitem>
<listitem>
<simpara>To view the created ext3 file system:</simpara>
<screen># blkid</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>ext3</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mkfs.ext3</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mounting-an-ext-file-system_getting-started-with-an-ext3-file-system">
<title>Mounting an ext3 file system</title>
<simpara>As a system administrator, you can mount an ext3 file system using the <literal>mount</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An ext3 file system. For information on creating an ext3 file system, see <xref linkend="creating-an-ext-file-system_getting-started-with-an-ext3-file-system"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create a mount point to mount the file system:</simpara>
<screen># mkdir <emphasis>/mount/point</emphasis></screen>
<simpara>Replace <emphasis>/mount/point</emphasis> with the directory name where mount point of the partition must be created.</simpara>
</listitem>
<listitem>
<simpara>To mount an ext3 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>To mount an ext3 file system with no extra options:</simpara>
<screen># mount /dev/<emphasis>block_device /mount/point</emphasis></screen>
</listitem>
<listitem>
<simpara>To mount the file system persistently, see <xref linkend="assembly_persistently-mounting-file-systems_assembly_mounting-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To view the mounted file system:</simpara>
<screen># df -h</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>ext3</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>fstab</literal> man page.</simpara>
</listitem>
<listitem>
<simpara><xref linkend="assembly_mounting-file-systems_managing-file-systems"/></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="resizing-an-ext-file-system_getting-started-with-an-ext3-file-system">
<title>Resizing an ext3 file system</title>
<simpara>As a system administrator, you can  resize an ext3 file system using the <literal>resize2fs</literal> utility. The <literal>resize2fs</literal> utility reads the size in units of file system block size, unless a suffix indicating a specific unit is used. The following suffixes indicate specific units:</simpara>
<itemizedlist>
<listitem>
<simpara>s (sectors) - <literal>512</literal> byte sectors</simpara>
</listitem>
<listitem>
<simpara>K (kilobytes) - <literal>1,024</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>M (megabytes) - <literal>1,048,576</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>G (gigabytes) - <literal>1,073,741,824</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>T (terabytes) - <literal>1,099,511,627,776</literal> bytes</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An ext3 file system. For information on creating an ext3 file system, see <xref linkend="creating-an-ext-file-system_getting-started-with-an-ext3-file-system"/>.</simpara>
</listitem>
<listitem>
<simpara>An underlying block device of an appropriate size to hold the file system after resizing.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To resize an ext3 file system, take the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara>To shrink and grow the size of an unmounted ext3 file system:</simpara>
<screen># umount /dev/<emphasis>block_device</emphasis>
# e2fsck -f /dev/<emphasis>block_device</emphasis>
# resize2fs /dev/<emphasis>block_device</emphasis> <emphasis>size</emphasis></screen>
<simpara>Replace <emphasis>/dev/block_device</emphasis> with the path to the block device, for example <literal>/dev/sdb1</literal>.</simpara>
<simpara>Replace <emphasis>size</emphasis> with the required resize value using <literal>s</literal>, <literal>K</literal>, <literal>M</literal>, <literal>G</literal>, and <literal>T</literal> suffixes.</simpara>
</listitem>
<listitem>
<simpara>An ext3 file system may be grown while mounted using the <literal>resize2fs</literal> command:</simpara>
<screen># resize2fs <emphasis>/mount/device size</emphasis></screen>
<note>
<simpara>The size parameter is optional (and often redundant) when expanding. The <literal>resize2fs</literal> automatically expands to fill the available space of the container, usually a logical volume or partition.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To view the resized file system:</simpara>
<screen># df -h</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>resize2fs</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>e2fsck</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>ext3</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-mounting-ext3-filesystem-using-rhel-system-roles_getting-started-with-an-ext3-file-system">
<title>Creating and mounting ext3 file systems using RHEL System Roles</title>
<simpara>This section describes how to create an ext3 file system with a given label on a disk, and persistently mount the file system using the <literal>storage</literal> role.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Ansible playbook including the <literal>storage</literal> role exists.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on how to apply such a playbook, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#applying-a-role_con_intro-to-rhel-system-roles">Applying a role</link>.</simpara>
<section xml:id="an-example-ansible-playbook-to-create-mount-ext3-file-system_creating-mounting-ext3-filesystem-using-rhel-system-roles">
<title>Example Ansible playbook to create and mount an ext3 file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create and mount an Ext3 file system.</simpara>
<example>
<title>A playbook that creates Ext3 on /dev/sdb and mounts it at /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: ext3
        fs_label: <emphasis><phrase role="replaceable">label-name</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The playbook creates the file system on the <literal>/dev/sdb</literal> disk.</simpara>
</listitem>
<listitem>
<simpara>The playbook persistently mounts the file system at the <literal><emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis></literal> directory.</simpara>
</listitem>
<listitem>
<simpara>The label of the file system is <literal><emphasis><phrase role="replaceable">label-name</phrase></emphasis></literal>.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional_resources_8" remap="_additional_resources_8">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>For more information about the <literal>storage</literal> role, see <xref linkend="storage-role-intro_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="getting-started-with-an-ext4-file-system_managing-file-systems">
<title>Getting started with an ext4 file system</title>
<simpara>As a system administrator, you can create, mount, resize, backup, and restore an ext4 file system. The ext4 file system is a scalable extension of the ext3 file system. With Red Hat Enterprise Linux 8, it can support a maximum individual file size of <literal>16</literal> terabytes, and file system to a maximum of <literal>50</literal> terabytes.</simpara>
<section xml:id="features-of-an-ext4-file-system_getting-started-with-an-ext4-file-system">
<title>Features of an ext4 file system</title>
<simpara>Following are the features of an ext4 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>Using extents: The ext4 file system uses extents, which improves performance when using large files and reduces metadata overhead for large files.</simpara>
</listitem>
<listitem>
<simpara>Ext4 labels unallocated block groups and inode table sections accordingly,  which allows the block groups and table sections to be skipped during a file system check. It leads to a quick file system check, which becomes more beneficial as the file system grows in size.</simpara>
</listitem>
<listitem>
<simpara>Metadata checksum: By default, this feature is enabled in Red Hat Enterprise Linux 8.</simpara>
</listitem>
<listitem>
<simpara>Allocation features of an ext4 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>Persistent pre-allocation</simpara>
</listitem>
<listitem>
<simpara>Delayed allocation</simpara>
</listitem>
<listitem>
<simpara>Multi-block allocation</simpara>
</listitem>
<listitem>
<simpara>Stripe-aware allocation</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Extended attributes (<literal>xattr</literal>): This allows the system to associate several additional name and value pairs per file.</simpara>
</listitem>
<listitem>
<simpara>Quota journaling: This avoids the need for lengthy quota consistency checks after a crash.</simpara>
<note>
<simpara>The only supported journaling mode in ext4 is <literal>data=ordered</literal> (default). For more information, see <link xlink:href="https://access.redhat.com/solutions/424073">Is the EXT journaling option "data=writeback" supported in RHEL?</link> Knowledgebase article.</simpara>
</note>
</listitem>
<listitem>
<simpara>Subsecond timestamps — This gives timestamps to the subsecond.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>ext4</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-an-ext-file-system_getting-started-with-an-ext4-file-system">
<title>Creating an ext4 file system</title>
<simpara>As a system administrator, you can create an ext4 file system on a block device using <literal>mkfs.ext4</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A partition on your disk.  For information on creating MBR or GPT partitions, see <xref linkend="assembly_creating-a-partition-table-on-a-disk_assembly_getting-started-with-partitions"/>.</simpara>
<simpara>Alternatively, use an LVM or MD volume.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create an ext4 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>For a regular-partition device, an LVM volume, an MD volume, or a similar device, use the following command:</simpara>
<screen># mkfs.ext4 /dev/<emphasis>block_device</emphasis></screen>
<simpara>Replace /dev/<emphasis>block_device</emphasis> with the path to a block device.</simpara>
<simpara>For example, <literal>/dev/sdb1</literal>, <literal>/dev/disk/by-uuid/05e99ec8-def1-4a5e-8a9d-5945339ceb2a</literal>, or <literal>/dev/my-volgroup/my-lv</literal>.
In general, the default options are optimal for most usage scenarios.</simpara>
</listitem>
<listitem>
<simpara>For striped block devices (for example, RAID5 arrays), the stripe geometry can be specified at the time of file system creation. Using proper stripe geometry enhances the performance of an ext4 file system. For example, to create a file system with a 64k stride (that is, 16 x 4096) on a 4k-block file system, use the following command:</simpara>
<screen># mkfs.ext4 -E stride=16,stripe-width=64 /dev/<emphasis>block_device</emphasis></screen>
<simpara>In the given example:</simpara>
<itemizedlist>
<listitem>
<simpara>stride=value: Specifies the RAID chunk size</simpara>
</listitem>
<listitem>
<simpara>stripe-width=value: Specifies the number of data disks in a RAID device, or the number of stripe units in the stripe.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<simpara>To specify a UUID when creating a file system:</simpara>
<screen># mkfs.ext4 -U <emphasis>UUID</emphasis> /dev/<emphasis>block_device</emphasis></screen>
<simpara>Replace <emphasis>UUID</emphasis> with the UUID you want to set: for example, <literal>7cd65de3-e0be-41d9-b66d-96d749c02da7</literal>.</simpara>
<simpara>Replace /dev/<emphasis>block_device</emphasis> with the path to an ext4 file system to have the UUID added to it: for example, <literal>/dev/sda8</literal>.</simpara>
</listitem>
<listitem>
<simpara>To specify a label when creating a file system:</simpara>
<screen># mkfs.ext4 -L <emphasis>label-name</emphasis> /dev/<emphasis>block_device</emphasis></screen>
</listitem>
</itemizedlist>
</note>
</listitem>
<listitem>
<simpara>To view the created ext4 file system:</simpara>
<screen># blkid</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>ext4</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mkfs.ext4</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mounting-an-ext-file-system_getting-started-with-an-ext4-file-system">
<title>Mounting an ext4 file system</title>
<simpara>As a system administrator, you can mount an ext4 file system using the <literal>mount</literal> utility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An ext4 file system. For information on creating an ext4 file system, see <xref linkend="creating-an-ext-file-system_getting-started-with-an-ext4-file-system"/>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create a mount point to mount the file system:</simpara>
<screen># mkdir <emphasis>/mount/point</emphasis></screen>
<simpara>Replace <emphasis>/mount/point</emphasis> with the directory name where mount point of the partition must be created.</simpara>
</listitem>
<listitem>
<simpara>To mount an ext4 file system:</simpara>
<itemizedlist>
<listitem>
<simpara>To mount an ext4 file system with no extra options:</simpara>
<screen># mount /dev/<emphasis>block_device /mount/point</emphasis></screen>
</listitem>
<listitem>
<simpara>To mount the file system persistently, see <xref linkend="assembly_persistently-mounting-file-systems_assembly_mounting-file-systems"/>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To view the mounted file system:</simpara>
<screen># df -h</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>mount</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>ext4</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>fstab</literal> man page.</simpara>
</listitem>
<listitem>
<simpara><xref linkend="assembly_mounting-file-systems_managing-file-systems"/></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="resizing-an-ext-file-system_getting-started-with-an-ext4-file-system">
<title>Resizing an ext4 file system</title>
<simpara>As a system administrator, you can  resize an ext4 file system using the <literal>resize2fs</literal> utility. The <literal>resize2fs</literal> utility reads the size in units of file system block size, unless a suffix indicating a specific unit is used. The following suffixes indicate specific units:</simpara>
<itemizedlist>
<listitem>
<simpara>s (sectors) - <literal>512</literal> byte sectors</simpara>
</listitem>
<listitem>
<simpara>K (kilobytes) - <literal>1,024</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>M (megabytes) - <literal>1,048,576</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>G (gigabytes) - <literal>1,073,741,824</literal> bytes</simpara>
</listitem>
<listitem>
<simpara>T (terabytes) - <literal>1,099,511,627,776</literal> bytes</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An ext4 file system. For information on creating an ext4 file system, see <xref linkend="creating-an-ext-file-system_getting-started-with-an-ext4-file-system"/>.</simpara>
</listitem>
<listitem>
<simpara>An underlying block device of an appropriate size to hold the file system after resizing.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To resize an ext4 file system, take the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara>To shrink and grow the size of an unmounted ext4 file system:</simpara>
<screen># umount /dev/<emphasis>block_device</emphasis>
# e2fsck -f /dev/<emphasis>block_device</emphasis>
# resize2fs /dev/<emphasis>block_device</emphasis> <emphasis>size</emphasis></screen>
<simpara>Replace <emphasis>/dev/block_device</emphasis> with the path to the block device, for example <literal>/dev/sdb1</literal>.</simpara>
<simpara>Replace <emphasis>size</emphasis> with the required resize value using <literal>s</literal>, <literal>K</literal>, <literal>M</literal>, <literal>G</literal>, and <literal>T</literal> suffixes.</simpara>
</listitem>
<listitem>
<simpara>An ext4 file system may be grown while mounted using the <literal>resize2fs</literal> command:</simpara>
<screen># resize2fs <emphasis>/mount/device size</emphasis></screen>
<note>
<simpara>The size parameter is optional (and often redundant) when expanding. The <literal>resize2fs</literal> automatically expands to fill the available space of the container, usually a logical volume or partition.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To view the resized file system:</simpara>
<screen># df -h</screen>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>The <literal>resize2fs</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>e2fsck</literal> man page.</simpara>
</listitem>
<listitem>
<simpara>The <literal>ext4</literal> man page.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-mounting-ext4-file-system-using-rhel-system-roles_getting-started-with-an-ext4-file-system">
<title>Creating and mounting ext4 file systems using RHEL System Roles</title>
<simpara>This section describes how to create an ext4 file system with a given label on a disk, and persistently mount the file system using the <literal>storage</literal> role.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Ansible playbook including the <literal>storage</literal> role exists.</simpara>
</listitem>
</itemizedlist>
<simpara>For information on how to apply such a playbook, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/getting-started-with-system-administration_configuring-basic-system-settings#applying-a-role_con_intro-to-rhel-system-roles">Applying a role</link>.</simpara>
<section xml:id="an-example-playbook-to-create-mount-an-ext4-file-system_creating-mounting-ext4-file-system-using-rhel-system-roles">
<title>Example Ansible playbook to create and mount an Ext4 file system</title>
<simpara>This section provides an example Ansible playbook. This playbook applies the <literal>storage</literal> role to create and mount an Ext4 file system.</simpara>
<example>
<title>A playbook that creates Ext4 on /dev/sdb and mounts it at /mnt/data</title>
<screen>---
- hosts: all
  vars:
    storage_volumes:
      - name: <emphasis><phrase role="replaceable">barefs</phrase></emphasis>
        type: disk
        disks:
          - <emphasis><phrase role="replaceable">sdb</phrase></emphasis>
        fs_type: ext4
        fs_label: <emphasis><phrase role="replaceable">label-name</phrase></emphasis>
        mount_point: <emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis>
  roles:
    - rhel-system-roles.storage</screen>
<itemizedlist>
<listitem>
<simpara>The playbook creates the file system on the <literal>/dev/sdb</literal> disk.</simpara>
</listitem>
<listitem>
<simpara>The playbook persistently mounts the file system at the <literal><emphasis><phrase role="replaceable">/mnt/data</phrase></emphasis></literal> directory.</simpara>
</listitem>
<listitem>
<simpara>The label of the file system is <literal><emphasis><phrase role="replaceable">label-name</phrase></emphasis></literal>.</simpara>
</listitem>
</itemizedlist>
</example>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For details about the parameters used in the <literal>storage</literal> system role, see the <literal role="filename">/usr/share/ansible/roles/rhel-system-roles.storage/README.md</literal> file.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara>For more information about the <literal>storage</literal> role, see <xref linkend="storage-role-intro_managing-local-storage-using-rhel-system-roles"/>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="comparison-of-tools-used-with-ext4-and-xfs_getting-started-with-an-ext4-file-system">
<title>Comparison of tools used with ext4 and XFS</title>
<simpara>This section compares which tools to use to accomplish common tasks on the ext4 and XFS file systems.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Task</entry>
<entry align="left" valign="top">ext4</entry>
<entry align="left" valign="top">XFS</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Create a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>mkfs.ext4</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>mkfs.xfs</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File system check</simpara></entry>
<entry align="left" valign="top"><simpara><literal>e2fsck</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_repair</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Resize a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>resize2fs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_growfs</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Save an image of a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>e2image</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_metadump</literal> and <literal>xfs_mdrestore</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Label or tune a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>tune2fs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_admin</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Back up a file system</simpara></entry>
<entry align="left" valign="top"><simpara><literal>dump</literal> and <literal>restore</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfsdump</literal> and <literal>xfsrestore</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Quota management</simpara></entry>
<entry align="left" valign="top"><simpara><literal>quota</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_quota</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File mapping</simpara></entry>
<entry align="left" valign="top"><simpara><literal>filefrag</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>xfs_bmap</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</chapter>
</book>
